{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -r sample_data"
      ],
      "metadata": {
        "id": "umPPoBGMXd6B",
        "outputId": "90bb2337-22af-4389-8946-feb434f3610a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1-J8d0-gmoD0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe original"
      ],
      "metadata": {
        "id": "wGOLuWHWpKh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('p2_serie_de_tiempo.xlsx')\n",
        "df.drop(\"Unnamed: 0\",axis=1,inplace=True) #Elimina la columna indefinida, la que cuenta datos\n",
        "df.drop(\"Fecha\",axis=1,inplace=True) #Elimina la columna de la fecha ya que tenemos de por si los datos de la fecha\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "833NITSamz29",
        "outputId": "72a8e02e-6e9f-4719-cb2e-ea2e04376e6c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Horas  Agno  Mes  Dia_mes  Dia_sem  fest         20         21  \\\n",
              "0         6  2015    1        1        4     1  76.967742  65.548387   \n",
              "1         7  2015    1        1        4     1  70.451613  66.467742   \n",
              "2         8  2015    1        1        4     1  77.903226  47.193548   \n",
              "3         9  2015    1        1        4     1  70.806452  48.338710   \n",
              "4        10  2015    1        1        4     1  68.451613  50.924731   \n",
              "...     ...   ...  ...      ...      ...   ...        ...        ...   \n",
              "7303     15  2016   12       30        5     1  12.000000  38.500000   \n",
              "7304     16  2016   12       30        5     1  49.500000  44.000000   \n",
              "7305     17  2016   12       30        5     1  30.000000  54.000000   \n",
              "7306     18  2016   12       30        5     1  38.000000  38.000000   \n",
              "7307     19  2016   12       30        5     1  36.000000  28.000000   \n",
              "\n",
              "             22         23  ...         58         59         60         61  \\\n",
              "0     59.419355  60.258065  ...  64.645161  46.612903  89.967742  83.451613   \n",
              "1     35.903226  48.403226  ...  56.661290  48.806452  75.209677  79.758065   \n",
              "2     26.451613  33.419355  ...  59.000000  46.709677  72.741935  71.161290   \n",
              "3     32.693548  43.193548  ...  54.758065  49.000000  75.612903  78.741935   \n",
              "4     25.129032  44.516129  ...  50.473118  55.290323  75.645161  81.612903   \n",
              "...         ...        ...  ...        ...        ...        ...        ...   \n",
              "7303  22.500000  25.000000  ...  61.500000  38.000000  58.000000  55.000000   \n",
              "7304  24.000000  35.500000  ...  28.000000  42.000000  60.000000  30.000000   \n",
              "7305  25.000000  58.500000  ...  41.000000  48.000000  81.000000  72.000000   \n",
              "7306  27.000000  15.000000  ...  26.000000  37.000000  37.000000  37.000000   \n",
              "7307  27.000000  27.000000  ...  28.000000  47.000000  32.000000  25.000000   \n",
              "\n",
              "             62         64         65         66         67         68  \n",
              "0     90.645161  85.838710  85.612903  88.258065  85.709677  85.096774  \n",
              "1     68.161290  83.096774  81.048387  78.370968  73.306452  70.129032  \n",
              "2     65.096774  81.870968  79.225806  77.000000  70.419355  56.548387  \n",
              "3     79.419355  84.741935  80.419355  83.516129  75.741935  65.774194  \n",
              "4     75.064516  85.870968  80.838710  83.709677  77.677419  66.322581  \n",
              "...         ...        ...        ...        ...        ...        ...  \n",
              "7303  21.500000  25.500000  36.000000  45.500000  33.000000  38.500000  \n",
              "7304  88.000000  89.000000  86.500000  84.000000  90.000000  90.000000  \n",
              "7305  77.000000  71.000000  79.000000  74.000000  72.000000  77.000000  \n",
              "7306  37.000000  37.000000  37.000000  37.000000  37.000000  37.000000  \n",
              "7307  25.000000  26.000000  58.000000  71.000000  73.000000  75.000000  \n",
              "\n",
              "[7308 rows x 50 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eea046d8-af6a-49be-861a-7b8381e04e05\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>fest</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>76.967742</td>\n",
              "      <td>65.548387</td>\n",
              "      <td>59.419355</td>\n",
              "      <td>60.258065</td>\n",
              "      <td>...</td>\n",
              "      <td>64.645161</td>\n",
              "      <td>46.612903</td>\n",
              "      <td>89.967742</td>\n",
              "      <td>83.451613</td>\n",
              "      <td>90.645161</td>\n",
              "      <td>85.838710</td>\n",
              "      <td>85.612903</td>\n",
              "      <td>88.258065</td>\n",
              "      <td>85.709677</td>\n",
              "      <td>85.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>70.451613</td>\n",
              "      <td>66.467742</td>\n",
              "      <td>35.903226</td>\n",
              "      <td>48.403226</td>\n",
              "      <td>...</td>\n",
              "      <td>56.661290</td>\n",
              "      <td>48.806452</td>\n",
              "      <td>75.209677</td>\n",
              "      <td>79.758065</td>\n",
              "      <td>68.161290</td>\n",
              "      <td>83.096774</td>\n",
              "      <td>81.048387</td>\n",
              "      <td>78.370968</td>\n",
              "      <td>73.306452</td>\n",
              "      <td>70.129032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>77.903226</td>\n",
              "      <td>47.193548</td>\n",
              "      <td>26.451613</td>\n",
              "      <td>33.419355</td>\n",
              "      <td>...</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>46.709677</td>\n",
              "      <td>72.741935</td>\n",
              "      <td>71.161290</td>\n",
              "      <td>65.096774</td>\n",
              "      <td>81.870968</td>\n",
              "      <td>79.225806</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>70.419355</td>\n",
              "      <td>56.548387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>70.806452</td>\n",
              "      <td>48.338710</td>\n",
              "      <td>32.693548</td>\n",
              "      <td>43.193548</td>\n",
              "      <td>...</td>\n",
              "      <td>54.758065</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>75.612903</td>\n",
              "      <td>78.741935</td>\n",
              "      <td>79.419355</td>\n",
              "      <td>84.741935</td>\n",
              "      <td>80.419355</td>\n",
              "      <td>83.516129</td>\n",
              "      <td>75.741935</td>\n",
              "      <td>65.774194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>68.451613</td>\n",
              "      <td>50.924731</td>\n",
              "      <td>25.129032</td>\n",
              "      <td>44.516129</td>\n",
              "      <td>...</td>\n",
              "      <td>50.473118</td>\n",
              "      <td>55.290323</td>\n",
              "      <td>75.645161</td>\n",
              "      <td>81.612903</td>\n",
              "      <td>75.064516</td>\n",
              "      <td>85.870968</td>\n",
              "      <td>80.838710</td>\n",
              "      <td>83.709677</td>\n",
              "      <td>77.677419</td>\n",
              "      <td>66.322581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7303</th>\n",
              "      <td>15</td>\n",
              "      <td>2016</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>22.500000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>61.500000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>25.500000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>45.500000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>38.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7304</th>\n",
              "      <td>16</td>\n",
              "      <td>2016</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>49.500000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>35.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>86.500000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7305</th>\n",
              "      <td>17</td>\n",
              "      <td>2016</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>77.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7306</th>\n",
              "      <td>18</td>\n",
              "      <td>2016</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7307</th>\n",
              "      <td>19</td>\n",
              "      <td>2016</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7308 rows × 50 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eea046d8-af6a-49be-861a-7b8381e04e05')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eea046d8-af6a-49be-861a-7b8381e04e05 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eea046d8-af6a-49be-861a-7b8381e04e05');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "MR6fT9CQFDSN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "8490c7a2-f705-492a-c3a6-ec677ef1c00f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Horas         Agno          Mes      Dia_mes      Dia_sem  \\\n",
              "count  7308.000000  7308.000000  7308.000000  7308.000000  7308.000000   \n",
              "mean     12.500000  2015.500000     6.532567    15.697318     3.005747   \n",
              "std       4.031405     0.500034     3.447136     8.799132     1.414976   \n",
              "min       6.000000  2015.000000     1.000000     1.000000     1.000000   \n",
              "25%       9.000000  2015.000000     4.000000     8.000000     2.000000   \n",
              "50%      12.500000  2015.500000     7.000000    16.000000     3.000000   \n",
              "75%      16.000000  2016.000000    10.000000    23.000000     4.000000   \n",
              "max      19.000000  2016.000000    12.000000    31.000000     5.000000   \n",
              "\n",
              "         fest           20           21           22           23  ...  \\\n",
              "count  7308.0  7308.000000  7308.000000  7308.000000  7308.000000  ...   \n",
              "mean      1.0    71.125408    52.484308    31.633115    44.454830  ...   \n",
              "std       0.0    10.171946    10.552757    10.107581    10.729746  ...   \n",
              "min       1.0     7.000000     9.000000     1.000000     7.500000  ...   \n",
              "25%       1.0    67.000000    46.500000    26.000000    37.500000  ...   \n",
              "50%       1.0    73.000000    52.500000    30.000000    44.500000  ...   \n",
              "75%       1.0    77.000000    58.000000    35.500000    51.000000  ...   \n",
              "max       1.0   108.000000    99.000000    91.000000    96.000000  ...   \n",
              "\n",
              "                58           59           60           61           62  \\\n",
              "count  7308.000000  7308.000000  7308.000000  7308.000000  7308.000000   \n",
              "mean     44.398710    43.690538    65.702178    71.219704    72.261489   \n",
              "std      11.648445     9.835086    16.197768    14.303353    17.169331   \n",
              "min      11.500000     1.000000     6.000000     5.000000     1.000000   \n",
              "25%      36.500000    37.500000    57.000000    65.000000    64.000000   \n",
              "50%      44.000000    43.000000    69.000000    74.500000    77.000000   \n",
              "75%      52.000000    49.000000    77.000000    81.000000    84.090217   \n",
              "max      90.500000    97.000000   109.000000   111.000000   115.000000   \n",
              "\n",
              "                64           65           66           67           68  \n",
              "count  7308.000000  7308.000000  7308.000000  7308.000000  7308.000000  \n",
              "mean     73.287025    73.209006    73.928125    72.313911    60.004856  \n",
              "std      15.242735    14.646374    13.545830    13.915047    17.576601  \n",
              "min       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
              "25%      68.000000    67.500000    68.000000    66.000000    48.000000  \n",
              "50%      77.000000    77.000000    77.000000    76.000000    62.000000  \n",
              "75%      83.500000    83.000000    83.000000    82.000000    73.500000  \n",
              "max     110.000000   109.500000   119.000000   117.000000   112.000000  \n",
              "\n",
              "[8 rows x 50 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ff56d70-4467-4777-9de1-a4e3dbe1fa65\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>fest</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.0</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "      <td>7308.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>12.500000</td>\n",
              "      <td>2015.500000</td>\n",
              "      <td>6.532567</td>\n",
              "      <td>15.697318</td>\n",
              "      <td>3.005747</td>\n",
              "      <td>1.0</td>\n",
              "      <td>71.125408</td>\n",
              "      <td>52.484308</td>\n",
              "      <td>31.633115</td>\n",
              "      <td>44.454830</td>\n",
              "      <td>...</td>\n",
              "      <td>44.398710</td>\n",
              "      <td>43.690538</td>\n",
              "      <td>65.702178</td>\n",
              "      <td>71.219704</td>\n",
              "      <td>72.261489</td>\n",
              "      <td>73.287025</td>\n",
              "      <td>73.209006</td>\n",
              "      <td>73.928125</td>\n",
              "      <td>72.313911</td>\n",
              "      <td>60.004856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.031405</td>\n",
              "      <td>0.500034</td>\n",
              "      <td>3.447136</td>\n",
              "      <td>8.799132</td>\n",
              "      <td>1.414976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.171946</td>\n",
              "      <td>10.552757</td>\n",
              "      <td>10.107581</td>\n",
              "      <td>10.729746</td>\n",
              "      <td>...</td>\n",
              "      <td>11.648445</td>\n",
              "      <td>9.835086</td>\n",
              "      <td>16.197768</td>\n",
              "      <td>14.303353</td>\n",
              "      <td>17.169331</td>\n",
              "      <td>15.242735</td>\n",
              "      <td>14.646374</td>\n",
              "      <td>13.545830</td>\n",
              "      <td>13.915047</td>\n",
              "      <td>17.576601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>7.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>11.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>2015.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>46.500000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>37.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>36.500000</td>\n",
              "      <td>37.500000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>67.500000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>48.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>12.500000</td>\n",
              "      <td>2015.500000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>52.500000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>44.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>74.500000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>62.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>16.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>35.500000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>84.090217</td>\n",
              "      <td>83.500000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>82.000000</td>\n",
              "      <td>73.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>19.000000</td>\n",
              "      <td>2016.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>96.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>90.500000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>111.000000</td>\n",
              "      <td>115.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>109.500000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>112.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 50 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ff56d70-4467-4777-9de1-a4e3dbe1fa65')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ff56d70-4467-4777-9de1-a4e3dbe1fa65 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ff56d70-4467-4777-9de1-a4e3dbe1fa65');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Al tener que el promedio de los dias festivos es 1, significa que todos los dias\n",
        "#Son festivos por lo cual la eliminaremos ya que no aportaria mucha informacion\n",
        "#Debido a que son todos iguales y no hay distintos\n",
        "df.drop(\"fest\",axis=1,inplace=True) #Eliminar este\n",
        "df.head()"
      ],
      "metadata": {
        "id": "L8msfh6XUoOe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "df561e2a-906f-4f48-a1cd-ad8d6f2f5fb0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Horas  Agno  Mes  Dia_mes  Dia_sem         20         21         22  \\\n",
              "0      6  2015    1        1        4  76.967742  65.548387  59.419355   \n",
              "1      7  2015    1        1        4  70.451613  66.467742  35.903226   \n",
              "2      8  2015    1        1        4  77.903226  47.193548  26.451613   \n",
              "3      9  2015    1        1        4  70.806452  48.338710  32.693548   \n",
              "4     10  2015    1        1        4  68.451613  50.924731  25.129032   \n",
              "\n",
              "          23         24  ...         58         59         60         61  \\\n",
              "0  60.258065  78.338710  ...  64.645161  46.612903  89.967742  83.451613   \n",
              "1  48.403226  50.145161  ...  56.661290  48.806452  75.209677  79.758065   \n",
              "2  33.419355  39.629032  ...  59.000000  46.709677  72.741935  71.161290   \n",
              "3  43.193548  59.338710  ...  54.758065  49.000000  75.612903  78.741935   \n",
              "4  44.516129  52.150538  ...  50.473118  55.290323  75.645161  81.612903   \n",
              "\n",
              "          62         64         65         66         67         68  \n",
              "0  90.645161  85.838710  85.612903  88.258065  85.709677  85.096774  \n",
              "1  68.161290  83.096774  81.048387  78.370968  73.306452  70.129032  \n",
              "2  65.096774  81.870968  79.225806  77.000000  70.419355  56.548387  \n",
              "3  79.419355  84.741935  80.419355  83.516129  75.741935  65.774194  \n",
              "4  75.064516  85.870968  80.838710  83.709677  77.677419  66.322581  \n",
              "\n",
              "[5 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c0ec55c-386a-4285-9246-e67d756661e5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>76.967742</td>\n",
              "      <td>65.548387</td>\n",
              "      <td>59.419355</td>\n",
              "      <td>60.258065</td>\n",
              "      <td>78.338710</td>\n",
              "      <td>...</td>\n",
              "      <td>64.645161</td>\n",
              "      <td>46.612903</td>\n",
              "      <td>89.967742</td>\n",
              "      <td>83.451613</td>\n",
              "      <td>90.645161</td>\n",
              "      <td>85.838710</td>\n",
              "      <td>85.612903</td>\n",
              "      <td>88.258065</td>\n",
              "      <td>85.709677</td>\n",
              "      <td>85.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70.451613</td>\n",
              "      <td>66.467742</td>\n",
              "      <td>35.903226</td>\n",
              "      <td>48.403226</td>\n",
              "      <td>50.145161</td>\n",
              "      <td>...</td>\n",
              "      <td>56.661290</td>\n",
              "      <td>48.806452</td>\n",
              "      <td>75.209677</td>\n",
              "      <td>79.758065</td>\n",
              "      <td>68.161290</td>\n",
              "      <td>83.096774</td>\n",
              "      <td>81.048387</td>\n",
              "      <td>78.370968</td>\n",
              "      <td>73.306452</td>\n",
              "      <td>70.129032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>77.903226</td>\n",
              "      <td>47.193548</td>\n",
              "      <td>26.451613</td>\n",
              "      <td>33.419355</td>\n",
              "      <td>39.629032</td>\n",
              "      <td>...</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>46.709677</td>\n",
              "      <td>72.741935</td>\n",
              "      <td>71.161290</td>\n",
              "      <td>65.096774</td>\n",
              "      <td>81.870968</td>\n",
              "      <td>79.225806</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>70.419355</td>\n",
              "      <td>56.548387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70.806452</td>\n",
              "      <td>48.338710</td>\n",
              "      <td>32.693548</td>\n",
              "      <td>43.193548</td>\n",
              "      <td>59.338710</td>\n",
              "      <td>...</td>\n",
              "      <td>54.758065</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>75.612903</td>\n",
              "      <td>78.741935</td>\n",
              "      <td>79.419355</td>\n",
              "      <td>84.741935</td>\n",
              "      <td>80.419355</td>\n",
              "      <td>83.516129</td>\n",
              "      <td>75.741935</td>\n",
              "      <td>65.774194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>2015</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>68.451613</td>\n",
              "      <td>50.924731</td>\n",
              "      <td>25.129032</td>\n",
              "      <td>44.516129</td>\n",
              "      <td>52.150538</td>\n",
              "      <td>...</td>\n",
              "      <td>50.473118</td>\n",
              "      <td>55.290323</td>\n",
              "      <td>75.645161</td>\n",
              "      <td>81.612903</td>\n",
              "      <td>75.064516</td>\n",
              "      <td>85.870968</td>\n",
              "      <td>80.838710</td>\n",
              "      <td>83.709677</td>\n",
              "      <td>77.677419</td>\n",
              "      <td>66.322581</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c0ec55c-386a-4285-9246-e67d756661e5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c0ec55c-386a-4285-9246-e67d756661e5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c0ec55c-386a-4285-9246-e67d756661e5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df\n",
        "correlation_matrix = df.corr()\n",
        "columns_to_keep = ['Horas','Agno','Mes','Dia_mes','Dia_sem']\n",
        "correlation_matrix_subset = correlation_matrix.loc[columns_to_keep, :][df.columns[6:]]\n",
        "colormap = sns.color_palette(\"pastel\")\n",
        "sns.heatmap(correlation_matrix_subset,cmap=colormap)"
      ],
      "metadata": {
        "id": "eQnSnib07r_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "f718c824-623a-4c7b-8d0b-04e8a89dee16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg8AAAGdCAYAAACVY5B3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+E0lEQVR4nO3deVRV5f7H8c9hEBxCIhnCMYcyFbO0DPVqpTkmoa5EM+ccKqzEm4ENahNWTmV2LS3NcijTzPJqmmMaOeCYkuYU9yo4REJKIcLz++Mu+XViOGcfDna092utsxbs/ezv/vLAqY97OjZjjBEAAICTvP7qBgAAwJWF8AAAACwhPAAAAEsIDwAAwBLCAwAAsITwAAAALCE8AAAASwgPAADAEsIDAACwxOevbqDAnFkOh+Re28nhGN+ode7oRpK06lxXh2PaV/rC4ZjcZXc7HONs37t2N3I4puFPwW7bnzOcmSdnODOXzvz8znJmnq5U+ytWdVutW/7r+L25u9rDbqmj4DTH+/J7znEdJ/c3uX05h2Pidv7kcMzc4Djnesqq6HDMmgZzHY5pu7+fwzEBcmK+JWXJPb87Z/4GnOnphv86HPI/AxzvrzRW/Zrptlrtr6nstlqegiMPAADAEsIDAACwhPAAAAAsITwAAABLCA8AAMASwgMAALCE8AAAACwhPAAAAEsIDwAAwBLCAwAAsITwAAAALCE8AAAASwgPAADAEsIDAACwhPAAAAAsITwAAABLCA8AAMASwgMAALCE8AAAACxxOTzs2LFDe/fuLfj+888/V3R0tMaMGaMLFy64pTkAAOB5XA4Pw4YN08GDByVJR44cUa9evVShQgUtWrRIo0ePdluDAADAs7gcHg4ePKgmTZpIkhYtWqTWrVtr/vz5mjNnjhYvXuyu/gAAgIdxOTwYY5Sfny9J+vrrr9W5c2dJUvXq1XXmzBn3dAcAADyOy+GhWbNmeumll/Thhx9qw4YN6tKliyTp6NGjCg0NdVuDAADAs7gcHqZOnaodO3YoNjZWzzzzjOrWrStJ+vTTT9WiRQu3NQgAADyLj6sbNm7c2O5ui0tef/11eXt7l6opAADguVwOD8Xx9/d3d0kAAOBBXA4PeXl5mjJlij755BOlpqYWerZDRkZGqZsDAACex+VrHsaPH6/JkycrJiZGmZmZiouLU/fu3eXl5aVx48a5sUUAAOBJXA4P8+bN08yZMzVq1Cj5+Piod+/emjVrlp5//nl999137uwRAAB4EJfDQ3p6uiIiIiRJlSpVUmZmpiTpvvvu0/Lly93THQAA8Dguh4dq1aopLS1NklSnTh2tWrVKkrRt2zb5+fm5pzsAAOBxXA4P3bp105o1ayRJI0aM0HPPPad69eqpX79+GjRokNsaBAAAnsXluy0mTJhQ8HVMTIxq1qypb7/9VvXq1VPXrl3d0hwAAPA8Lh15yM3N1aBBg3T06NGCZXfeeafi4uIIDgAAlNL06dNVq1Yt+fv7q3nz5tq6dWuxY/ft26cePXqoVq1astlsmjp1apn351J48PX1LdUnZ+bk5CgrK8vulZOb63I9AACuFh9//LHi4uI0duxY7dixQ7fccos6dOigU6dOFTk+OztbtWvX1oQJExQWFnZZenT5tEV0dLSWLl2qkSNHWt42MTFR48ePt1s29v6uGhcd5Wo7AAC4TftKX7ix2kOWRk+ePFlDhgzRwIEDJUkzZszQ8uXL9f777ys+Pr7Q+Ntvv1233367JBW5viy4HB7q1aunF154QZs3b1bTpk1VsWJFu/WPP/54sdsmJCQoLi7Obpnfx/NcbQUAAI+Vk5OjnJwcu2V+fn5F3pl44cIFJScnKyEhoWCZl5eX2rVrp6SkpDLv1Vkuh4f33ntPgYGBSk5OVnJyst06m81WYngoctJ8fV1tBQAAj1Xk0faxY4t8GvOZM2eUl5en0NBQu+WhoaH64YcfyrJNS1wOD3+8WBIAABStyKPtV/jzkNzyqZrGGEn/O+IAAAD+X3GnKIpSpUoVeXt76+TJk3bLT548edkuhnSGyw+JkqS5c+cqIiJC5cuXV/ny5dW4cWN9+OGH7uoNAIC/lXLlyqlp06YFD2GUpPz8fK1Zs0aRkZF/YWf2XD7yMHnyZD333HOKjY1Vy5YtJUmbNm3S8OHDdebMGZfuwgAA4O8uLi5O/fv3V7NmzXTHHXdo6tSpOn/+fMHdF/369VPVqlWVmJgo6X8XWe7fv7/g6+PHj2vXrl2qVKmS6tatWyY9uhwepk2bpn/961/q169fwbKoqCg1bNhQ48aNIzwAAOCCmJgYnT59Ws8//7zS09PVpEkTrVy5suAiytTUVHl5/f+JgxMnTujWW28t+H7ixImaOHGi2rRpo/Xr15dJjy6Hh7S0NLVo0aLQ8hYtWhR8YBYAALAuNjZWsbGxRa77cyCoVatWwbWHl4vL1zzUrVtXn3zySaHlH3/8serVq1eqpgAAgOdy+cjD+PHjFRMTo40bNxZc87B582atWbOmyFABAACuDi4feejRo4e2bNmiKlWqaOnSpVq6dKmqVKmirVu3qlu3bu7sEQAAeBDLRx6ysrIKvq5Xr57efvvtIscEBASUrjMAAOCRLIeHwMBApx4GlZeX51JDAADAs1kOD+vWrSv42hijzp07a9asWapatapbGwMAAJ7Jcnho06aN3ffe3t668847Vbt2bbc1BQAAPFepHk8NAAD+fggPAADAEreEBz5NEwCAvw/L1zx0797d7vvff/9dw4cPV8WKFe2WL1mypHSdAQAAj2Q5PFSuXNnu+4ceeshtzQAAAM9nOTzMnj27LPoAAABXCC6YBAAAlhAeAACAJYQHAABgCeEBAABYQngAAACWEB4AAIAlhAcAAGAJ4QEAAFhCeAAAAJYQHgAAgCWEBwAAYAnhAQAAWEJ4AAAAlhAeAACAJYQHAABgic0YY/7qJiRp165dDsdUPrPd4ZgsPezU/hqcP+7UOEf+W3GFwzHVzndyS53LzZm+Jff17uz+nLGv5mmHYxr+FOyWOk1u+d7hmKNrfnc4RnL+79cd3PUekNw3T87YtbuRW+o4y11/J+7kTE/Ovi/d+b5zB9+odc4NtD1Upn048/8kZzVp0sRttTwFRx4AAIAlhAcAAGAJ4QEAAFhCeAAAAJYQHgAAgCWEBwAAYAnhAQAAWEJ4AAAAlhAeAACAJYQHAABgCeEBAABYQngAAACWEB4AAPAw06dPV61ateTv76/mzZtr69atJY5ftGiR6tevL39/f0VEROjf//53mfZHeAAAwIN8/PHHiouL09ixY7Vjxw7dcsst6tChg06dOlXk+G+//Va9e/fW4MGDtXPnTkVHRys6Olrff++eT7ItCuEBAAAPMnnyZA0ZMkQDBw5UgwYNNGPGDFWoUEHvv/9+kePfeOMNdezYUU899ZRuvvlmvfjii7rtttv01ltvlVmPhAcAAMpQTk6OsrKy7F45OTlFjr1w4YKSk5PVrl27gmVeXl5q166dkpKSitwmKSnJbrwkdejQodjx7kB4AACgDCUmJqpy5cp2r8TExCLHnjlzRnl5eQoNDbVbHhoaqvT09CK3SU9PtzTeHXzKrDIAAFBCQoLi4uLslvn5+f1F3bgH4QEAgDLk5+fndFioUqWKvL29dfLkSbvlJ0+eVFhYWJHbhIWFWRrvDpy2AADAQ5QrV05NmzbVmjVrCpbl5+drzZo1ioyMLHKbyMhIu/GStHr16mLHuwNHHgAA8CBxcXHq37+/mjVrpjvuuENTp07V+fPnNXDgQElSv379VLVq1YLrJp544gm1adNGkyZNUpcuXbRw4UJt375d7777bpn16FJ4OHz4sKZOnaqUlBRJUoMGDfTEE0+oTp06bm0OAIC/m5iYGJ0+fVrPP/+80tPT1aRJE61cubLgosjU1FR5ef3/iYMWLVpo/vz5evbZZzVmzBjVq1dPS5cuVaNGjcqsR8vh4auvvlJUVJSaNGmili1bSpI2b96shg0b6osvvtC9997r9iYBAPg7iY2NVWxsbJHr1q9fX2jZAw88oAceeKCMu/p/lsNDfHy8Ro4cqQkTJhRa/vTTTxMeAAC4ylm+YDIlJUWDBw8utHzQoEHav3+/W5oCAACey3J4CA4O1q5duwot37Vrl0JCQtzREwAA8GCWT1sMGTJEQ4cO1ZEjR9SiRQtJ/7vm4dVXXy30EAwAAHD1sRwennvuOV1zzTWaNGmSEhISJEnh4eEaN26cHn/8cbc3CAAAPIvl8GCz2TRy5EiNHDlSv/76qyTpmmuucXtjAADAM5XqIVGEBgAA/n4sXzB58uRJ9e3bV+Hh4fLx8ZG3t7fdCwAAXN0sH3kYMGCAUlNT9dxzz+n666+XzWYri74AAICHshweNm3apG+++UZNmjQpg3YAAICns3zaonr16jLGlEUvAADgCmA5PEydOlXx8fE6duxYGbQDAAA8neXTFjExMcrOzladOnVUoUIF+fr62q3PyMhwWCMnJ0c5OTl2yy5cuKBy5cpZbQcAAFxmlsPD1KlTS73TxMREjR8/3m7ZsGHDNHz48FLXBgCgtBr+FOy+Yk3cV8pTWA4P/fv3L/VOExISCj3K+ocffih1XQAAUPYsh4esrKwil9tsNvn5+Tl16sHPz09+fn52yzhlAQDAlcFyeAgMDCzx2Q7VqlXTgAEDNHbsWHl5Wb4eEwAAeDjL4WHOnDl65plnNGDAAN1xxx2SpK1bt+qDDz7Qs88+q9OnT2vixIny8/PTmDFj3N4wAAD4a1kODx988IEmTZqknj17Fizr2rWrIiIi9M4772jNmjWqUaOGXn75ZcIDAABXIcvnFb799lvdeuuthZbfeuutSkpKkiS1atVKqamppe8OAAB4HJeeMPnee+8VWv7ee++pevXqkqSff/5Z1157bem7AwAAHsfyaYuJEyfqgQce0IoVK3T77bdLkrZv366UlBQtXrxYkrRt2zbFxMS4t1MAAOARLIeHqKgoHThwQDNmzNDBgwclSZ06ddLSpUt17tw5SdIjjzzi3i4BAIDHsBweJKlWrVqaMGGCpP8992HBggWKiYnR9u3blZeX59YGAQCAZ3H5QQwbN25U//79FR4erkmTJunuu+/Wd999587eAACAB7J05CE9PV1z5szRe++9p6ysLPXs2VM5OTlaunSpGjRoUFY9AgAAD+L0kYeuXbvqpptu0p49ezR16lSdOHFC06ZNK8veAACAB3L6yMOKFSv0+OOP65FHHlG9evXKsicAAODBnD7ysGnTJv36669q2rSpmjdvrrfeektnzpwpy94AAIAHcjo83HnnnZo5c6bS0tI0bNgwLVy4UOHh4crPz9fq1av166+/lmWfAADAQ1i+26JixYoaNGiQNm3apL1792rUqFGaMGGCQkJCFBUVVRY9AgAAD1Kqz8y+6aab9Nprr+m///2vFixY4K6eAACABytVeLjE29tb0dHRWrZsmTvKAQAAD+aW8AAAAP4+CA8AAMASwgMAALCE8AAAACwhPAAAAEsIDwAAwBLCAwAAsITwAAAALCE8AABwBcrIyFCfPn0UEBCgwMBADR48WOfOnStxm3fffVd33XWXAgICZLPZdPbsWZf2TXgAAOAK1KdPH+3bt0+rV6/Wl19+qY0bN2ro0KElbpOdna2OHTtqzJgxpdq3T6m2BgAAl11KSopWrlypbdu2qVmzZpKkadOmqXPnzpo4caLCw8OL3O7JJ5+UJK1fv75U++fIAwAAZSgnJ0dZWVl2r5ycnFLVTEpKUmBgYEFwkKR27drJy8tLW7ZsKW3LDhEeAAAoQ4mJiapcubLdKzExsVQ109PTFRISYrfMx8dHQUFBSk9PL1VtZxAeAAAoQwkJCcrMzLR7JSQkFDk2Pj5eNputxNcPP/xwmX+CwrjmAQCAMuTn5yc/Pz+nxo4aNUoDBgwocUzt2rUVFhamU6dO2S2/ePGiMjIyFBYW5mqrTiM8AADgIYKDgxUcHOxwXGRkpM6ePavk5GQ1bdpUkrR27Vrl5+erefPmZd0mpy0AALjS3HzzzerYsaOGDBmirVu3avPmzYqNjVWvXr0K7rQ4fvy46tevr61btxZsl56erl27dunQoUOSpL1792rXrl3KyMiwtH+POfLQ8CfHSUvq5HDE/orO7W9/xaoOxwRolnPFHPhvxRVuqeOpqp13/Hv5u8us0szxIEnSLrfsz7n30+W1a3cjh2Mqn9nueIwcj3Gvy/v37a7f3ZX6vsxddrdT43zvL9s+3Pnf7Rv0sNtq/dG8efMUGxurtm3bysvLSz169NCbb75ZsD43N1cHDhxQdnZ2wbIZM2Zo/PjxBd+3bt1akjR79myHp0v+yGPCAwAAcF5QUJDmz59f7PpatWrJGGO3bNy4cRo3blyp981pCwAAYAnhAQAAWEJ4AAAAlhAeAACAJYQHAABgCeEBAABYQngAAACWEB4AAIAlhAcAAGAJ4QEAAFhCeAAAAJYQHgAAgCWEBwAAYAnhAQAAWEJ4AAAAlhAeAACAJYQHAABgCeEBAABYQngAAACWEB4AAIAlhAcAAGAJ4QEAAFhCeAAAAJYQHgAAgCWEBwAAYInL4eGDDz7Q8uXLC74fPXq0AgMD1aJFC/30009uaQ4AAHgel8PDK6+8ovLly0uSkpKSNH36dL322muqUqWKRo4c6bYGAQCAZ/FxdcP//Oc/qlu3riRp6dKl6tGjh4YOHaqWLVvqrrvucld/AADAw7h85KFSpUr6+eefJUmrVq3SvffeK0ny9/fXb7/95p7uAACAx3H5yMO9996rhx9+WLfeeqsOHjyozp07S5L27dunWrVquas/AADgYVw+8jB9+nRFRkbq9OnTWrx4sa677jpJUnJysnr37u22BgEAgGdx+chDYGCg3nrrrULLx48fX6qGAACAZyvVcx6++eYbPfTQQ2rRooWOHz8uSfrwww+1adMmtzQHAAA8j8vhYfHixerQoYPKly+vHTt2KCcnR5KUmZmpV155pcRtc3JylJWVZffKyc1xtRUAAHAZuXza4qWXXtKMGTPUr18/LVy4sGB5y5Yt9dJLL5W4bWJiYqHTG8/GjNTzvUe52g4AAG6TpYf/6hY8msvh4cCBA2rdunWh5ZUrV9bZs2dL3DYhIUFxcXF2y7xWnXG1FQAAcBm5HB7CwsJ06NChQrdlbtq0SbVr1y5xWz8/P/n5+dkty/X91dVWAADAZeTyNQ9DhgzRE088oS1btshms+nEiROaN2+e/vnPf+qRRx5xZ48AAMCDuHzkIT4+Xvn5+Wrbtq2ys7PVunVr+fn56Z///KdGjBjhzh4BAIAHcTk82Gw2PfPMM3rqqad06NAhnTt3Tg0aNFClSpXc2R8AAPAwlsPDoEGDnBr3/vvvW24GAAB4PsvXPMyZM0fr1q3T2bNn9csvvxT7AgAAZScjI0N9+vRRQECAAgMDNXjwYJ07d67E8SNGjNBNN92k8uXLq0aNGnr88ceVmZlped+Wjzw88sgjWrBggY4ePaqBAwfqoYceUlBQkOUdAwAA1/Xp00dpaWlavXq1cnNzNXDgQA0dOlTz588vcvyJEyd04sQJTZw4UQ0aNNBPP/2k4cOH68SJE/r0008t7dvykYfp06crLS1No0eP1hdffKHq1aurZ8+e+uqrr2SMsVoOAABYlJKSopUrV2rWrFlq3ry5WrVqpWnTpmnhwoU6ceJEkds0atRIixcvVteuXVWnTh3dc889evnll/XFF1/o4sWLlvbv0q2afn5+6t27t1avXq39+/erYcOGevTRR1WrVq0SD5kAAPB3U+RHMuSU7iMZkpKSFBgYqGbNmhUsa9eunby8vLRlyxan62RmZiogIEA+PtZORJTqg7EkycvLSzabTcYY5eXllbYcAABXlcTERFWuXNnulZiYWKqa6enpCgkJsVvm4+OjoKAgpaenO1XjzJkzevHFFzV06FDL+3cpPOTk5GjBggW69957deONN2rv3r166623lJqayq2aAAD8QUJCgjIzM+1eCQkJRY6Nj4+XzWYr8fXDDz+UuqesrCx16dJFDRo00Lhx4yxvb/mCyUcffVQLFy5U9erVNWjQIC1YsEBVqlSxvGMAAP4OivpIhuKMGjVKAwYMKHFM7dq1FRYWplOnTtktv3jxojIyMhQWFlbi9r/++qs6duyoa665Rp999pl8fX2d6u2PLIeHGTNmqEaNGqpdu7Y2bNigDRs2FDluyZIllpsBAODvLDg4WMHBwQ7HRUZG6uzZs0pOTlbTpk0lSWvXrlV+fr6aN29e7HZZWVnq0KGD/Pz8tGzZMvn7+7vUp+Xw0K9fP9lsNpd2BgAASu/mm29Wx44dNWTIEM2YMUO5ubmKjY1Vr169FB4eLkk6fvy42rZtq7lz5+qOO+5QVlaW2rdvr+zsbH300UcFF29K/wst3t7eTu/fcniYM2eO1U0AAICbzZs3T7GxsWrbtq28vLzUo0cPvfnmmwXrc3NzdeDAAWVnZ0uSduzYUXAnRt26de1qHT16tNCnZJfE5c+2AAAAf52goKBiHwglSbVq1bJ7/tJdd93ltucxlfpWTQAA8PdCeAAAAJYQHgAAgCWEBwAAYAnhAQAAWEJ4AAAAlhAeAACAJYQHAABgCeEBAABYQngAAACWEB4AAIAlhAcAAGAJ4QEAAFhCeAAAAJYQHgAAgCWEBwAAYAnhAQAAWEJ4AAAAlhAeAACAJTZjjPmrm5CkXbt2uaVOw5+C3VJHkvbVPO2W/TlTB1c3Z/8u3fW3wt8lrnZNmjQp2x3MmeW+WgMedl8tD8GRBwAAYAnhAQAAWEJ4AAAAlhAeAACAJYQHAABgCeEBAABYQngAAACWEB4AAIAlhAcAAGAJ4QEAAFjicnjYsWOH9u7dW/D9559/rujoaI0ZM0YXLlxwS3MAAMDzuBwehg0bpoMHD0qSjhw5ol69eqlChQpatGiRRo8e7bYGAQCAZ3E5PBw8eLDgg0kWLVqk1q1ba/78+ZozZ44WL17srv4AAICHcTk8GGOUn58vSfr666/VuXNnSVL16tV15swZ93QHAAA8jsvhoVmzZnrppZf04YcfasOGDerSpYsk6ejRowoNDXVbgwAAwLO4HB6mTp2qHTt2KDY2Vs8884zq1q0rSfr000/VokULtzUIAAA8i4+rGzZu3NjubotLXn/9dXl7e5eqKQAA4LlK9ZyHs2fPatasWUpISFBGRoYkaf/+/Tp16pRbmgMAAJ7H5SMPe/bsUdu2bRUYGKhjx45pyJAhCgoK0pIlS5Samqq5c+e6s08AAOAhXD7yEBcXp4EDB+rHH3+Uv79/wfLOnTtr48aNbmkOAAB4HpfDw7Zt2zRs2LBCy6tWrar09PRSNQUAADyXy+HBz89PWVlZhZYfPHhQwcHBpWoKAACULCMjQ3369FFAQIACAwM1ePBgnTt3rsRthg0bpjp16qh8+fIKDg7W/fffrx9++MHyvl0OD1FRUXrhhReUm5srSbLZbEpNTdXTTz+tHj16uFoWAAA4oU+fPtq3b59Wr16tL7/8Uhs3btTQoUNL3KZp06aaPXu2UlJS9NVXX8kYo/bt2ysvL8/Svl0OD5MmTdK5c+cUEhKi3377TW3atFHdunV1zTXX6OWXX3a1LAAAcCAlJUUrV67UrFmz1Lx5c7Vq1UrTpk3TwoULdeLEiWK3Gzp0qFq3bq1atWrptttu00svvaT//Oc/OnbsmKX9u3y3ReXKlbV69Wpt2rRJe/bs0blz53TbbbepXbt2rpYEAOCqk5OTo5ycHLtlfn5+8vPzc7lmUlKSAgMD1axZs4Jl7dq1k5eXl7Zs2aJu3bo5rHH+/HnNnj1bN9xwg6pXr25p/6V6zoMktWrVSo8++qhGjx5NcAAA4E8SExNVuXJlu1diYmKpaqanpyskJMRumY+Pj4KCghzetPD222+rUqVKqlSpklasWKHVq1erXLlylvbv8pEH6X93XKxbt06nTp0q+JCsSyZPnlya0gAAXBUSEhIUFxdnt6y4ow7x8fF69dVXS6yXkpJSqn769Omje++9V2lpaZo4caJ69uypzZs32z12wRGXw8Mrr7yiZ599VjfddJNCQ0Nls9kK1v3xawAA/s6snKIYNWqUBgwYUOKY2rVrKywsrNDTnC9evKiMjAyFhYWVuP2lox/16tXTnXfeqWuvvVafffaZevfu7VSPUinCwxtvvKH333/f4Q8JAACcExwc7NTjDiIjI3X27FklJyeradOmkqS1a9cqPz9fzZs3d3p/xhgZYwpdk+GIy9c8eHl5qWXLlq5uDgAAXHTzzTerY8eOGjJkiLZu3arNmzcrNjZWvXr1Unh4uCTp+PHjql+/vrZu3SpJOnLkiBITE5WcnKzU1FR9++23euCBB1S+fHl17tzZ0v5dDg8jR47U9OnTXd0cAACUwrx581S/fn21bdtWnTt3VqtWrfTuu+8WrM/NzdWBAweUnZ0tSfL399c333yjzp07q27duoqJidE111yjb7/9ttDFl464fNrin//8p7p06aI6deqoQYMG8vX1tVu/ZMkSV0sDAAAHgoKCNH/+/GLX16pVS8aYgu/Dw8P173//2y37djk8PP7441q3bp3uvvtuXXfddVwkCQDA34TL4eGDDz7Q4sWL1aVLF3f2AwAAPJzL1zwEBQWpTp067uwFAABcAVwOD+PGjdPYsWMLLsSwIicnR1lZWXavCxcuuNoKAAC4jFw+bfHmm2/q8OHDCg0NVa1atQpdMLljx45it01MTNT48ePtlg0bNkzDhw93tR0AANxmcntrj2suSZzjIVccl8NDdHS0yzst6lGdrnyeOAAAuPxcDg9jx451atyCBQsUFRWlihUrFiwr6lGdVj+UAwAA/DVK/amajgwbNkwnT54s690AAIDLpMzDwx8fUAEAAK58ZR4eAADA1YXwAAAALCE8AAAASwgPAADAkjIPDzVr1iz0ACkAAHDlcvk5D876/vvvy3oXAADgMnI5POTl5WnKlCn65JNPlJqaWuizKTIyMkrdHAAA8Dwun7YYP368Jk+erJiYGGVmZiouLk7du3eXl5eXxo0b58YWAQCAJ3E5PMybN08zZ87UqFGj5OPjo969e2vWrFl6/vnn9d1337mzRwAA4EFcDg/p6emKiIiQJFWqVEmZmZmSpPvuu0/Lly93T3cAAMDjuBweqlWrprS0NElSnTp1tGrVKknStm3bCn3oFQAAuHq4HB66deumNWvWSJJGjBih5557TvXq1VO/fv00aNAgtzUIAAA8i8t3W0yYMKHg65iYGNWoUUNJSUmqV6+eunbt6pbmAACA53Hbcx4iIyMVGRnprnIAAMBDWQoPy5YtU6dOneTr66tly5aVODYqKqpUjQEAAM9kKTxER0crPT1dISEhio6OLnaczWZTXl5eaXsDAAAeyFJ4yM/PL/JrAADw9+HSNQ/5+fmaM2eOlixZomPHjslms6l27drq0aOH+vbtK5vN5u4+AQCAh7B8q6YxRlFRUXr44Yd1/PhxRUREqGHDhjp27JgGDBigbt26lUWfAADAQ1g+8jBnzhxt3LhRa9as0d133223bu3atYqOjtbcuXPVr18/tzUJAAA8h+UjDwsWLNCYMWMKBQdJuueeexQfH6958+a5pTkAAOB5LIeHPXv2qGPHjsWu79Spk3bv3l2qpgAAgOeyHB4yMjIUGhpa7PrQ0FD98ssvpWoKAAB4LsvhIS8vTz4+xV8q4e3trYsXL5aqKQAA4LksXzBpjNGAAQOK/eTMnJycUjcFAAA8l+Xw0L9/f4djuNMCAICrl+XwMHv27LLoAwAAXCEsX/MAAAD+3ggPAADAEsIDAACwhPAAAAAsITwAAABLbMYY81c3IUm5nx93OGZfzdMOx1Q+s92p/VU738mpcbh8nPn9Nvwp+DJ0Yo0zfXsid86lJ/7uPLGny8nZv8srdQ58769atjtY/qL7anV5zn21/iAjI0MjRozQF198IS8vL/Xo0UNvvPGGKlWq5HBbY4w6d+6slStX6rPPPlN0dLSlfXPkAQCAK1CfPn20b98+rV69Wl9++aU2btyooUOHOrXt1KlTZbPZXN635ec8AACAv1ZKSopWrlypbdu2qVmzZpKkadOmqXPnzpo4caLCw8OL3XbXrl2aNGmStm/fruuvv96l/XPkAQCAMpSTk6OsrCy7V2k/yiEpKUmBgYEFwUGS2rVrJy8vL23ZsqXY7bKzs/Xggw9q+vTpCgsLc3n/hAcAAMpQYmKiKleubPdKTEwsVc309HSFhITYLfPx8VFQUJDS09OL3W7kyJFq0aKF7r///lLtn9MWAACUoYSEBMXFxdktK+7DJePj4/Xqq6+WWC8lJcWlPpYtW6a1a9dq586dLm3/R4QHAADKkJ+fX7Fh4c9GjRqlAQMGlDimdu3aCgsL06lTp+yWX7x4URkZGcWejli7dq0OHz6swMBAu+U9evTQP/7xD61fv96pHiXCAwAAHiM4OFjBwY5vn42MjNTZs2eVnJyspk2bSvpfOMjPz1fz5s2L3CY+Pl4PP/yw3bKIiAhNmTJFXbt2tdQn4QEAgCvMzTffrI4dO2rIkCGaMWOGcnNzFRsbq169ehXcaXH8+HG1bdtWc+fO1R133KGwsLAij0rUqFFDN9xwg6X9c8EkAABXoHnz5ql+/fpq27atOnfurFatWundd98tWJ+bm6sDBw4oOzvb7fvmyAMAAFegoKAgzZ8/v9j1tWrVkqOHSLv6kOlSh4dTp07p1KlTys/Pt1veuHHj0pYGAAAeyOXwkJycrP79+yslJaUgudhsNhljZLPZlJeX57YmAQCA53A5PAwaNEg33nij3nvvPYWGhpbqGdkAAODK4XJ4OHLkiBYvXqy6deu6sx8AAODhXL7bom3bttq9e7c7ewEAAFcAl488zJo1S/3799f333+vRo0aydfX1259VFRUqZsDAACex+XwkJSUpM2bN2vFihWF1nHBJAAAVy+XT1uMGDFCDz30kNLS0pSfn2/3IjgAAHD1cjk8/Pzzzxo5cqRCQ0Pd2Q8AAPBwLoeH7t27a926de7sBQAAXAFcvubhxhtvVEJCgjZt2qSIiIhCF0w+/vjjpW4OAAB4nlLdbVGpUiVt2LBBGzZssFtns9kIDwAAXKVcDg9Hjx51Zx8AAOAKUeqP5L5w4YIOHDigixcvuqMfAADg4VwOD9nZ2Ro8eLAqVKighg0bKjU1VdL/buGcMGGC2xoEAACexeXwkJCQoN27d2v9+vXy9/cvWN6uXTt9/PHHbmkOAAB4HpeveVi6dKk+/vhj3XnnnXafqNmwYUMdPnzYLc0BAADP4/KRh9OnTyskJKTQ8vPnz/Px3AAAXMVcDg/NmjXT8uXLC76/FBhmzZqlyMjI0ncGAAA8ksunLV555RV16tRJ+/fv18WLF/XGG29o//79+vbbbws99wEAAFw9XD7y0KpVK+3atUsXL15URESEVq1apZCQECUlJalp06bu7BEAAHgQl488SFKdOnU0c+ZMd/UCAACuAC4fedixY4f27t1b8P3nn3+u6OhojRkzRhcuXHBLcwAAwPO4HB6GDRumgwcPSpKOHDmimJgYVahQQYsWLdLo0aPd1iAAAPAsLoeHgwcPqkmTJpKkRYsWqU2bNpo/f77mzJmjxYsXu6s/AADgYVy+5sEYo/z8fEnS119/rfvuu0+SVL16dZ05c6bEbXNycpSTk2O3zCs3R36+fq62AwAALhOXw0OzZs300ksvqV27dtqwYYP+9a9/Sfrfp22GhoaWuG1iYqLGjx9vt+zZmJF6vvcoV9sBAMBt5gbHua1WP7dV8hwuh4epU6eqT58+Wrp0qZ555hnVrVtXkvTpp5+qRYsWJW6bkJCguDj7X4zXqpKPVgAAAM/gcnho3Lix3d0Wl7z++uvy9vYu+H7BggWKiopSxYoVC5b5+fnJz8/+FEWu76+utgIAAC4jly+YLI6/v798fX0Lvh82bJhOnjzp7t0AAIC/iNvDw58ZY8p6FwAA4DIq8/AAAACuLoQHAABgCeEBAABYQngAAACWlHl4qFmzpt3dFwAA4MpWqo/kdsb3339f1rsAAACXkcvhIS8vT1OmTNEnn3yi1NTUQh/DnZGRUermAACA53H5tMX48eM1efJkxcTEKDMzU3Fxcerevbu8vLw0btw4N7YIAAA8icvhYd68eZo5c6ZGjRolHx8f9e7dW7NmzdLzzz+v7777zp09AgAAD+JyeEhPT1dERIQkqVKlSsrMzJQk3XfffVq+fLl7ugMAAB7H5fBQrVo1paWlSZLq1KmjVatWSZK2bdtW6EOvAACAe2VkZKhPnz4KCAhQYGCgBg8erHPnzpW4zV133SWbzWb3Gj58uOV9uxweunXrpjVr1kiSRowYoeeee0716tVTv379NGjQIFfLAgAAJ/Tp00f79u3T6tWr9eWXX2rjxo0aOnSow+2GDBmitLS0gtdrr71med8u320xYcKEgq9jYmJUo0YNJSUlqV69eurataurZQEAgAMpKSlauXKltm3bpmbNmkmSpk2bps6dO2vixIkKDw8vdtsKFSooLCysVPt320OiIiMjFRcXR3AAAOAPcnJylJWVZffKyckpVc2kpCQFBgYWBAdJateunby8vLRly5YSt503b56qVKmiRo0aKSEhQdnZ2Zb3b+nIw7Jly9SpUyf5+vpq2bJlJY6Nioqy3AwAAFebxMREjR8/3m7Z2LFjS/VYg/T0dIWEhNgt8/HxUVBQkNLT04vd7sEHH1TNmjUVHh6uPXv26Omnn9aBAwe0ZMkSS/u3FB6io6MLGo6Oji52nM1mU15enqVGAAC4GiUkJCguLs5uWXE3FsTHx+vVV18tsV5KSorLvfzxmoiIiAhdf/31atu2rQ4fPqw6deo4XcdSeMjPzy/yawAAUDQ/Pz+n70IcNWqUBgwYUOKY2rVrKywsTKdOnbJbfvHiRWVkZFi6nqF58+aSpEOHDpVdeLgkPz9fc+bM0ZIlS3Ts2DHZbDbVrl1bPXr0UN++fWWz2VwpCwDA31pwcLCCg4MdjouMjNTZs2eVnJyspk2bSpLWrl2r/Pz8gkDgjF27dkmSrr/+ekt9Wr5g0hijqKgoPfzwwzp+/LgiIiLUsGFDHTt2TAMGDFC3bt2slgQAABbcfPPN6tixo4YMGaKtW7dq8+bNio2NVa9evQrutDh+/Ljq16+vrVu3SpIOHz6sF198UcnJyTp27JiWLVumfv36qXXr1mrcuLGl/Vs+8jBnzhxt3LhRa9as0d133223bu3atYqOjtbcuXPVr18/q6UBAICT5s2bp9jYWLVt21ZeXl7q0aOH3nzzzYL1ubm5OnDgQMHdFOXKldPXX3+tqVOn6vz586pevbp69OihZ5991vK+LYeHBQsWaMyYMYWCgyTdc889io+P17x58wgPAACUoaCgIM2fP7/Y9bVq1ZIxpuD76tWra8OGDW7Zt+XTFnv27FHHjh2LXd+pUyft3r27VE0BAADPZTk8ZGRkKDQ0tNj1oaGh+uWXX0rVFAAA8FyWw0NeXp58fIo/2+Ht7a2LFy+WqikAAOC5LF/zYIzRgAEDir1ntbSP3AQAAJ7Ncnjo37+/wzFcLAkAwNXLcniYPXt2WfQBAACuEG77VE0AAPD3QHgAAACWEB4AAIAlhAcAAGAJ4QEAAFhjPNDvv/9uxo4da37//XePqUVP9OSJPV3NPxs90ZO7e4L72Iz5w6dmeIisrCxVrlxZmZmZCggI8Iha9ERPntjT1fyz0RM9ubsnuA+nLQAAgCWEBwAAYAnhAQAAWOKR4cHPz09jx44t9sO3/opa9ERPntjT1fyz0RM9ubsnuI9HXjAJAAA8l0ceeQAAAJ6L8AAAACwhPAAAAEsIDwAAwJK/NDwkJibq9ttv1zXXXKOQkBBFR0frwIEDdmPeffdd3XXXXQoICJDNZtPZs2ddqjNs2DDVqVNH5cuXV3BwsO6//3798MMPLtW66667ZLPZ7F7Dhw+3VOfYsWOFalx6LVq0yFI/hw8fVrdu3RQcHKyAgAD17NlTJ0+eLPSz/etf/1Ljxo0VEBCggIAARUZGasWKFZKkjIwMjRgxQjfddJPKly+vGjVq6PHHH1dmZmahOo5qOTtHztRJT09X3759FRYWpooVK+q2227T4sWLi+zpjyZMmCCbzaYnn3yyYNnvv/+uxx57TNddd50qVaqkHj16FDlPjmpZnauSerrEGKNOnTrJZrNp6dKlLtVJSkrSPffco4oVKyogIECtW7fWb7/9ZrmWs3M+bty4Qr/j+vXrF6x3dr5LqmN1rh31dImj+S6pjrPvXWf6cfa9e8nx48f10EMP6brrrlP58uUVERGh7du3F6xfsmSJ2rdvr+uuu042m027du1yqc4fDR8+XDabTVOnTnW5VkpKiqKiolS5cmVVrFhRt99+u1JTUy3VOXfunGJjY1WtWjWVL19eDRo00IwZM4qdK5StvzQ8bNiwQY899pi+++47rV69Wrm5uWrfvr3Onz9fMCY7O1sdO3bUmDFjSlWnadOmmj17tlJSUvTVV1/JGKP27dsrLy/Pci1JGjJkiNLS0gper732mqU61atXt9s+LS1N48ePV6VKldSpUyen65w/f17t27eXzWbT2rVrtXnzZl24cEFdu3ZVfn6+XU/VqlXThAkTlJycrO3bt+uee+7R/fffr3379unEiRM6ceKEJk6cqO+//15z5szRypUrNXjw4CLnvKRazs6RM3X69eunAwcOaNmyZdq7d6+6d++unj17aufOnUX/MUjatm2b3nnnHTVu3Nhu+ciRI/XFF19o0aJF2rBhg06cOKHu3bsXW6e4WlbnqqSeLpk6dapsNluJvZRUJykpSR07dlT79u21detWbdu2TbGxsfLyKv4tXlwtK3PesGFDu9/xpk2bCtZZme/i6rgy1yX1dIkz811cHWffu47qWHnvStIvv/yili1bytfXVytWrND+/fs1adIkXXvttQVjzp8/r1atWunVV18t9udyps4ln332mb777juFh4e7XOvw4cNq1aqV6tevr/Xr12vPnj167rnn5O/vb6lOXFycVq5cqY8++kgpKSl68sknFRsbq2XLlhX7s6IM/YWfq1HIqVOnjCSzYcOGQuvWrVtnJJlffvmlVHUu2b17t5FkDh06ZLlWmzZtzBNPPOGwD6s9NWnSxAwaNMhSna+++sp4eXmZzMzMgjFnz541NpvNrF692mFf1157rZk1a1aR6z755BNTrlw5k5ub67DOn2u5MkdF1alYsaKZO3eu3fqgoCAzc+bMIrf99ddfTb169czq1avtejh79qzx9fU1ixYtKhibkpJiJJmkpCRLtYpS0lw5qrNz505TtWpVk5aWZiSZzz77zHI/zZs3N88++2yx/Vmp5eycjx071txyyy1F1rcy3yXVKUpJc+1MLWfm22pPxb13S6pj9b379NNPm1atWjnVz9GjR40ks3PnTpfr/Pe//zVVq1Y133//valZs6aZMmWKS7ViYmLMQw89VOIYZ+o0bNjQvPDCC3bLbrvtNvPMM8+UuB3Khkdd83DpUGRQUFCZ1jl//rxmz56tG264QdWrV3ep1rx581SlShU1atRICQkJys7OLlVPycnJ2rVrV4n/oiqqTk5Ojmw2m90DVPz9/eXl5VXkv7guycvL08KFC3X+/HlFRkYWu6+AgAD5+PiU2FNxtazOUVF1WrRooY8//lgZGRnKz8/XwoUL9fvvv+uuu+4qssZjjz2mLl26qF27dnbLk5OTlZuba7e8fv36qlGjhpKSkizVKkpJc1VSnezsbD344IOaPn26wsLCStxHcXVOnTqlLVu2KCQkRC1atFBoaKjatGlT4u+/pJ6szPmPP/6o8PBw1a5dW3369Ck4FG11vourUxRHf5cl1bIy38725Oi9W1wdq+/dZcuWqVmzZnrggQcUEhKiW2+9VTNnzizxZyiKM3Xy8/PVt29fPfXUU2rYsKHLtfLz87V8+XLdeOON6tChg0JCQtS8efNCp4qc6alFixZatmyZjh8/LmOM1q1bp4MHD6p9+/aW5wBu8Fenl0vy8vJMly5dTMuWLYtc7+yRh5LqTJ8+3VSsWNFIMjfddJPDow7F1XrnnXfMypUrzZ49e8xHH31kqlatarp16+byz2aMMY888oi5+eabLfdz6tQpExAQYJ544glz/vx5c+7cORMbG2skmaFDhxaqsWfPHlOxYkXj7e1tKleubJYvX17kvk6fPm1q1KhhxowZU2w/JdWyMkcl1fnll19M+/btjSTj4+NjAgICzFdffVVknQULFphGjRqZ3377zRhjf/Rj3rx5ply5coW2uf32283o0aMt1fqzkubKUZ2hQ4eawYMHF3yvYv4lXFKdpKQkI8kEBQWZ999/3+zYscM8+eSTply5cubgwYOWe3J2zv/973+bTz75xOzevdusXLnSREZGmho1apisrCxL811SnT9z9HfpqJaz822lp5LeuyXVsfre9fPzM35+fiYhIcHs2LHDvPPOO8bf39/MmTOn0NiSjjw4U+eVV14x9957r8nPzzfGmGKPPDiqdenoToUKFczkyZPNzp07TWJiorHZbGb9+vWWevr9999Nv379Cv4uy5UrZz744IMi5x1lz2PCw/Dhw03NmjXNf/7znyLXOxseSqpz9uxZc/DgQbNhwwbTtWtXc9tttxX8B9SVni5Zs2ZNiadAHNXJzs42lStXNhMnTixxP8XV+eqrr0zt2rWNzWYz3t7e5qGHHjK33XabGT58eKEaOTk55scffzTbt2838fHxpkqVKmbfvn12YzIzM80dd9xhOnbsaC5cuFBsP87UuqSkOSqpTmxsrLnjjjvM119/bXbt2mXGjRtnKleubPbs2WNXIzU11YSEhJjdu3cXLHM1PDiq9UclzZWjOp9//rmpW7eu+fXXXwvWF/U/M0d1Nm/ebCSZhIQEu+0iIiJMfHy85Z/N2Tn/s19++cUEBASYWbNmWQ5rxdX5I2f/Lour5ex8W+nJ2fducXWsvHd9fX1NZGSk3bIRI0aYO++8s9DYksKDozrbt283oaGh5vjx4wXriwsPjmodP37cSDK9e/e2G9O1a1fTq1cvSz/b66+/bm688UazbNkys3v3bjNt2jRTqVIlp07Pwv08Ijw89thjplq1aubIkSPFjnEmPDhT55KcnBxToUIFM3/+/FLXOnfunJFkVq5c6VKduXPnGl9fX3Pq1KlixzhT5/Tp0wXzExoaal577TWHvbdt29buXzlZWVkmMjLStG3btsRg5UytPyppjoqrc+jQISPJfP/994XWDxs2zG7ZZ599ZiQZb2/vgpekgv8of/3110X+/dSoUcNMnjzZUq2LFy8aYxzPlaM6sbGxBV//cb2Xl5dp06aN03UuzdOHH35ot/+ePXuaBx980FJPVua8KM2aNTPx8fEFYdGZ+S6pziWl+bu8VOuJJ55war6d7ckY5967ztRx5r1bo0YNu6Mmxhjz9ttvm/Dw8EJjSwoPjupMmTKl2HmqWbOmpVo5OTnGx8fHvPjii3ZjRo8ebVq0aOF0nezsbOPr62u+/PJLuzGDBw82HTp0KPQzouyVfDK7jBljNGLECH322Wdav369brjhhstWx/wvOCknJ6fUtS7dDnX99de7VOe9995TVFSUgoODS/WzValSRZK0du1anTp1SlFRUQ57z8/PL5iDrKwsdejQQX5+flq2bJnd1dDO+GOtPytqjhzVuXSNxJ/vGPD29i50NXrbtm21d+9eu2UDBw5U/fr19fTTT6t69ery9fXVmjVr1KNHD0nSgQMHlJqaWuiaD0e1vL29nZorR3WqVKmiYcOG2a2PiIjQlClT1LVrV6fr1K5dW+Hh4YVu4T148GChq/8d1bIy53927tw5HT58WH379lXTpk2dnu+S6kil+7v8Y62ePXvq4Ycftltf1Hw709MlJb13rdRx5r3bsmXLIn/HNWvWdGrfztbp27dvoWthOnTooL59+2rgwIGWapUrV0633367w74d1cnNzVVubq5Lf5coI39hcDGPPPKIqVy5slm/fr1JS0sreGVnZxeMSUtLMzt37jQzZ840kszGjRvNzp07zc8//+x0ncOHD5tXXnnFbN++3fz0009m8+bNpmvXriYoKMicPHnSUk+HDh0yL7zwgtm+fbs5evSo+fzzz03t2rVN69atLf9sxhjz448/GpvNZlasWOHyHL3//vsmKSnJHDp0yHz44YcmKCjIxMXFFaoVHx9vNmzYYI4ePWr27Nlj4uPjjc1mM6tWrTKZmZmmefPmJiIiwhw6dMhuX5f+pe1sLWfnyFGdCxcumLp165p//OMfZsuWLebQoUNm4sSJxmazFXutxh/9+XD88OHDTY0aNczatWvN9u3bTWRkZKFDpc7UsjpXJfX0Z3LyMPqf60yZMsUEBASYRYsWmR9//NE8++yzxt/f3+F1PX+uZWXOR40aZdavX2+OHj1qNm/ebNq1a2eqVKlS8K9wZ+e7pDpW59pRT39W3Hw7U8fRe9eZOs6+d40xZuvWrcbHx8e8/PLL5scffzTz5s0zFSpUMB999FHBmJ9//tns3LnTLF++3EgyCxcuNDt37jRpaWmW6vxZcactnKm1ZMkS4+vra959913z448/mmnTphlvb2/zzTffWKrTpk0b07BhQ7Nu3Tpz5MgRM3v2bOPv72/efvvtYvtG2flLw4OkIl+zZ88uGDN27FiHYxzVOX78uOnUqZMJCQkxvr6+plq1aubBBx80P/zwg+WeUlNTTevWrU1QUJDx8/MzdevWNU899ZTd7VbO/mzGGJOQkGCqV69u8vLyXJ6jp59+2oSGhhpfX19Tr149M2nSpIILnf5o0KBBpmbNmqZcuXImODjYtG3b1qxatcoY8/+nhYp6HT161FItZ+fIUR1jjDl48KDp3r27CQkJMRUqVDCNGzcudBthcf78P9jffvvNPProo+baa681FSpUMN26dbP7j6qztazOVUk9/Zmr4cEYYxITE021atVMhQoVTGRkpN1/nK3UcnbOY2JizPXXX2/KlStnqlatamJiYuzCirPzXVIdq3PtqKc/K26+nanj6L3rTB1n37uXfPHFF6ZRo0bGz8/P1K9f37z77rt262fPnl3kXI0dO9ZSnT8rLjw4W+u9994zdevWNf7+/uaWW24xS5cutVwnLS3NDBgwwISHhxt/f39z0003OZwvlB0+khsAAFjiUc95AAAAno/wAAAALCE8AAAASwgPAADAEsIDAACwhPAAAAAsITwAAABLCA8AAMASwgMAALCE8AAAACwhPAAAAEsIDwAAwJL/A5asZU+dERcjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot()"
      ],
      "metadata": {
        "id": "YPJRWVSysmAF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b43ad5d0-5a38-4556-ea2c-1cf398ea80a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modificacion de datos\n",
        "Por lo que se observa en la imagen anterior, los años son valores muy altos, entonces se intentara tomar el punto 0 como el primer año que se encuentra en el dataset que seria 2015.\n",
        "\n",
        "Tambien se tomra todas las velocidades y las aproximaremos para intentar mejorar la precision."
      ],
      "metadata": {
        "id": "gJXvJ7r-tFoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func = lambda x: (x - 2015)\n",
        "df['Agno'] = df['Agno'].apply(func)\n",
        "df"
      ],
      "metadata": {
        "id": "xGQpE0OGtW-Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "86971cdd-7d04-496f-bec1-d80d7c3f1210"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Horas  Agno  Mes  Dia_mes  Dia_sem         20         21         22  \\\n",
              "0         6     0    1        1        4  76.967742  65.548387  59.419355   \n",
              "1         7     0    1        1        4  70.451613  66.467742  35.903226   \n",
              "2         8     0    1        1        4  77.903226  47.193548  26.451613   \n",
              "3         9     0    1        1        4  70.806452  48.338710  32.693548   \n",
              "4        10     0    1        1        4  68.451613  50.924731  25.129032   \n",
              "...     ...   ...  ...      ...      ...        ...        ...        ...   \n",
              "7303     15     1   12       30        5  12.000000  38.500000  22.500000   \n",
              "7304     16     1   12       30        5  49.500000  44.000000  24.000000   \n",
              "7305     17     1   12       30        5  30.000000  54.000000  25.000000   \n",
              "7306     18     1   12       30        5  38.000000  38.000000  27.000000   \n",
              "7307     19     1   12       30        5  36.000000  28.000000  27.000000   \n",
              "\n",
              "             23         24  ...         58         59         60         61  \\\n",
              "0     60.258065  78.338710  ...  64.645161  46.612903  89.967742  83.451613   \n",
              "1     48.403226  50.145161  ...  56.661290  48.806452  75.209677  79.758065   \n",
              "2     33.419355  39.629032  ...  59.000000  46.709677  72.741935  71.161290   \n",
              "3     43.193548  59.338710  ...  54.758065  49.000000  75.612903  78.741935   \n",
              "4     44.516129  52.150538  ...  50.473118  55.290323  75.645161  81.612903   \n",
              "...         ...        ...  ...        ...        ...        ...        ...   \n",
              "7303  25.000000  36.500000  ...  61.500000  38.000000  58.000000  55.000000   \n",
              "7304  35.500000  48.000000  ...  28.000000  42.000000  60.000000  30.000000   \n",
              "7305  58.500000  51.500000  ...  41.000000  48.000000  81.000000  72.000000   \n",
              "7306  15.000000  39.000000  ...  26.000000  37.000000  37.000000  37.000000   \n",
              "7307  27.000000  26.000000  ...  28.000000  47.000000  32.000000  25.000000   \n",
              "\n",
              "             62         64         65         66         67         68  \n",
              "0     90.645161  85.838710  85.612903  88.258065  85.709677  85.096774  \n",
              "1     68.161290  83.096774  81.048387  78.370968  73.306452  70.129032  \n",
              "2     65.096774  81.870968  79.225806  77.000000  70.419355  56.548387  \n",
              "3     79.419355  84.741935  80.419355  83.516129  75.741935  65.774194  \n",
              "4     75.064516  85.870968  80.838710  83.709677  77.677419  66.322581  \n",
              "...         ...        ...        ...        ...        ...        ...  \n",
              "7303  21.500000  25.500000  36.000000  45.500000  33.000000  38.500000  \n",
              "7304  88.000000  89.000000  86.500000  84.000000  90.000000  90.000000  \n",
              "7305  77.000000  71.000000  79.000000  74.000000  72.000000  77.000000  \n",
              "7306  37.000000  37.000000  37.000000  37.000000  37.000000  37.000000  \n",
              "7307  25.000000  26.000000  58.000000  71.000000  73.000000  75.000000  \n",
              "\n",
              "[7308 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c959b565-7d3d-4413-8102-34b772f51534\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>76.967742</td>\n",
              "      <td>65.548387</td>\n",
              "      <td>59.419355</td>\n",
              "      <td>60.258065</td>\n",
              "      <td>78.338710</td>\n",
              "      <td>...</td>\n",
              "      <td>64.645161</td>\n",
              "      <td>46.612903</td>\n",
              "      <td>89.967742</td>\n",
              "      <td>83.451613</td>\n",
              "      <td>90.645161</td>\n",
              "      <td>85.838710</td>\n",
              "      <td>85.612903</td>\n",
              "      <td>88.258065</td>\n",
              "      <td>85.709677</td>\n",
              "      <td>85.096774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70.451613</td>\n",
              "      <td>66.467742</td>\n",
              "      <td>35.903226</td>\n",
              "      <td>48.403226</td>\n",
              "      <td>50.145161</td>\n",
              "      <td>...</td>\n",
              "      <td>56.661290</td>\n",
              "      <td>48.806452</td>\n",
              "      <td>75.209677</td>\n",
              "      <td>79.758065</td>\n",
              "      <td>68.161290</td>\n",
              "      <td>83.096774</td>\n",
              "      <td>81.048387</td>\n",
              "      <td>78.370968</td>\n",
              "      <td>73.306452</td>\n",
              "      <td>70.129032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>77.903226</td>\n",
              "      <td>47.193548</td>\n",
              "      <td>26.451613</td>\n",
              "      <td>33.419355</td>\n",
              "      <td>39.629032</td>\n",
              "      <td>...</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>46.709677</td>\n",
              "      <td>72.741935</td>\n",
              "      <td>71.161290</td>\n",
              "      <td>65.096774</td>\n",
              "      <td>81.870968</td>\n",
              "      <td>79.225806</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>70.419355</td>\n",
              "      <td>56.548387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70.806452</td>\n",
              "      <td>48.338710</td>\n",
              "      <td>32.693548</td>\n",
              "      <td>43.193548</td>\n",
              "      <td>59.338710</td>\n",
              "      <td>...</td>\n",
              "      <td>54.758065</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>75.612903</td>\n",
              "      <td>78.741935</td>\n",
              "      <td>79.419355</td>\n",
              "      <td>84.741935</td>\n",
              "      <td>80.419355</td>\n",
              "      <td>83.516129</td>\n",
              "      <td>75.741935</td>\n",
              "      <td>65.774194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>68.451613</td>\n",
              "      <td>50.924731</td>\n",
              "      <td>25.129032</td>\n",
              "      <td>44.516129</td>\n",
              "      <td>52.150538</td>\n",
              "      <td>...</td>\n",
              "      <td>50.473118</td>\n",
              "      <td>55.290323</td>\n",
              "      <td>75.645161</td>\n",
              "      <td>81.612903</td>\n",
              "      <td>75.064516</td>\n",
              "      <td>85.870968</td>\n",
              "      <td>80.838710</td>\n",
              "      <td>83.709677</td>\n",
              "      <td>77.677419</td>\n",
              "      <td>66.322581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7303</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>38.500000</td>\n",
              "      <td>22.500000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>36.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>61.500000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>25.500000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>45.500000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>38.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7304</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>49.500000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>35.500000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>89.000000</td>\n",
              "      <td>86.500000</td>\n",
              "      <td>84.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>90.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7305</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>58.500000</td>\n",
              "      <td>51.500000</td>\n",
              "      <td>...</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>77.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>77.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7306</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>37.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7307</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>58.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7308 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c959b565-7d3d-4413-8102-34b772f51534')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c959b565-7d3d-4413-8102-34b772f51534 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c959b565-7d3d-4413-8102-34b772f51534');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:,5:] = df.iloc[:,5:].apply(lambda x: x.round(0).astype(int) )\n",
        "df"
      ],
      "metadata": {
        "id": "VS45IU95CiIm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "0275b259-51dc-4c9e-9619-bf0c6d090558"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e82c0aa5964d>:1: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df.iloc[:,5:] = df.iloc[:,5:].apply(lambda x: x.round(0).astype(int) )\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Horas  Agno  Mes  Dia_mes  Dia_sem  20  21  22  23  24  ...  58  59  60  \\\n",
              "0         6     0    1        1        4  77  66  59  60  78  ...  65  47  90   \n",
              "1         7     0    1        1        4  70  66  36  48  50  ...  57  49  75   \n",
              "2         8     0    1        1        4  78  47  26  33  40  ...  59  47  73   \n",
              "3         9     0    1        1        4  71  48  33  43  59  ...  55  49  76   \n",
              "4        10     0    1        1        4  68  51  25  45  52  ...  50  55  76   \n",
              "...     ...   ...  ...      ...      ...  ..  ..  ..  ..  ..  ...  ..  ..  ..   \n",
              "7303     15     1   12       30        5  12  38  22  25  36  ...  62  38  58   \n",
              "7304     16     1   12       30        5  50  44  24  36  48  ...  28  42  60   \n",
              "7305     17     1   12       30        5  30  54  25  58  52  ...  41  48  81   \n",
              "7306     18     1   12       30        5  38  38  27  15  39  ...  26  37  37   \n",
              "7307     19     1   12       30        5  36  28  27  27  26  ...  28  47  32   \n",
              "\n",
              "      61  62  64  65  66  67  68  \n",
              "0     83  91  86  86  88  86  85  \n",
              "1     80  68  83  81  78  73  70  \n",
              "2     71  65  82  79  77  70  57  \n",
              "3     79  79  85  80  84  76  66  \n",
              "4     82  75  86  81  84  78  66  \n",
              "...   ..  ..  ..  ..  ..  ..  ..  \n",
              "7303  55  22  26  36  46  33  38  \n",
              "7304  30  88  89  86  84  90  90  \n",
              "7305  72  77  71  79  74  72  77  \n",
              "7306  37  37  37  37  37  37  37  \n",
              "7307  25  25  26  58  71  73  75  \n",
              "\n",
              "[7308 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d2a0aa2d-9d5c-4103-a7a5-2854bace54e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>77</td>\n",
              "      <td>66</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>78</td>\n",
              "      <td>...</td>\n",
              "      <td>65</td>\n",
              "      <td>47</td>\n",
              "      <td>90</td>\n",
              "      <td>83</td>\n",
              "      <td>91</td>\n",
              "      <td>86</td>\n",
              "      <td>86</td>\n",
              "      <td>88</td>\n",
              "      <td>86</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>66</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>50</td>\n",
              "      <td>...</td>\n",
              "      <td>57</td>\n",
              "      <td>49</td>\n",
              "      <td>75</td>\n",
              "      <td>80</td>\n",
              "      <td>68</td>\n",
              "      <td>83</td>\n",
              "      <td>81</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>78</td>\n",
              "      <td>47</td>\n",
              "      <td>26</td>\n",
              "      <td>33</td>\n",
              "      <td>40</td>\n",
              "      <td>...</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>73</td>\n",
              "      <td>71</td>\n",
              "      <td>65</td>\n",
              "      <td>82</td>\n",
              "      <td>79</td>\n",
              "      <td>77</td>\n",
              "      <td>70</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>71</td>\n",
              "      <td>48</td>\n",
              "      <td>33</td>\n",
              "      <td>43</td>\n",
              "      <td>59</td>\n",
              "      <td>...</td>\n",
              "      <td>55</td>\n",
              "      <td>49</td>\n",
              "      <td>76</td>\n",
              "      <td>79</td>\n",
              "      <td>79</td>\n",
              "      <td>85</td>\n",
              "      <td>80</td>\n",
              "      <td>84</td>\n",
              "      <td>76</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>68</td>\n",
              "      <td>51</td>\n",
              "      <td>25</td>\n",
              "      <td>45</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>50</td>\n",
              "      <td>55</td>\n",
              "      <td>76</td>\n",
              "      <td>82</td>\n",
              "      <td>75</td>\n",
              "      <td>86</td>\n",
              "      <td>81</td>\n",
              "      <td>84</td>\n",
              "      <td>78</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7303</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>62</td>\n",
              "      <td>38</td>\n",
              "      <td>58</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>26</td>\n",
              "      <td>36</td>\n",
              "      <td>46</td>\n",
              "      <td>33</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7304</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>44</td>\n",
              "      <td>24</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>42</td>\n",
              "      <td>60</td>\n",
              "      <td>30</td>\n",
              "      <td>88</td>\n",
              "      <td>89</td>\n",
              "      <td>86</td>\n",
              "      <td>84</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7305</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>54</td>\n",
              "      <td>25</td>\n",
              "      <td>58</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>41</td>\n",
              "      <td>48</td>\n",
              "      <td>81</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "      <td>71</td>\n",
              "      <td>79</td>\n",
              "      <td>74</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7306</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>27</td>\n",
              "      <td>15</td>\n",
              "      <td>39</td>\n",
              "      <td>...</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7307</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>36</td>\n",
              "      <td>28</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>26</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>47</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>58</td>\n",
              "      <td>71</td>\n",
              "      <td>73</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7308 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d2a0aa2d-9d5c-4103-a7a5-2854bace54e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d2a0aa2d-9d5c-4103-a7a5-2854bace54e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d2a0aa2d-9d5c-4103-a7a5-2854bace54e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primer modelo con dataframe original"
      ],
      "metadata": {
        "id": "bedV4KRJwPyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Declaracion de la semilla para los modelos\n",
        "import tensorflow as tf\n",
        "import random\n",
        "seed = 200\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "8GtqdwdMV5jp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 0:5].values\n",
        "y = df.iloc[:, 5:].values"
      ],
      "metadata": {
        "id": "gvKs7k2BpqUY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "PMAuoWP1rioW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94120a58-8b8a-4efc-ce2f-249facfa05c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  0,  1,  1,  4],\n",
              "       [ 7,  0,  1,  1,  4],\n",
              "       [ 8,  0,  1,  1,  4],\n",
              "       ...,\n",
              "       [17,  1, 12, 30,  5],\n",
              "       [18,  1, 12, 30,  5],\n",
              "       [19,  1, 12, 30,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "uYSAaj2FsYYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cfdf04-26f0-4d4e-b64e-024fbb95d0cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[77, 66, 59, ..., 88, 86, 85],\n",
              "       [70, 66, 36, ..., 78, 73, 70],\n",
              "       [78, 47, 26, ..., 77, 70, 57],\n",
              "       ...,\n",
              "       [30, 54, 25, ..., 74, 72, 77],\n",
              "       [38, 38, 27, ..., 37, 37, 37],\n",
              "       [36, 28, 27, ..., 71, 73, 75]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "dQac8f7LtA_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493a9ba8-d64e-491d-8d2a-d9cd582c8ea8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7308, 5)\n",
            "(7308, 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.65, random_state=0, shuffle=True)\n",
        "print(\"Train Size\", X_train.shape)\n",
        "print(\"Rem Size\", X_rem.shape)\n",
        "print(\"Train Label\", y_train.shape)\n",
        "print(\"RemLabel: \", y_rem.shape)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, train_size=0.4285,shuffle=True)\n",
        "print(\"X Valid:\", X_valid.shape)\n",
        "print(\"X test:\", X_test.shape)\n",
        "print(\"y valid:\", y_valid.shape)\n",
        "print(\"y test:\" , y_test.shape)"
      ],
      "metadata": {
        "id": "RoM_-wa2tdX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96b5515-ab8f-4d1e-9155-9b46b8599e82"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size (4750, 5)\n",
            "Rem Size (2558, 5)\n",
            "Train Label (4750, 44)\n",
            "RemLabel:  (2558, 44)\n",
            "-------------------------------------\n",
            "X Valid: (1096, 5)\n",
            "X test: (1462, 5)\n",
            "y valid: (1096, 44)\n",
            "y test: (1462, 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(12, input_shape=(X.shape[-1],) , activation='relu'))\n",
        "model.add(Dropout(0.002))\n",
        "\n",
        "model.add(Dense(24, activation=\"relu\"))\n",
        "model.add(Dropout(0.002))\n",
        "\n",
        "model.add(Dense(48, activation=\"relu\"))\n",
        "model.add(Dropout(0.002))\n",
        "\n",
        "model.add(Dense(96, activation=\"relu\"))\n",
        "model.add(Dropout(0.002))\n",
        "\n",
        "model.add(Dense(44)) #Capa de salida \n",
        "\n",
        "model.summary()\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "adam = Adam(0.007) #learning rate\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"mse\"] )"
      ],
      "metadata": {
        "id": "gxP1le5fuZCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b4c227-881d-4f31-a9e4-b78bbce31ae5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 12)                72        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                312       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 24)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 48)                1200      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 48)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 96)                4704      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 96)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 44)                4268      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,556\n",
            "Trainable params: 10,556\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "H = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=400, batch_size=64)"
      ],
      "metadata": {
        "id": "_VYyQ9-OuuPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f079d79c-40cb-4df9-e5ed-249a8ae7a172"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "75/75 [==============================] - 2s 6ms/step - loss: 902.5967 - mse: 902.5967 - val_loss: 403.0983 - val_mse: 403.0983\n",
            "Epoch 2/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 368.1012 - mse: 368.1012 - val_loss: 342.8170 - val_mse: 342.8170\n",
            "Epoch 3/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 299.0719 - mse: 299.0719 - val_loss: 266.8448 - val_mse: 266.8448\n",
            "Epoch 4/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 246.6549 - mse: 246.6549 - val_loss: 224.8917 - val_mse: 224.8917\n",
            "Epoch 5/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 225.0945 - mse: 225.0945 - val_loss: 224.1473 - val_mse: 224.1473\n",
            "Epoch 6/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 219.1408 - mse: 219.1408 - val_loss: 206.1658 - val_mse: 206.1658\n",
            "Epoch 7/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 213.0464 - mse: 213.0464 - val_loss: 208.0869 - val_mse: 208.0869\n",
            "Epoch 8/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 212.8445 - mse: 212.8445 - val_loss: 217.0469 - val_mse: 217.0469\n",
            "Epoch 9/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 205.0398 - mse: 205.0398 - val_loss: 216.2883 - val_mse: 216.2883\n",
            "Epoch 10/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 205.9708 - mse: 205.9708 - val_loss: 231.5515 - val_mse: 231.5515\n",
            "Epoch 11/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 202.5243 - mse: 202.5243 - val_loss: 247.1451 - val_mse: 247.1451\n",
            "Epoch 12/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 201.5423 - mse: 201.5423 - val_loss: 196.9105 - val_mse: 196.9105\n",
            "Epoch 13/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 196.9274 - mse: 196.9274 - val_loss: 197.0768 - val_mse: 197.0768\n",
            "Epoch 14/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 193.8563 - mse: 193.8563 - val_loss: 194.1465 - val_mse: 194.1465\n",
            "Epoch 15/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 192.2874 - mse: 192.2874 - val_loss: 191.3388 - val_mse: 191.3388\n",
            "Epoch 16/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 193.9462 - mse: 193.9462 - val_loss: 196.3233 - val_mse: 196.3233\n",
            "Epoch 17/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 189.4871 - mse: 189.4871 - val_loss: 192.0491 - val_mse: 192.0491\n",
            "Epoch 18/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 186.7213 - mse: 186.7213 - val_loss: 190.4372 - val_mse: 190.4372\n",
            "Epoch 19/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 192.3604 - mse: 192.3604 - val_loss: 186.2265 - val_mse: 186.2265\n",
            "Epoch 20/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 182.1815 - mse: 182.1815 - val_loss: 188.8244 - val_mse: 188.8244\n",
            "Epoch 21/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 185.3161 - mse: 185.3161 - val_loss: 187.5376 - val_mse: 187.5376\n",
            "Epoch 22/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 180.6107 - mse: 180.6107 - val_loss: 178.4982 - val_mse: 178.4982\n",
            "Epoch 23/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 176.9863 - mse: 176.9863 - val_loss: 175.5638 - val_mse: 175.5638\n",
            "Epoch 24/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 179.0634 - mse: 179.0634 - val_loss: 177.4641 - val_mse: 177.4641\n",
            "Epoch 25/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 179.9253 - mse: 179.9253 - val_loss: 191.8421 - val_mse: 191.8421\n",
            "Epoch 26/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 181.4258 - mse: 181.4258 - val_loss: 191.8905 - val_mse: 191.8905\n",
            "Epoch 27/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 174.6122 - mse: 174.6122 - val_loss: 173.8828 - val_mse: 173.8828\n",
            "Epoch 28/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 174.7560 - mse: 174.7560 - val_loss: 178.4629 - val_mse: 178.4629\n",
            "Epoch 29/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 173.5631 - mse: 173.5631 - val_loss: 170.7653 - val_mse: 170.7653\n",
            "Epoch 30/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 174.1721 - mse: 174.1721 - val_loss: 171.9027 - val_mse: 171.9027\n",
            "Epoch 31/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 172.1803 - mse: 172.1803 - val_loss: 169.1888 - val_mse: 169.1888\n",
            "Epoch 32/400\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 174.0577 - mse: 174.0577 - val_loss: 174.6737 - val_mse: 174.6737\n",
            "Epoch 33/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 173.5928 - mse: 173.5928 - val_loss: 175.7811 - val_mse: 175.7811\n",
            "Epoch 34/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 170.1149 - mse: 170.1149 - val_loss: 187.9084 - val_mse: 187.9084\n",
            "Epoch 35/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 170.6640 - mse: 170.6640 - val_loss: 172.2620 - val_mse: 172.2620\n",
            "Epoch 36/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.5693 - mse: 168.5693 - val_loss: 168.5356 - val_mse: 168.5356\n",
            "Epoch 37/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.0623 - mse: 168.0623 - val_loss: 166.0399 - val_mse: 166.0399\n",
            "Epoch 38/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.2556 - mse: 169.2556 - val_loss: 172.5370 - val_mse: 172.5370\n",
            "Epoch 39/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 167.9657 - mse: 167.9657 - val_loss: 168.1604 - val_mse: 168.1604\n",
            "Epoch 40/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.8879 - mse: 168.8879 - val_loss: 166.0321 - val_mse: 166.0321\n",
            "Epoch 41/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.2757 - mse: 166.2757 - val_loss: 170.7463 - val_mse: 170.7463\n",
            "Epoch 42/400\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 166.3824 - mse: 166.3824 - val_loss: 165.2147 - val_mse: 165.2147\n",
            "Epoch 43/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 167.0058 - mse: 167.0058 - val_loss: 182.0628 - val_mse: 182.0628\n",
            "Epoch 44/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.5533 - mse: 169.5533 - val_loss: 173.3206 - val_mse: 173.3206\n",
            "Epoch 45/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 167.7234 - mse: 167.7234 - val_loss: 164.6209 - val_mse: 164.6209\n",
            "Epoch 46/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.0556 - mse: 166.0556 - val_loss: 172.2628 - val_mse: 172.2628\n",
            "Epoch 47/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.6169 - mse: 164.6169 - val_loss: 163.2359 - val_mse: 163.2359\n",
            "Epoch 48/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.2097 - mse: 166.2097 - val_loss: 190.0141 - val_mse: 190.0141\n",
            "Epoch 49/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.3218 - mse: 168.3218 - val_loss: 163.6995 - val_mse: 163.6995\n",
            "Epoch 50/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.3609 - mse: 164.3609 - val_loss: 165.2692 - val_mse: 165.2692\n",
            "Epoch 51/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.0825 - mse: 166.0825 - val_loss: 165.1111 - val_mse: 165.1111\n",
            "Epoch 52/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.4763 - mse: 164.4763 - val_loss: 175.7930 - val_mse: 175.7930\n",
            "Epoch 53/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.7339 - mse: 166.7339 - val_loss: 164.4593 - val_mse: 164.4593\n",
            "Epoch 54/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 165.0359 - mse: 165.0359 - val_loss: 162.8121 - val_mse: 162.8121\n",
            "Epoch 55/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.7673 - mse: 160.7673 - val_loss: 161.7383 - val_mse: 161.7383\n",
            "Epoch 56/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.6639 - mse: 166.6639 - val_loss: 165.4498 - val_mse: 165.4498\n",
            "Epoch 57/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 162.7965 - mse: 162.7965 - val_loss: 180.3861 - val_mse: 180.3861\n",
            "Epoch 58/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.7833 - mse: 164.7833 - val_loss: 161.9361 - val_mse: 161.9361\n",
            "Epoch 59/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 165.2201 - mse: 165.2201 - val_loss: 166.2507 - val_mse: 166.2507\n",
            "Epoch 60/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 167.2978 - mse: 167.2978 - val_loss: 164.3854 - val_mse: 164.3854\n",
            "Epoch 61/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 162.6528 - mse: 162.6528 - val_loss: 161.0145 - val_mse: 161.0145\n",
            "Epoch 62/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 162.4923 - mse: 162.4923 - val_loss: 164.1487 - val_mse: 164.1487\n",
            "Epoch 63/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 162.2268 - mse: 162.2268 - val_loss: 171.2437 - val_mse: 171.2437\n",
            "Epoch 64/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 164.5993 - mse: 164.5993 - val_loss: 164.3483 - val_mse: 164.3483\n",
            "Epoch 65/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.6794 - mse: 161.6794 - val_loss: 160.9074 - val_mse: 160.9074\n",
            "Epoch 66/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.2625 - mse: 163.2625 - val_loss: 163.8119 - val_mse: 163.8119\n",
            "Epoch 67/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.1928 - mse: 160.1928 - val_loss: 163.2772 - val_mse: 163.2772\n",
            "Epoch 68/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.0535 - mse: 161.0535 - val_loss: 160.4564 - val_mse: 160.4564\n",
            "Epoch 69/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.3701 - mse: 161.3701 - val_loss: 164.4031 - val_mse: 164.4031\n",
            "Epoch 70/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 162.3962 - mse: 162.3962 - val_loss: 159.2847 - val_mse: 159.2847\n",
            "Epoch 71/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.6394 - mse: 160.6394 - val_loss: 160.7501 - val_mse: 160.7501\n",
            "Epoch 72/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.0401 - mse: 160.0401 - val_loss: 158.7720 - val_mse: 158.7720\n",
            "Epoch 73/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.9192 - mse: 159.9192 - val_loss: 161.4804 - val_mse: 161.4804\n",
            "Epoch 74/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.5911 - mse: 163.5911 - val_loss: 159.0241 - val_mse: 159.0241\n",
            "Epoch 75/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.0035 - mse: 163.0035 - val_loss: 161.1573 - val_mse: 161.1573\n",
            "Epoch 76/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.5233 - mse: 161.5233 - val_loss: 164.5784 - val_mse: 164.5784\n",
            "Epoch 77/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.3715 - mse: 159.3715 - val_loss: 162.2474 - val_mse: 162.2474\n",
            "Epoch 78/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.9200 - mse: 160.9200 - val_loss: 159.3507 - val_mse: 159.3507\n",
            "Epoch 79/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.8361 - mse: 159.8361 - val_loss: 158.6156 - val_mse: 158.6156\n",
            "Epoch 80/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.2610 - mse: 159.2610 - val_loss: 159.2609 - val_mse: 159.2609\n",
            "Epoch 81/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.3143 - mse: 160.3143 - val_loss: 161.9930 - val_mse: 161.9930\n",
            "Epoch 82/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.5136 - mse: 159.5136 - val_loss: 163.8008 - val_mse: 163.8008\n",
            "Epoch 83/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.7156 - mse: 158.7156 - val_loss: 161.9434 - val_mse: 161.9434\n",
            "Epoch 84/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.0975 - mse: 160.0975 - val_loss: 161.7870 - val_mse: 161.7870\n",
            "Epoch 85/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.5973 - mse: 160.5973 - val_loss: 161.3169 - val_mse: 161.3169\n",
            "Epoch 86/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.5665 - mse: 161.5665 - val_loss: 159.1360 - val_mse: 159.1360\n",
            "Epoch 87/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.4892 - mse: 159.4892 - val_loss: 158.9872 - val_mse: 158.9872\n",
            "Epoch 88/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.6884 - mse: 158.6884 - val_loss: 158.2809 - val_mse: 158.2809\n",
            "Epoch 89/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.7355 - mse: 158.7355 - val_loss: 160.6521 - val_mse: 160.6521\n",
            "Epoch 90/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.4277 - mse: 157.4277 - val_loss: 158.5785 - val_mse: 158.5785\n",
            "Epoch 91/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.3183 - mse: 158.3183 - val_loss: 159.8397 - val_mse: 159.8397\n",
            "Epoch 92/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.9694 - mse: 158.9694 - val_loss: 159.8072 - val_mse: 159.8072\n",
            "Epoch 93/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.2802 - mse: 159.2802 - val_loss: 159.1333 - val_mse: 159.1333\n",
            "Epoch 94/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.7826 - mse: 159.7826 - val_loss: 162.0758 - val_mse: 162.0758\n",
            "Epoch 95/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.7744 - mse: 159.7744 - val_loss: 158.3214 - val_mse: 158.3214\n",
            "Epoch 96/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.8005 - mse: 157.8005 - val_loss: 165.6999 - val_mse: 165.6999\n",
            "Epoch 97/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.4869 - mse: 157.4869 - val_loss: 158.2799 - val_mse: 158.2799\n",
            "Epoch 98/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.2986 - mse: 157.2986 - val_loss: 159.9240 - val_mse: 159.9240\n",
            "Epoch 99/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 158.0170 - mse: 158.0170 - val_loss: 166.4763 - val_mse: 166.4763\n",
            "Epoch 100/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.1662 - mse: 158.1662 - val_loss: 163.8106 - val_mse: 163.8106\n",
            "Epoch 101/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.6804 - mse: 158.6804 - val_loss: 158.4275 - val_mse: 158.4275\n",
            "Epoch 102/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.8666 - mse: 158.8666 - val_loss: 166.2785 - val_mse: 166.2785\n",
            "Epoch 103/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.9649 - mse: 158.9649 - val_loss: 158.2356 - val_mse: 158.2356\n",
            "Epoch 104/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.9465 - mse: 157.9465 - val_loss: 156.3865 - val_mse: 156.3865\n",
            "Epoch 105/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.4673 - mse: 156.4673 - val_loss: 160.2539 - val_mse: 160.2539\n",
            "Epoch 106/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.2984 - mse: 158.2984 - val_loss: 157.2129 - val_mse: 157.2129\n",
            "Epoch 107/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.5550 - mse: 157.5550 - val_loss: 159.6121 - val_mse: 159.6121\n",
            "Epoch 108/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.7233 - mse: 156.7233 - val_loss: 156.7062 - val_mse: 156.7062\n",
            "Epoch 109/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.3104 - mse: 157.3104 - val_loss: 160.2384 - val_mse: 160.2384\n",
            "Epoch 110/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.9542 - mse: 155.9542 - val_loss: 158.7280 - val_mse: 158.7280\n",
            "Epoch 111/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.5130 - mse: 157.5130 - val_loss: 159.4160 - val_mse: 159.4160\n",
            "Epoch 112/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.1991 - mse: 158.1991 - val_loss: 157.3541 - val_mse: 157.3541\n",
            "Epoch 113/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.9726 - mse: 155.9726 - val_loss: 163.0315 - val_mse: 163.0315\n",
            "Epoch 114/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.1344 - mse: 158.1344 - val_loss: 161.8387 - val_mse: 161.8387\n",
            "Epoch 115/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.4035 - mse: 157.4035 - val_loss: 158.8787 - val_mse: 158.8787\n",
            "Epoch 116/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.4370 - mse: 156.4370 - val_loss: 156.9873 - val_mse: 156.9873\n",
            "Epoch 117/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.6527 - mse: 155.6527 - val_loss: 169.0844 - val_mse: 169.0844\n",
            "Epoch 118/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.2247 - mse: 159.2247 - val_loss: 156.3088 - val_mse: 156.3088\n",
            "Epoch 119/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.8762 - mse: 157.8762 - val_loss: 160.9868 - val_mse: 160.9868\n",
            "Epoch 120/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.5806 - mse: 156.5806 - val_loss: 159.3055 - val_mse: 159.3055\n",
            "Epoch 121/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.0673 - mse: 157.0673 - val_loss: 155.2393 - val_mse: 155.2393\n",
            "Epoch 122/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.3983 - mse: 155.3983 - val_loss: 169.6751 - val_mse: 169.6751\n",
            "Epoch 123/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.6075 - mse: 158.6075 - val_loss: 157.7482 - val_mse: 157.7482\n",
            "Epoch 124/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.6536 - mse: 155.6536 - val_loss: 161.6413 - val_mse: 161.6413\n",
            "Epoch 125/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.9157 - mse: 157.9157 - val_loss: 155.7268 - val_mse: 155.7268\n",
            "Epoch 126/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.1781 - mse: 156.1781 - val_loss: 157.2310 - val_mse: 157.2310\n",
            "Epoch 127/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.0911 - mse: 157.0911 - val_loss: 158.2080 - val_mse: 158.2080\n",
            "Epoch 128/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.1336 - mse: 155.1336 - val_loss: 155.6277 - val_mse: 155.6277\n",
            "Epoch 129/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.3664 - mse: 155.3664 - val_loss: 160.4141 - val_mse: 160.4141\n",
            "Epoch 130/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.6888 - mse: 155.6888 - val_loss: 156.6471 - val_mse: 156.6471\n",
            "Epoch 131/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.6178 - mse: 155.6178 - val_loss: 159.1492 - val_mse: 159.1492\n",
            "Epoch 132/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.6087 - mse: 156.6087 - val_loss: 154.7551 - val_mse: 154.7551\n",
            "Epoch 133/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.3488 - mse: 155.3488 - val_loss: 154.5032 - val_mse: 154.5032\n",
            "Epoch 134/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.4337 - mse: 156.4337 - val_loss: 156.6486 - val_mse: 156.6486\n",
            "Epoch 135/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.9359 - mse: 156.9359 - val_loss: 156.1182 - val_mse: 156.1182\n",
            "Epoch 136/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.1408 - mse: 156.1408 - val_loss: 153.7442 - val_mse: 153.7442\n",
            "Epoch 137/400\n",
            "75/75 [==============================] - 0s 3ms/step - loss: 155.1638 - mse: 155.1638 - val_loss: 156.1048 - val_mse: 156.1048\n",
            "Epoch 138/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 156.1107 - mse: 156.1107 - val_loss: 155.3229 - val_mse: 155.3229\n",
            "Epoch 139/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.9697 - mse: 154.9697 - val_loss: 155.4367 - val_mse: 155.4367\n",
            "Epoch 140/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.6443 - mse: 155.6443 - val_loss: 162.0249 - val_mse: 162.0249\n",
            "Epoch 141/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.2867 - mse: 155.2867 - val_loss: 156.1226 - val_mse: 156.1226\n",
            "Epoch 142/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.9674 - mse: 155.9674 - val_loss: 154.3395 - val_mse: 154.3395\n",
            "Epoch 143/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.5379 - mse: 154.5379 - val_loss: 155.1711 - val_mse: 155.1711\n",
            "Epoch 144/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.7070 - mse: 154.7070 - val_loss: 155.0549 - val_mse: 155.0549\n",
            "Epoch 145/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.6881 - mse: 155.6881 - val_loss: 165.5958 - val_mse: 165.5958\n",
            "Epoch 146/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.3926 - mse: 155.3926 - val_loss: 154.7621 - val_mse: 154.7621\n",
            "Epoch 147/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.2998 - mse: 155.2998 - val_loss: 155.2010 - val_mse: 155.2010\n",
            "Epoch 148/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.2416 - mse: 153.2416 - val_loss: 155.1525 - val_mse: 155.1525\n",
            "Epoch 149/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.5483 - mse: 154.5483 - val_loss: 162.0980 - val_mse: 162.0980\n",
            "Epoch 150/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.2852 - mse: 156.2852 - val_loss: 160.1389 - val_mse: 160.1389\n",
            "Epoch 151/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 156.1212 - mse: 156.1212 - val_loss: 158.5962 - val_mse: 158.5962\n",
            "Epoch 152/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6480 - mse: 154.6480 - val_loss: 156.3046 - val_mse: 156.3046\n",
            "Epoch 153/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.6581 - mse: 155.6581 - val_loss: 158.9992 - val_mse: 158.9992\n",
            "Epoch 154/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.5687 - mse: 154.5687 - val_loss: 155.3411 - val_mse: 155.3411\n",
            "Epoch 155/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.7392 - mse: 153.7392 - val_loss: 158.9526 - val_mse: 158.9526\n",
            "Epoch 156/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.7534 - mse: 154.7534 - val_loss: 158.7479 - val_mse: 158.7479\n",
            "Epoch 157/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.6135 - mse: 153.6135 - val_loss: 154.3790 - val_mse: 154.3790\n",
            "Epoch 158/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6207 - mse: 154.6207 - val_loss: 156.2240 - val_mse: 156.2240\n",
            "Epoch 159/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.1926 - mse: 154.1926 - val_loss: 161.6953 - val_mse: 161.6953\n",
            "Epoch 160/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.2607 - mse: 154.2607 - val_loss: 157.3324 - val_mse: 157.3324\n",
            "Epoch 161/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.5444 - mse: 156.5444 - val_loss: 154.1689 - val_mse: 154.1689\n",
            "Epoch 162/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.0135 - mse: 153.0135 - val_loss: 156.3535 - val_mse: 156.3535\n",
            "Epoch 163/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.3134 - mse: 155.3134 - val_loss: 159.8319 - val_mse: 159.8319\n",
            "Epoch 164/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.4843 - mse: 155.4843 - val_loss: 154.8189 - val_mse: 154.8189\n",
            "Epoch 165/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.1617 - mse: 154.1617 - val_loss: 157.1396 - val_mse: 157.1396\n",
            "Epoch 166/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.6150 - mse: 153.6150 - val_loss: 154.4558 - val_mse: 154.4558\n",
            "Epoch 167/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.0236 - mse: 155.0236 - val_loss: 155.2552 - val_mse: 155.2552\n",
            "Epoch 168/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.4902 - mse: 153.4902 - val_loss: 154.5822 - val_mse: 154.5822\n",
            "Epoch 169/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.3349 - mse: 154.3349 - val_loss: 158.1031 - val_mse: 158.1031\n",
            "Epoch 170/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.5198 - mse: 154.5198 - val_loss: 154.5275 - val_mse: 154.5275\n",
            "Epoch 171/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.9321 - mse: 152.9321 - val_loss: 154.5591 - val_mse: 154.5591\n",
            "Epoch 172/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.3364 - mse: 154.3364 - val_loss: 156.4166 - val_mse: 156.4166\n",
            "Epoch 173/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.5924 - mse: 153.5924 - val_loss: 156.2665 - val_mse: 156.2665\n",
            "Epoch 174/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.9376 - mse: 153.9376 - val_loss: 155.4017 - val_mse: 155.4017\n",
            "Epoch 175/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.4402 - mse: 153.4402 - val_loss: 156.0243 - val_mse: 156.0243\n",
            "Epoch 176/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.9386 - mse: 152.9386 - val_loss: 156.7838 - val_mse: 156.7838\n",
            "Epoch 177/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.6975 - mse: 155.6975 - val_loss: 155.1086 - val_mse: 155.1086\n",
            "Epoch 178/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.5743 - mse: 153.5743 - val_loss: 154.3458 - val_mse: 154.3458\n",
            "Epoch 179/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.8618 - mse: 153.8618 - val_loss: 154.6696 - val_mse: 154.6696\n",
            "Epoch 180/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.3324 - mse: 153.3324 - val_loss: 154.4299 - val_mse: 154.4299\n",
            "Epoch 181/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.8276 - mse: 152.8276 - val_loss: 154.2003 - val_mse: 154.2003\n",
            "Epoch 182/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 152.8484 - mse: 152.8484 - val_loss: 155.0357 - val_mse: 155.0357\n",
            "Epoch 183/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.9521 - mse: 152.9521 - val_loss: 153.9146 - val_mse: 153.9146\n",
            "Epoch 184/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.7479 - mse: 151.7479 - val_loss: 155.0296 - val_mse: 155.0296\n",
            "Epoch 185/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.6986 - mse: 152.6986 - val_loss: 161.6216 - val_mse: 161.6216\n",
            "Epoch 186/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.4983 - mse: 153.4983 - val_loss: 156.7723 - val_mse: 156.7723\n",
            "Epoch 187/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.7206 - mse: 152.7206 - val_loss: 154.1508 - val_mse: 154.1508\n",
            "Epoch 188/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.7129 - mse: 151.7129 - val_loss: 155.2435 - val_mse: 155.2435\n",
            "Epoch 189/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.6897 - mse: 152.6897 - val_loss: 154.3186 - val_mse: 154.3186\n",
            "Epoch 190/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.4759 - mse: 152.4759 - val_loss: 159.8463 - val_mse: 159.8463\n",
            "Epoch 191/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.6099 - mse: 153.6099 - val_loss: 156.3046 - val_mse: 156.3046\n",
            "Epoch 192/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.4206 - mse: 151.4206 - val_loss: 158.2530 - val_mse: 158.2530\n",
            "Epoch 193/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.7097 - mse: 152.7097 - val_loss: 155.7215 - val_mse: 155.7215\n",
            "Epoch 194/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6334 - mse: 154.6334 - val_loss: 154.8823 - val_mse: 154.8823\n",
            "Epoch 195/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.7356 - mse: 153.7356 - val_loss: 154.9159 - val_mse: 154.9159\n",
            "Epoch 196/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.9022 - mse: 151.9022 - val_loss: 154.0109 - val_mse: 154.0109\n",
            "Epoch 197/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.7284 - mse: 150.7284 - val_loss: 160.3496 - val_mse: 160.3496\n",
            "Epoch 198/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.5912 - mse: 151.5912 - val_loss: 154.9068 - val_mse: 154.9068\n",
            "Epoch 199/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.7316 - mse: 153.7316 - val_loss: 153.6431 - val_mse: 153.6431\n",
            "Epoch 200/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.5029 - mse: 152.5029 - val_loss: 153.9623 - val_mse: 153.9623\n",
            "Epoch 201/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.0215 - mse: 151.0215 - val_loss: 155.1223 - val_mse: 155.1223\n",
            "Epoch 202/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.1119 - mse: 152.1119 - val_loss: 154.2592 - val_mse: 154.2592\n",
            "Epoch 203/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.6723 - mse: 150.6723 - val_loss: 156.2277 - val_mse: 156.2277\n",
            "Epoch 204/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.3403 - mse: 152.3403 - val_loss: 160.6253 - val_mse: 160.6253\n",
            "Epoch 205/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.3387 - mse: 152.3387 - val_loss: 155.0633 - val_mse: 155.0633\n",
            "Epoch 206/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.7106 - mse: 151.7106 - val_loss: 160.1734 - val_mse: 160.1734\n",
            "Epoch 207/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.4870 - mse: 151.4870 - val_loss: 153.7433 - val_mse: 153.7433\n",
            "Epoch 208/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.6863 - mse: 151.6863 - val_loss: 154.9940 - val_mse: 154.9940\n",
            "Epoch 209/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.5282 - mse: 153.5282 - val_loss: 158.2878 - val_mse: 158.2878\n",
            "Epoch 210/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.3513 - mse: 152.3513 - val_loss: 153.4091 - val_mse: 153.4091\n",
            "Epoch 211/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.7344 - mse: 150.7344 - val_loss: 154.9749 - val_mse: 154.9749\n",
            "Epoch 212/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.0051 - mse: 151.0051 - val_loss: 155.0396 - val_mse: 155.0396\n",
            "Epoch 213/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.3848 - mse: 150.3848 - val_loss: 158.4893 - val_mse: 158.4893\n",
            "Epoch 214/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.6401 - mse: 150.6401 - val_loss: 157.8444 - val_mse: 157.8444\n",
            "Epoch 215/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 150.7654 - mse: 150.7654 - val_loss: 155.8733 - val_mse: 155.8733\n",
            "Epoch 216/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.2149 - mse: 152.2149 - val_loss: 153.2387 - val_mse: 153.2387\n",
            "Epoch 217/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.8464 - mse: 153.8464 - val_loss: 158.3696 - val_mse: 158.3696\n",
            "Epoch 218/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.6117 - mse: 152.6117 - val_loss: 158.6723 - val_mse: 158.6723\n",
            "Epoch 219/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 150.7003 - mse: 150.7003 - val_loss: 157.1118 - val_mse: 157.1118\n",
            "Epoch 220/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 152.0596 - mse: 152.0596 - val_loss: 154.5380 - val_mse: 154.5380\n",
            "Epoch 221/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.4467 - mse: 150.4467 - val_loss: 153.7902 - val_mse: 153.7902\n",
            "Epoch 222/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.6436 - mse: 149.6436 - val_loss: 154.4343 - val_mse: 154.4343\n",
            "Epoch 223/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.5373 - mse: 150.5373 - val_loss: 157.3672 - val_mse: 157.3672\n",
            "Epoch 224/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.7375 - mse: 151.7375 - val_loss: 155.6915 - val_mse: 155.6915\n",
            "Epoch 225/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.2559 - mse: 151.2559 - val_loss: 156.0691 - val_mse: 156.0691\n",
            "Epoch 226/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.3166 - mse: 151.3166 - val_loss: 154.9364 - val_mse: 154.9364\n",
            "Epoch 227/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.5456 - mse: 150.5456 - val_loss: 154.3318 - val_mse: 154.3318\n",
            "Epoch 228/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.5790 - mse: 151.5790 - val_loss: 153.4899 - val_mse: 153.4899\n",
            "Epoch 229/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.9466 - mse: 150.9466 - val_loss: 153.5097 - val_mse: 153.5097\n",
            "Epoch 230/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.9060 - mse: 150.9060 - val_loss: 153.8561 - val_mse: 153.8561\n",
            "Epoch 231/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.5910 - mse: 150.5910 - val_loss: 153.8990 - val_mse: 153.8990\n",
            "Epoch 232/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.3397 - mse: 150.3397 - val_loss: 154.3631 - val_mse: 154.3631\n",
            "Epoch 233/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.4950 - mse: 151.4950 - val_loss: 153.7896 - val_mse: 153.7896\n",
            "Epoch 234/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.5427 - mse: 149.5427 - val_loss: 153.4703 - val_mse: 153.4703\n",
            "Epoch 235/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.5179 - mse: 149.5179 - val_loss: 158.3380 - val_mse: 158.3380\n",
            "Epoch 236/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.6459 - mse: 149.6459 - val_loss: 156.4620 - val_mse: 156.4620\n",
            "Epoch 237/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.1556 - mse: 151.1556 - val_loss: 156.8114 - val_mse: 156.8114\n",
            "Epoch 238/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.8280 - mse: 150.8280 - val_loss: 153.4911 - val_mse: 153.4911\n",
            "Epoch 239/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.3473 - mse: 150.3473 - val_loss: 156.9716 - val_mse: 156.9716\n",
            "Epoch 240/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.3517 - mse: 150.3517 - val_loss: 152.9565 - val_mse: 152.9565\n",
            "Epoch 241/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.5160 - mse: 150.5160 - val_loss: 153.2443 - val_mse: 153.2443\n",
            "Epoch 242/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.6834 - mse: 149.6834 - val_loss: 161.0534 - val_mse: 161.0534\n",
            "Epoch 243/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.1821 - mse: 150.1821 - val_loss: 154.1484 - val_mse: 154.1484\n",
            "Epoch 244/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.0957 - mse: 150.0957 - val_loss: 155.4863 - val_mse: 155.4863\n",
            "Epoch 245/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.0172 - mse: 151.0172 - val_loss: 160.6116 - val_mse: 160.6116\n",
            "Epoch 246/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.1270 - mse: 150.1270 - val_loss: 153.8788 - val_mse: 153.8788\n",
            "Epoch 247/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.6837 - mse: 150.6837 - val_loss: 154.8037 - val_mse: 154.8037\n",
            "Epoch 248/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.5375 - mse: 149.5375 - val_loss: 154.9933 - val_mse: 154.9933\n",
            "Epoch 249/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.6165 - mse: 150.6165 - val_loss: 154.3582 - val_mse: 154.3582\n",
            "Epoch 250/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.7824 - mse: 149.7824 - val_loss: 154.4077 - val_mse: 154.4077\n",
            "Epoch 251/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.7837 - mse: 149.7837 - val_loss: 156.3151 - val_mse: 156.3151\n",
            "Epoch 252/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.9799 - mse: 150.9799 - val_loss: 153.9462 - val_mse: 153.9462\n",
            "Epoch 253/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 150.1535 - mse: 150.1535 - val_loss: 153.5187 - val_mse: 153.5187\n",
            "Epoch 254/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.6439 - mse: 148.6439 - val_loss: 151.2321 - val_mse: 151.2321\n",
            "Epoch 255/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.9171 - mse: 148.9171 - val_loss: 152.0535 - val_mse: 152.0535\n",
            "Epoch 256/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.9422 - mse: 148.9422 - val_loss: 154.6751 - val_mse: 154.6751\n",
            "Epoch 257/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.3039 - mse: 148.3039 - val_loss: 153.3568 - val_mse: 153.3568\n",
            "Epoch 258/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 149.5695 - mse: 149.5695 - val_loss: 154.1323 - val_mse: 154.1323\n",
            "Epoch 259/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 150.1653 - mse: 150.1653 - val_loss: 152.2279 - val_mse: 152.2279\n",
            "Epoch 260/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.3646 - mse: 148.3646 - val_loss: 154.5877 - val_mse: 154.5877\n",
            "Epoch 261/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.3932 - mse: 150.3932 - val_loss: 152.6238 - val_mse: 152.6238\n",
            "Epoch 262/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.2406 - mse: 149.2406 - val_loss: 155.4973 - val_mse: 155.4973\n",
            "Epoch 263/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.4993 - mse: 149.4993 - val_loss: 151.9676 - val_mse: 151.9676\n",
            "Epoch 264/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.5811 - mse: 148.5811 - val_loss: 155.2744 - val_mse: 155.2744\n",
            "Epoch 265/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.7742 - mse: 149.7742 - val_loss: 152.9294 - val_mse: 152.9294\n",
            "Epoch 266/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.5149 - mse: 148.5149 - val_loss: 152.5783 - val_mse: 152.5783\n",
            "Epoch 267/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.7708 - mse: 148.7708 - val_loss: 153.0773 - val_mse: 153.0773\n",
            "Epoch 268/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.3022 - mse: 148.3022 - val_loss: 154.4210 - val_mse: 154.4210\n",
            "Epoch 269/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.6354 - mse: 148.6354 - val_loss: 154.0253 - val_mse: 154.0253\n",
            "Epoch 270/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.0317 - mse: 149.0317 - val_loss: 153.5027 - val_mse: 153.5027\n",
            "Epoch 271/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.4521 - mse: 148.4521 - val_loss: 152.6671 - val_mse: 152.6671\n",
            "Epoch 272/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.9177 - mse: 149.9177 - val_loss: 153.1193 - val_mse: 153.1193\n",
            "Epoch 273/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.0243 - mse: 149.0243 - val_loss: 154.3625 - val_mse: 154.3625\n",
            "Epoch 274/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.3241 - mse: 148.3241 - val_loss: 155.6556 - val_mse: 155.6556\n",
            "Epoch 275/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.0195 - mse: 150.0195 - val_loss: 153.0973 - val_mse: 153.0973\n",
            "Epoch 276/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.9420 - mse: 148.9420 - val_loss: 155.1249 - val_mse: 155.1249\n",
            "Epoch 277/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.7339 - mse: 149.7339 - val_loss: 154.8992 - val_mse: 154.8992\n",
            "Epoch 278/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.4459 - mse: 149.4459 - val_loss: 152.1716 - val_mse: 152.1716\n",
            "Epoch 279/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.5207 - mse: 148.5207 - val_loss: 154.7508 - val_mse: 154.7508\n",
            "Epoch 280/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.8683 - mse: 148.8683 - val_loss: 155.0348 - val_mse: 155.0348\n",
            "Epoch 281/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.9318 - mse: 148.9318 - val_loss: 152.0417 - val_mse: 152.0417\n",
            "Epoch 282/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.2122 - mse: 148.2122 - val_loss: 155.0175 - val_mse: 155.0175\n",
            "Epoch 283/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.9387 - mse: 148.9387 - val_loss: 154.6939 - val_mse: 154.6939\n",
            "Epoch 284/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 150.0780 - mse: 150.0780 - val_loss: 152.8577 - val_mse: 152.8577\n",
            "Epoch 285/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.4786 - mse: 149.4786 - val_loss: 151.9486 - val_mse: 151.9486\n",
            "Epoch 286/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.2096 - mse: 148.2096 - val_loss: 155.2303 - val_mse: 155.2303\n",
            "Epoch 287/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.1800 - mse: 148.1800 - val_loss: 152.9769 - val_mse: 152.9769\n",
            "Epoch 288/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.7139 - mse: 149.7139 - val_loss: 152.8600 - val_mse: 152.8600\n",
            "Epoch 289/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.1396 - mse: 149.1396 - val_loss: 153.8150 - val_mse: 153.8150\n",
            "Epoch 290/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.3460 - mse: 147.3460 - val_loss: 153.5481 - val_mse: 153.5481\n",
            "Epoch 291/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.6158 - mse: 147.6158 - val_loss: 153.9600 - val_mse: 153.9600\n",
            "Epoch 292/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 149.1764 - mse: 149.1764 - val_loss: 152.5535 - val_mse: 152.5535\n",
            "Epoch 293/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 149.3362 - mse: 149.3362 - val_loss: 152.6846 - val_mse: 152.6846\n",
            "Epoch 294/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.3745 - mse: 148.3745 - val_loss: 154.8185 - val_mse: 154.8185\n",
            "Epoch 295/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.4448 - mse: 148.4448 - val_loss: 155.7541 - val_mse: 155.7541\n",
            "Epoch 296/400\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 150.1849 - mse: 150.1849 - val_loss: 152.7649 - val_mse: 152.7649\n",
            "Epoch 297/400\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 151.1516 - mse: 151.1516 - val_loss: 154.9686 - val_mse: 154.9686\n",
            "Epoch 298/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.1479 - mse: 149.1479 - val_loss: 153.8259 - val_mse: 153.8259\n",
            "Epoch 299/400\n",
            "75/75 [==============================] - 1s 10ms/step - loss: 147.9801 - mse: 147.9801 - val_loss: 153.2146 - val_mse: 153.2146\n",
            "Epoch 300/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.7575 - mse: 148.7575 - val_loss: 153.2384 - val_mse: 153.2384\n",
            "Epoch 301/400\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 147.9886 - mse: 147.9886 - val_loss: 155.1909 - val_mse: 155.1909\n",
            "Epoch 302/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 148.6008 - mse: 148.6008 - val_loss: 153.8595 - val_mse: 153.8595\n",
            "Epoch 303/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.4129 - mse: 148.4129 - val_loss: 153.4975 - val_mse: 153.4975\n",
            "Epoch 304/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.8600 - mse: 148.8600 - val_loss: 154.8555 - val_mse: 154.8555\n",
            "Epoch 305/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.4830 - mse: 147.4830 - val_loss: 153.8159 - val_mse: 153.8159\n",
            "Epoch 306/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.3756 - mse: 147.3756 - val_loss: 153.5653 - val_mse: 153.5653\n",
            "Epoch 307/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.8138 - mse: 147.8138 - val_loss: 153.4485 - val_mse: 153.4485\n",
            "Epoch 308/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.8871 - mse: 147.8871 - val_loss: 156.2793 - val_mse: 156.2793\n",
            "Epoch 309/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 149.2539 - mse: 149.2539 - val_loss: 155.2653 - val_mse: 155.2653\n",
            "Epoch 310/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.8148 - mse: 147.8148 - val_loss: 155.0454 - val_mse: 155.0454\n",
            "Epoch 311/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.2090 - mse: 148.2090 - val_loss: 152.8007 - val_mse: 152.8007\n",
            "Epoch 312/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.0351 - mse: 148.0351 - val_loss: 153.2824 - val_mse: 153.2824\n",
            "Epoch 313/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.7442 - mse: 146.7442 - val_loss: 154.8712 - val_mse: 154.8712\n",
            "Epoch 314/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.5076 - mse: 147.5076 - val_loss: 155.1076 - val_mse: 155.1076\n",
            "Epoch 315/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.3997 - mse: 147.3997 - val_loss: 156.6698 - val_mse: 156.6698\n",
            "Epoch 316/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.3981 - mse: 148.3981 - val_loss: 158.7010 - val_mse: 158.7010\n",
            "Epoch 317/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.3398 - mse: 147.3398 - val_loss: 153.8228 - val_mse: 153.8228\n",
            "Epoch 318/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.9326 - mse: 147.9326 - val_loss: 154.8798 - val_mse: 154.8798\n",
            "Epoch 319/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.9815 - mse: 148.9815 - val_loss: 155.4312 - val_mse: 155.4312\n",
            "Epoch 320/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.5295 - mse: 148.5295 - val_loss: 152.5492 - val_mse: 152.5492\n",
            "Epoch 321/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.5217 - mse: 147.5217 - val_loss: 153.5622 - val_mse: 153.5622\n",
            "Epoch 322/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.5481 - mse: 148.5481 - val_loss: 152.8287 - val_mse: 152.8287\n",
            "Epoch 323/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.0991 - mse: 148.0991 - val_loss: 154.1913 - val_mse: 154.1913\n",
            "Epoch 324/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.6045 - mse: 146.6045 - val_loss: 153.0529 - val_mse: 153.0529\n",
            "Epoch 325/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 148.5219 - mse: 148.5219 - val_loss: 158.3914 - val_mse: 158.3914\n",
            "Epoch 326/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.9530 - mse: 147.9530 - val_loss: 153.3790 - val_mse: 153.3790\n",
            "Epoch 327/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.4922 - mse: 147.4922 - val_loss: 155.8242 - val_mse: 155.8242\n",
            "Epoch 328/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 146.4660 - mse: 146.4660 - val_loss: 157.4698 - val_mse: 157.4698\n",
            "Epoch 329/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 149.0987 - mse: 149.0987 - val_loss: 152.7453 - val_mse: 152.7453\n",
            "Epoch 330/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.4990 - mse: 147.4990 - val_loss: 155.1682 - val_mse: 155.1682\n",
            "Epoch 331/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 149.4075 - mse: 149.4075 - val_loss: 155.9454 - val_mse: 155.9454\n",
            "Epoch 332/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.7457 - mse: 147.7457 - val_loss: 155.1382 - val_mse: 155.1382\n",
            "Epoch 333/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.2722 - mse: 147.2722 - val_loss: 159.8668 - val_mse: 159.8668\n",
            "Epoch 334/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.1717 - mse: 147.1717 - val_loss: 155.6886 - val_mse: 155.6886\n",
            "Epoch 335/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.5968 - mse: 147.5968 - val_loss: 151.9465 - val_mse: 151.9465\n",
            "Epoch 336/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.9197 - mse: 146.9197 - val_loss: 154.0076 - val_mse: 154.0076\n",
            "Epoch 337/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.7911 - mse: 146.7911 - val_loss: 152.9712 - val_mse: 152.9712\n",
            "Epoch 338/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.8148 - mse: 146.8148 - val_loss: 152.4877 - val_mse: 152.4877\n",
            "Epoch 339/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.6740 - mse: 147.6740 - val_loss: 154.0787 - val_mse: 154.0787\n",
            "Epoch 340/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.7575 - mse: 146.7575 - val_loss: 154.1020 - val_mse: 154.1020\n",
            "Epoch 341/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.4371 - mse: 147.4371 - val_loss: 155.6744 - val_mse: 155.6744\n",
            "Epoch 342/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.8895 - mse: 147.8895 - val_loss: 155.5301 - val_mse: 155.5301\n",
            "Epoch 343/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.0576 - mse: 147.0576 - val_loss: 153.0534 - val_mse: 153.0534\n",
            "Epoch 344/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.8060 - mse: 146.8060 - val_loss: 155.0040 - val_mse: 155.0040\n",
            "Epoch 345/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.1417 - mse: 146.1417 - val_loss: 153.0313 - val_mse: 153.0313\n",
            "Epoch 346/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.9020 - mse: 146.9020 - val_loss: 152.1514 - val_mse: 152.1514\n",
            "Epoch 347/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.6470 - mse: 147.6470 - val_loss: 152.7980 - val_mse: 152.7980\n",
            "Epoch 348/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.6690 - mse: 147.6690 - val_loss: 154.5576 - val_mse: 154.5576\n",
            "Epoch 349/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.5676 - mse: 147.5676 - val_loss: 157.2898 - val_mse: 157.2898\n",
            "Epoch 350/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.6235 - mse: 147.6235 - val_loss: 154.2782 - val_mse: 154.2782\n",
            "Epoch 351/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.8684 - mse: 146.8684 - val_loss: 151.5894 - val_mse: 151.5894\n",
            "Epoch 352/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.3894 - mse: 148.3894 - val_loss: 153.2092 - val_mse: 153.2092\n",
            "Epoch 353/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.9468 - mse: 146.9468 - val_loss: 153.7910 - val_mse: 153.7910\n",
            "Epoch 354/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.4854 - mse: 147.4854 - val_loss: 152.6282 - val_mse: 152.6282\n",
            "Epoch 355/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.5934 - mse: 147.5934 - val_loss: 152.1828 - val_mse: 152.1828\n",
            "Epoch 356/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.1058 - mse: 147.1058 - val_loss: 154.4514 - val_mse: 154.4514\n",
            "Epoch 357/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.0743 - mse: 148.0743 - val_loss: 152.1888 - val_mse: 152.1888\n",
            "Epoch 358/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.2580 - mse: 147.2580 - val_loss: 157.0656 - val_mse: 157.0656\n",
            "Epoch 359/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.2963 - mse: 148.2963 - val_loss: 153.1915 - val_mse: 153.1915\n",
            "Epoch 360/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.0942 - mse: 147.0942 - val_loss: 153.6911 - val_mse: 153.6911\n",
            "Epoch 361/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.8808 - mse: 146.8808 - val_loss: 152.3619 - val_mse: 152.3619\n",
            "Epoch 362/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.6980 - mse: 146.6980 - val_loss: 154.9948 - val_mse: 154.9948\n",
            "Epoch 363/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 145.9362 - mse: 145.9362 - val_loss: 156.4147 - val_mse: 156.4147\n",
            "Epoch 364/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.5781 - mse: 147.5781 - val_loss: 156.6414 - val_mse: 156.6414\n",
            "Epoch 365/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 145.8961 - mse: 145.8961 - val_loss: 153.7327 - val_mse: 153.7327\n",
            "Epoch 366/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 146.2323 - mse: 146.2323 - val_loss: 153.5721 - val_mse: 153.5721\n",
            "Epoch 367/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.4895 - mse: 147.4895 - val_loss: 153.7956 - val_mse: 153.7956\n",
            "Epoch 368/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 147.1796 - mse: 147.1796 - val_loss: 152.8829 - val_mse: 152.8829\n",
            "Epoch 369/400\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 146.2843 - mse: 146.2843 - val_loss: 154.4815 - val_mse: 154.4815\n",
            "Epoch 370/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.5486 - mse: 146.5486 - val_loss: 155.4367 - val_mse: 155.4367\n",
            "Epoch 371/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.7110 - mse: 146.7110 - val_loss: 154.9386 - val_mse: 154.9386\n",
            "Epoch 372/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.8047 - mse: 147.8047 - val_loss: 153.8073 - val_mse: 153.8073\n",
            "Epoch 373/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.5158 - mse: 146.5158 - val_loss: 157.8287 - val_mse: 157.8287\n",
            "Epoch 374/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.4693 - mse: 146.4693 - val_loss: 151.6570 - val_mse: 151.6570\n",
            "Epoch 375/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.2341 - mse: 145.2341 - val_loss: 152.1362 - val_mse: 152.1362\n",
            "Epoch 376/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.2964 - mse: 146.2964 - val_loss: 156.2406 - val_mse: 156.2406\n",
            "Epoch 377/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.9551 - mse: 146.9551 - val_loss: 155.6454 - val_mse: 155.6454\n",
            "Epoch 378/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.8784 - mse: 146.8784 - val_loss: 155.2920 - val_mse: 155.2920\n",
            "Epoch 379/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.5715 - mse: 145.5715 - val_loss: 155.1514 - val_mse: 155.1514\n",
            "Epoch 380/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.4939 - mse: 146.4939 - val_loss: 153.3331 - val_mse: 153.3331\n",
            "Epoch 381/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.3506 - mse: 146.3506 - val_loss: 156.4801 - val_mse: 156.4801\n",
            "Epoch 382/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.9940 - mse: 147.9940 - val_loss: 156.5070 - val_mse: 156.5070\n",
            "Epoch 383/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.0035 - mse: 148.0035 - val_loss: 157.3297 - val_mse: 157.3297\n",
            "Epoch 384/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.8310 - mse: 146.8310 - val_loss: 153.8531 - val_mse: 153.8531\n",
            "Epoch 385/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.4113 - mse: 145.4113 - val_loss: 156.3573 - val_mse: 156.3573\n",
            "Epoch 386/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.2688 - mse: 146.2688 - val_loss: 153.0135 - val_mse: 153.0135\n",
            "Epoch 387/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.6040 - mse: 145.6040 - val_loss: 159.4615 - val_mse: 159.4615\n",
            "Epoch 388/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.3892 - mse: 146.3892 - val_loss: 154.7081 - val_mse: 154.7081\n",
            "Epoch 389/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.2655 - mse: 146.2655 - val_loss: 152.4428 - val_mse: 152.4428\n",
            "Epoch 390/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 145.7176 - mse: 145.7176 - val_loss: 154.5580 - val_mse: 154.5580\n",
            "Epoch 391/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.8287 - mse: 145.8287 - val_loss: 154.2334 - val_mse: 154.2334\n",
            "Epoch 392/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.3358 - mse: 145.3358 - val_loss: 152.5036 - val_mse: 152.5036\n",
            "Epoch 393/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.7485 - mse: 146.7485 - val_loss: 154.4945 - val_mse: 154.4945\n",
            "Epoch 394/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 147.4397 - mse: 147.4397 - val_loss: 153.2776 - val_mse: 153.2776\n",
            "Epoch 395/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 146.0046 - mse: 146.0046 - val_loss: 154.0426 - val_mse: 154.0426\n",
            "Epoch 396/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 145.5355 - mse: 145.5355 - val_loss: 152.5983 - val_mse: 152.5983\n",
            "Epoch 397/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 146.8261 - mse: 146.8261 - val_loss: 157.0499 - val_mse: 157.0499\n",
            "Epoch 398/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 148.4884 - mse: 148.4884 - val_loss: 153.9083 - val_mse: 153.9083\n",
            "Epoch 399/400\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 145.7422 - mse: 145.7422 - val_loss: 158.8442 - val_mse: 158.8442\n",
            "Epoch 400/400\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 149.4369 - mse: 149.4369 - val_loss: 153.0274 - val_mse: 153.0274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "C8sfXOKrhcFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0df59eb-dde6-4606-ae63-02e4fe18e626"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 0s 3ms/step - loss: 146.1002 - mse: 146.1002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "y_predict = model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_predict, y_test)\n",
        "mae = mean_absolute_error(y_predict, y_test)\n",
        "y2 = r2_score(y_predict,y_test)\n",
        "print(\"MSE: \" , mse)\n",
        "print(\"MAE: \" , mae)\n",
        "print(\"R2: \" , y2)"
      ],
      "metadata": {
        "id": "jCG91pXrivju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461962e5-bf5c-4327-fe94-2dfcd51996cf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 0s 3ms/step\n",
            "MSE:  146.1001544934703\n",
            "MAE:  8.71728661564093\n",
            "R2:  -2.8655478200702613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_arr = np.array([[6,0,1,1,4]]) #El primer ejemplo del dataset original\n",
        "predicciones = model.predict(test_arr)\n",
        "print(predicciones)"
      ],
      "metadata": {
        "id": "pCqAN5uFzXmV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233f8c9b-2e39-464c-f6a2-cc9b602dbca6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n",
            "[[74.75188  62.738384 48.053734 57.055798 64.695465 78.56271  82.10305\n",
            "  78.25408  79.80514  76.96907  80.77302  82.00083  81.88461  79.738335\n",
            "  74.88367  78.19074  83.17446  79.101944 79.5241   78.09631  74.097046\n",
            "  71.31904  72.94934  61.539124 74.25062  74.480415 65.51843  51.244972\n",
            "  61.31016  55.770187 76.78978  80.87811  81.505295 77.95355  58.814644\n",
            "  48.75876  80.50881  84.8982   86.71807  84.48107  85.87727  84.56599\n",
            "  84.765396 81.85567 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataframe con rutas especificadas en una columna"
      ],
      "metadata": {
        "id": "YkHWiYNVJCVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Cargar el documento con los datos\n",
        "df2 = pd.read_excel('p2_serie_de_tiempo.xlsx')\n",
        "df2.drop(\"Unnamed: 0\",axis=1,inplace=True) #Elimina la columna indefinida, la que cuenta datos\n",
        "df2.drop(\"Fecha\",axis=1,inplace=True) #Elimina la columna de la fecha ya que tenemos de por si los datos de la fecha\n",
        "\n",
        "#Eliminacion de la columna fest\n",
        "df2.drop(\"fest\",axis=1,inplace=True) #Eliminar este\n",
        "\n",
        "#Reduccion de los años\n",
        "func = lambda x: (x - 2015)\n",
        "df2['Agno'] = df2['Agno'].apply(func)\n",
        "df2\n",
        "\n",
        "#Aproximacion de las velocidades\n",
        "df2.iloc[:,5:] = df2.iloc[:,5:].apply(lambda x: x.round(0).astype(int) )\n",
        "df2\n",
        "\n",
        "df2\n"
      ],
      "metadata": {
        "id": "IAhEdZueJFIK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "ead982c8-4efc-4a5f-a267-dae44aae9c51"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-f6d5676d6923>:15: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df2.iloc[:,5:] = df2.iloc[:,5:].apply(lambda x: x.round(0).astype(int) )\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Horas  Agno  Mes  Dia_mes  Dia_sem  20  21  22  23  24  ...  58  59  60  \\\n",
              "0         6     0    1        1        4  77  66  59  60  78  ...  65  47  90   \n",
              "1         7     0    1        1        4  70  66  36  48  50  ...  57  49  75   \n",
              "2         8     0    1        1        4  78  47  26  33  40  ...  59  47  73   \n",
              "3         9     0    1        1        4  71  48  33  43  59  ...  55  49  76   \n",
              "4        10     0    1        1        4  68  51  25  45  52  ...  50  55  76   \n",
              "...     ...   ...  ...      ...      ...  ..  ..  ..  ..  ..  ...  ..  ..  ..   \n",
              "7303     15     1   12       30        5  12  38  22  25  36  ...  62  38  58   \n",
              "7304     16     1   12       30        5  50  44  24  36  48  ...  28  42  60   \n",
              "7305     17     1   12       30        5  30  54  25  58  52  ...  41  48  81   \n",
              "7306     18     1   12       30        5  38  38  27  15  39  ...  26  37  37   \n",
              "7307     19     1   12       30        5  36  28  27  27  26  ...  28  47  32   \n",
              "\n",
              "      61  62  64  65  66  67  68  \n",
              "0     83  91  86  86  88  86  85  \n",
              "1     80  68  83  81  78  73  70  \n",
              "2     71  65  82  79  77  70  57  \n",
              "3     79  79  85  80  84  76  66  \n",
              "4     82  75  86  81  84  78  66  \n",
              "...   ..  ..  ..  ..  ..  ..  ..  \n",
              "7303  55  22  26  36  46  33  38  \n",
              "7304  30  88  89  86  84  90  90  \n",
              "7305  72  77  71  79  74  72  77  \n",
              "7306  37  37  37  37  37  37  37  \n",
              "7307  25  25  26  58  71  73  75  \n",
              "\n",
              "[7308 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c76d6229-3a32-48b9-8e67-d5ef31ebd96b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>77</td>\n",
              "      <td>66</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>78</td>\n",
              "      <td>...</td>\n",
              "      <td>65</td>\n",
              "      <td>47</td>\n",
              "      <td>90</td>\n",
              "      <td>83</td>\n",
              "      <td>91</td>\n",
              "      <td>86</td>\n",
              "      <td>86</td>\n",
              "      <td>88</td>\n",
              "      <td>86</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>66</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>50</td>\n",
              "      <td>...</td>\n",
              "      <td>57</td>\n",
              "      <td>49</td>\n",
              "      <td>75</td>\n",
              "      <td>80</td>\n",
              "      <td>68</td>\n",
              "      <td>83</td>\n",
              "      <td>81</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>78</td>\n",
              "      <td>47</td>\n",
              "      <td>26</td>\n",
              "      <td>33</td>\n",
              "      <td>40</td>\n",
              "      <td>...</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>73</td>\n",
              "      <td>71</td>\n",
              "      <td>65</td>\n",
              "      <td>82</td>\n",
              "      <td>79</td>\n",
              "      <td>77</td>\n",
              "      <td>70</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>71</td>\n",
              "      <td>48</td>\n",
              "      <td>33</td>\n",
              "      <td>43</td>\n",
              "      <td>59</td>\n",
              "      <td>...</td>\n",
              "      <td>55</td>\n",
              "      <td>49</td>\n",
              "      <td>76</td>\n",
              "      <td>79</td>\n",
              "      <td>79</td>\n",
              "      <td>85</td>\n",
              "      <td>80</td>\n",
              "      <td>84</td>\n",
              "      <td>76</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>68</td>\n",
              "      <td>51</td>\n",
              "      <td>25</td>\n",
              "      <td>45</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>50</td>\n",
              "      <td>55</td>\n",
              "      <td>76</td>\n",
              "      <td>82</td>\n",
              "      <td>75</td>\n",
              "      <td>86</td>\n",
              "      <td>81</td>\n",
              "      <td>84</td>\n",
              "      <td>78</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7303</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>62</td>\n",
              "      <td>38</td>\n",
              "      <td>58</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>26</td>\n",
              "      <td>36</td>\n",
              "      <td>46</td>\n",
              "      <td>33</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7304</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>44</td>\n",
              "      <td>24</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>42</td>\n",
              "      <td>60</td>\n",
              "      <td>30</td>\n",
              "      <td>88</td>\n",
              "      <td>89</td>\n",
              "      <td>86</td>\n",
              "      <td>84</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7305</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>54</td>\n",
              "      <td>25</td>\n",
              "      <td>58</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>41</td>\n",
              "      <td>48</td>\n",
              "      <td>81</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "      <td>71</td>\n",
              "      <td>79</td>\n",
              "      <td>74</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7306</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>27</td>\n",
              "      <td>15</td>\n",
              "      <td>39</td>\n",
              "      <td>...</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7307</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>36</td>\n",
              "      <td>28</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>26</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>47</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>58</td>\n",
              "      <td>71</td>\n",
              "      <td>73</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7308 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c76d6229-3a32-48b9-8e67-d5ef31ebd96b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c76d6229-3a32-48b9-8e67-d5ef31ebd96b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c76d6229-3a32-48b9-8e67-d5ef31ebd96b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns = df2.columns[5:]\n",
        "print(columns)"
      ],
      "metadata": {
        "id": "QSf5xma2MQvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c1ca06-414b-428e-dce1-20eabd18d357"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index([20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 35, 36, 38, 39, 40,\n",
            "       41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59,\n",
            "       60, 61, 62, 64, 65, 66, 67, 68],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_melted = df2.melt(id_vars=['Horas', 'Agno','Mes','Dia_mes','Dia_sem'],\n",
        "                    value_vars=columns,\n",
        "                    var_name='Numero_ruta',\n",
        "                    )\n",
        "df_melted"
      ],
      "metadata": {
        "id": "HwMpQi_tKxkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "46ac8876-42fc-4b9d-ffd5-646118f630b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Horas  Agno  Mes  Dia_mes  Dia_sem Numero_ruta  value\n",
              "0           6     0    1        1        4          20     77\n",
              "1           7     0    1        1        4          20     70\n",
              "2           8     0    1        1        4          20     78\n",
              "3           9     0    1        1        4          20     71\n",
              "4          10     0    1        1        4          20     68\n",
              "...       ...   ...  ...      ...      ...         ...    ...\n",
              "321547     15     1   12       30        5          68     38\n",
              "321548     16     1   12       30        5          68     90\n",
              "321549     17     1   12       30        5          68     77\n",
              "321550     18     1   12       30        5          68     37\n",
              "321551     19     1   12       30        5          68     75\n",
              "\n",
              "[321552 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-236a920f-79dd-43a5-a41b-b501e88f1e5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>Numero_ruta</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321547</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321548</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321549</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321550</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321551</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>68</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>321552 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-236a920f-79dd-43a5-a41b-b501e88f1e5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-236a920f-79dd-43a5-a41b-b501e88f1e5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-236a920f-79dd-43a5-a41b-b501e88f1e5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_melted.describe()"
      ],
      "metadata": {
        "id": "VRekej2sN2Es",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "adab3aa6-75e3-409f-b404-eac0929fbb29"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               Horas           Agno            Mes        Dia_mes  \\\n",
              "count  321552.000000  321552.000000  321552.000000  321552.000000   \n",
              "mean       12.500000       0.500000       6.532567      15.697318   \n",
              "std         4.031135       0.500001       3.446906       8.798544   \n",
              "min         6.000000       0.000000       1.000000       1.000000   \n",
              "25%         9.000000       0.000000       4.000000       8.000000   \n",
              "50%        12.500000       0.500000       7.000000      16.000000   \n",
              "75%        16.000000       1.000000      10.000000      23.000000   \n",
              "max        19.000000       1.000000      12.000000      31.000000   \n",
              "\n",
              "             Dia_sem          value  \n",
              "count  321552.000000  321552.000000  \n",
              "mean        3.005747      64.932981  \n",
              "std         1.414881      18.227130  \n",
              "min         1.000000       1.000000  \n",
              "25%         2.000000      51.000000  \n",
              "50%         3.000000      70.000000  \n",
              "75%         4.000000      80.000000  \n",
              "max         5.000000     119.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c445151-d6a1-43a4-ad36-a876bd545fb7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>321552.000000</td>\n",
              "      <td>321552.000000</td>\n",
              "      <td>321552.000000</td>\n",
              "      <td>321552.000000</td>\n",
              "      <td>321552.000000</td>\n",
              "      <td>321552.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>12.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>6.532567</td>\n",
              "      <td>15.697318</td>\n",
              "      <td>3.005747</td>\n",
              "      <td>64.932981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.031135</td>\n",
              "      <td>0.500001</td>\n",
              "      <td>3.446906</td>\n",
              "      <td>8.798544</td>\n",
              "      <td>1.414881</td>\n",
              "      <td>18.227130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>51.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>12.500000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>70.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>16.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>80.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>19.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>31.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>119.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c445151-d6a1-43a4-ad36-a876bd545fb7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c445151-d6a1-43a4-ad36-a876bd545fb7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c445151-d6a1-43a4-ad36-a876bd545fb7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_melted.iloc[:, 0:6].values.astype('float')\n",
        "y = df_melted.iloc[:, 6:].values.astype('float')"
      ],
      "metadata": {
        "id": "yVmCApzkQuD8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "peSxmQz-SPfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d58e25-4bf7-494c-fb52-fa6527f4b5f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6.,  0.,  1.,  1.,  4., 20.],\n",
              "       [ 7.,  0.,  1.,  1.,  4., 20.],\n",
              "       [ 8.,  0.,  1.,  1.,  4., 20.],\n",
              "       ...,\n",
              "       [17.,  1., 12., 30.,  5., 68.],\n",
              "       [18.,  1., 12., 30.,  5., 68.],\n",
              "       [19.,  1., 12., 30.,  5., 68.]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "SE0Y4VO5SPtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab784dc5-9fe7-4081-89d0-78cee08cbccc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[77.],\n",
              "       [70.],\n",
              "       [78.],\n",
              "       ...,\n",
              "       [77.],\n",
              "       [37.],\n",
              "       [75.]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "5jg8dEa2SZtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27c5336f-51cc-4feb-d50a-052c08f300b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(321552, 6)\n",
            "(321552, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.65, shuffle=True)\n",
        "print(\"Train Size\", X_train.shape)\n",
        "print(\"Rem Size\", X_rem.shape)\n",
        "print(\"Train Label\", y_train.shape)\n",
        "print(\"RemLabel: \", y_rem.shape)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, train_size=0.4285,shuffle=True)\n",
        "print(\"X Valid:\", X_valid.shape)\n",
        "print(\"X test:\", X_test.shape)\n",
        "print(\"y valid:\", y_valid.shape)\n",
        "print(\"y test:\" , y_test.shape)\n"
      ],
      "metadata": {
        "id": "CQNjZnY-S9sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867f3c7b-d696-4d4a-82c8-4fc0b4f2be88"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size (209008, 6)\n",
            "Rem Size (112544, 6)\n",
            "Train Label (209008, 1)\n",
            "RemLabel:  (112544, 1)\n",
            "-------------------------------------\n",
            "X Valid: (48225, 6)\n",
            "X test: (64319, 6)\n",
            "y valid: (48225, 1)\n",
            "y test: (64319, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(6, input_shape=(X.shape[-1],) , activation='relu'))\n",
        "model2.add(Dense(12, activation=\"relu\"))\n",
        "model2.add(Dense(24, activation=\"relu\"))\n",
        "model2.add(Dense(1)) #Capa de salida \n",
        "\n",
        "model2.summary()\n",
        "\n",
        "adam = Adam(0.007) #learning rate\n",
        "model2.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[\"mse\"] )\n",
        "\n",
        "H2 = model2.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=100, batch_size=128)\n"
      ],
      "metadata": {
        "id": "raUecyHlTIdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166c8578-4366-4f83-f3d3-cde91f2ae1ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 6)                 42        \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 12)                84        \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 24)                312       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 25        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 463\n",
            "Trainable params: 463\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 409.3811 - mse: 409.3811 - val_loss: 328.9625 - val_mse: 328.9625\n",
            "Epoch 2/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 321.2589 - mse: 321.2589 - val_loss: 311.6911 - val_mse: 311.6911\n",
            "Epoch 3/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 291.2437 - mse: 291.2437 - val_loss: 258.3237 - val_mse: 258.3237\n",
            "Epoch 4/100\n",
            "1633/1633 [==============================] - 6s 4ms/step - loss: 256.2208 - mse: 256.2208 - val_loss: 248.0604 - val_mse: 248.0604\n",
            "Epoch 5/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 241.6362 - mse: 241.6362 - val_loss: 242.0749 - val_mse: 242.0749\n",
            "Epoch 6/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 233.0838 - mse: 233.0838 - val_loss: 224.8710 - val_mse: 224.8710\n",
            "Epoch 7/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 227.7732 - mse: 227.7732 - val_loss: 225.3179 - val_mse: 225.3179\n",
            "Epoch 8/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 224.6919 - mse: 224.6919 - val_loss: 226.0463 - val_mse: 226.0463\n",
            "Epoch 9/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 223.3411 - mse: 223.3411 - val_loss: 213.8123 - val_mse: 213.8123\n",
            "Epoch 10/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 222.6146 - mse: 222.6146 - val_loss: 213.4064 - val_mse: 213.4064\n",
            "Epoch 11/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 220.3572 - mse: 220.3572 - val_loss: 220.6932 - val_mse: 220.6932\n",
            "Epoch 12/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 219.7553 - mse: 219.7553 - val_loss: 214.3407 - val_mse: 214.3407\n",
            "Epoch 13/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 219.8237 - mse: 219.8237 - val_loss: 214.4105 - val_mse: 214.4105\n",
            "Epoch 14/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 218.4071 - mse: 218.4071 - val_loss: 229.6949 - val_mse: 229.6949\n",
            "Epoch 15/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 217.1421 - mse: 217.1421 - val_loss: 216.3114 - val_mse: 216.3114\n",
            "Epoch 16/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 216.8766 - mse: 216.8766 - val_loss: 218.5266 - val_mse: 218.5266\n",
            "Epoch 17/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 216.3537 - mse: 216.3537 - val_loss: 224.9072 - val_mse: 224.9072\n",
            "Epoch 18/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 216.2420 - mse: 216.2420 - val_loss: 208.6707 - val_mse: 208.6707\n",
            "Epoch 19/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 215.7980 - mse: 215.7980 - val_loss: 218.2884 - val_mse: 218.2884\n",
            "Epoch 20/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 216.1402 - mse: 216.1402 - val_loss: 210.1760 - val_mse: 210.1760\n",
            "Epoch 21/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 216.0692 - mse: 216.0692 - val_loss: 216.8329 - val_mse: 216.8329\n",
            "Epoch 22/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 214.2852 - mse: 214.2852 - val_loss: 219.0634 - val_mse: 219.0634\n",
            "Epoch 23/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 213.9600 - mse: 213.9600 - val_loss: 227.7585 - val_mse: 227.7585\n",
            "Epoch 24/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 214.7450 - mse: 214.7450 - val_loss: 206.8921 - val_mse: 206.8921\n",
            "Epoch 25/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 214.3362 - mse: 214.3362 - val_loss: 212.1699 - val_mse: 212.1699\n",
            "Epoch 26/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 213.9706 - mse: 213.9706 - val_loss: 224.1674 - val_mse: 224.1674\n",
            "Epoch 27/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 213.7425 - mse: 213.7425 - val_loss: 225.4137 - val_mse: 225.4137\n",
            "Epoch 28/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 214.1976 - mse: 214.1976 - val_loss: 217.6090 - val_mse: 217.6090\n",
            "Epoch 29/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 212.7667 - mse: 212.7667 - val_loss: 217.6266 - val_mse: 217.6266\n",
            "Epoch 30/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 213.4194 - mse: 213.4194 - val_loss: 216.7223 - val_mse: 216.7223\n",
            "Epoch 31/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 213.4483 - mse: 213.4483 - val_loss: 215.7339 - val_mse: 215.7339\n",
            "Epoch 32/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 213.5598 - mse: 213.5598 - val_loss: 207.1688 - val_mse: 207.1688\n",
            "Epoch 33/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 213.4576 - mse: 213.4576 - val_loss: 207.7970 - val_mse: 207.7970\n",
            "Epoch 34/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 212.2777 - mse: 212.2777 - val_loss: 208.3127 - val_mse: 208.3127\n",
            "Epoch 35/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 212.2596 - mse: 212.2596 - val_loss: 211.3657 - val_mse: 211.3657\n",
            "Epoch 36/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 212.3357 - mse: 212.3357 - val_loss: 207.1791 - val_mse: 207.1791\n",
            "Epoch 37/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 211.4268 - mse: 211.4268 - val_loss: 206.1401 - val_mse: 206.1401\n",
            "Epoch 38/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 211.7206 - mse: 211.7206 - val_loss: 211.2443 - val_mse: 211.2443\n",
            "Epoch 39/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 211.0086 - mse: 211.0086 - val_loss: 205.5662 - val_mse: 205.5662\n",
            "Epoch 40/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 211.2638 - mse: 211.2638 - val_loss: 208.8339 - val_mse: 208.8339\n",
            "Epoch 41/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 209.9876 - mse: 209.9876 - val_loss: 212.0865 - val_mse: 212.0865\n",
            "Epoch 42/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 209.5613 - mse: 209.5613 - val_loss: 214.5681 - val_mse: 214.5681\n",
            "Epoch 43/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 210.4400 - mse: 210.4400 - val_loss: 205.3745 - val_mse: 205.3745\n",
            "Epoch 44/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 210.2620 - mse: 210.2620 - val_loss: 211.5203 - val_mse: 211.5203\n",
            "Epoch 45/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 209.8099 - mse: 209.8099 - val_loss: 203.8840 - val_mse: 203.8840\n",
            "Epoch 46/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 209.5724 - mse: 209.5724 - val_loss: 206.3034 - val_mse: 206.3034\n",
            "Epoch 47/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 209.3440 - mse: 209.3440 - val_loss: 208.2426 - val_mse: 208.2426\n",
            "Epoch 48/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 209.0952 - mse: 209.0952 - val_loss: 212.3164 - val_mse: 212.3164\n",
            "Epoch 49/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 208.7244 - mse: 208.7244 - val_loss: 203.8477 - val_mse: 203.8477\n",
            "Epoch 50/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 209.3696 - mse: 209.3696 - val_loss: 206.3177 - val_mse: 206.3177\n",
            "Epoch 51/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 208.6328 - mse: 208.6328 - val_loss: 203.5674 - val_mse: 203.5674\n",
            "Epoch 52/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 208.1142 - mse: 208.1142 - val_loss: 210.1171 - val_mse: 210.1171\n",
            "Epoch 53/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 208.0102 - mse: 208.0102 - val_loss: 205.9184 - val_mse: 205.9184\n",
            "Epoch 54/100\n",
            "1633/1633 [==============================] - 6s 4ms/step - loss: 207.6384 - mse: 207.6384 - val_loss: 207.6699 - val_mse: 207.6699\n",
            "Epoch 55/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 208.2008 - mse: 208.2008 - val_loss: 206.6403 - val_mse: 206.6403\n",
            "Epoch 56/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.6570 - mse: 207.6570 - val_loss: 209.7591 - val_mse: 209.7591\n",
            "Epoch 57/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 208.7668 - mse: 208.7668 - val_loss: 204.8514 - val_mse: 204.8514\n",
            "Epoch 58/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.6030 - mse: 207.6030 - val_loss: 204.7774 - val_mse: 204.7774\n",
            "Epoch 59/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 208.0759 - mse: 208.0759 - val_loss: 201.8113 - val_mse: 201.8113\n",
            "Epoch 60/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.9261 - mse: 206.9261 - val_loss: 201.0191 - val_mse: 201.0191\n",
            "Epoch 61/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.7305 - mse: 207.7305 - val_loss: 208.2285 - val_mse: 208.2285\n",
            "Epoch 62/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.1980 - mse: 207.1980 - val_loss: 201.8778 - val_mse: 201.8778\n",
            "Epoch 63/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.0691 - mse: 207.0691 - val_loss: 205.4843 - val_mse: 205.4843\n",
            "Epoch 64/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 206.9876 - mse: 206.9876 - val_loss: 205.4529 - val_mse: 205.4529\n",
            "Epoch 65/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.3900 - mse: 207.3900 - val_loss: 206.0151 - val_mse: 206.0151\n",
            "Epoch 66/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.3693 - mse: 207.3693 - val_loss: 201.3810 - val_mse: 201.3810\n",
            "Epoch 67/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.6916 - mse: 206.6916 - val_loss: 209.0012 - val_mse: 209.0012\n",
            "Epoch 68/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.2062 - mse: 207.2062 - val_loss: 204.6679 - val_mse: 204.6679\n",
            "Epoch 69/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.4756 - mse: 207.4756 - val_loss: 207.5608 - val_mse: 207.5608\n",
            "Epoch 70/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.3921 - mse: 206.3921 - val_loss: 204.2248 - val_mse: 204.2248\n",
            "Epoch 71/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.1072 - mse: 207.1072 - val_loss: 204.4517 - val_mse: 204.4517\n",
            "Epoch 72/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.4474 - mse: 206.4474 - val_loss: 210.0517 - val_mse: 210.0517\n",
            "Epoch 73/100\n",
            "1633/1633 [==============================] - 6s 4ms/step - loss: 207.4840 - mse: 207.4840 - val_loss: 203.1516 - val_mse: 203.1516\n",
            "Epoch 74/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.0202 - mse: 207.0202 - val_loss: 205.2501 - val_mse: 205.2501\n",
            "Epoch 75/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.7138 - mse: 206.7138 - val_loss: 211.4176 - val_mse: 211.4176\n",
            "Epoch 76/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 206.3523 - mse: 206.3523 - val_loss: 206.5645 - val_mse: 206.5645\n",
            "Epoch 77/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.5434 - mse: 206.5434 - val_loss: 213.3625 - val_mse: 213.3625\n",
            "Epoch 78/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.9035 - mse: 206.9035 - val_loss: 200.0176 - val_mse: 200.0176\n",
            "Epoch 79/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.1751 - mse: 206.1751 - val_loss: 206.3440 - val_mse: 206.3440\n",
            "Epoch 80/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.9892 - mse: 205.9892 - val_loss: 201.4131 - val_mse: 201.4131\n",
            "Epoch 81/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.5269 - mse: 206.5269 - val_loss: 205.1949 - val_mse: 205.1949\n",
            "Epoch 82/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.5854 - mse: 206.5854 - val_loss: 206.7547 - val_mse: 206.7547\n",
            "Epoch 83/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 206.0068 - mse: 206.0068 - val_loss: 204.6238 - val_mse: 204.6238\n",
            "Epoch 84/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.8576 - mse: 205.8576 - val_loss: 202.8001 - val_mse: 202.8001\n",
            "Epoch 85/100\n",
            "1633/1633 [==============================] - 6s 4ms/step - loss: 206.1279 - mse: 206.1279 - val_loss: 201.9553 - val_mse: 201.9553\n",
            "Epoch 86/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.2635 - mse: 206.2635 - val_loss: 203.7161 - val_mse: 203.7161\n",
            "Epoch 87/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.0570 - mse: 206.0570 - val_loss: 207.2702 - val_mse: 207.2702\n",
            "Epoch 88/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.7843 - mse: 205.7843 - val_loss: 207.9389 - val_mse: 207.9389\n",
            "Epoch 89/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 207.0681 - mse: 207.0681 - val_loss: 208.2570 - val_mse: 208.2570\n",
            "Epoch 90/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 205.6860 - mse: 205.6860 - val_loss: 209.8881 - val_mse: 209.8881\n",
            "Epoch 91/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.7661 - mse: 205.7661 - val_loss: 203.7130 - val_mse: 203.7130\n",
            "Epoch 92/100\n",
            "1633/1633 [==============================] - 6s 4ms/step - loss: 206.3349 - mse: 206.3349 - val_loss: 204.6859 - val_mse: 204.6859\n",
            "Epoch 93/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.8252 - mse: 205.8252 - val_loss: 207.8642 - val_mse: 207.8642\n",
            "Epoch 94/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.7683 - mse: 205.7683 - val_loss: 206.7844 - val_mse: 206.7844\n",
            "Epoch 95/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 206.0007 - mse: 206.0007 - val_loss: 209.2790 - val_mse: 209.2790\n",
            "Epoch 96/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.7894 - mse: 205.7894 - val_loss: 212.1126 - val_mse: 212.1126\n",
            "Epoch 97/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.0762 - mse: 206.0762 - val_loss: 209.0829 - val_mse: 209.0829\n",
            "Epoch 98/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 205.2781 - mse: 205.2781 - val_loss: 202.3585 - val_mse: 202.3585\n",
            "Epoch 99/100\n",
            "1633/1633 [==============================] - 6s 3ms/step - loss: 205.5873 - mse: 205.5873 - val_loss: 206.8838 - val_mse: 206.8838\n",
            "Epoch 100/100\n",
            "1633/1633 [==============================] - 5s 3ms/step - loss: 206.1168 - mse: 206.1168 - val_loss: 202.4030 - val_mse: 202.4030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model2.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "-qYMxXv4Tahr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f5b6b50-d6f4-431f-eaf0-bce36f96a39d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2010/2010 [==============================] - 4s 2ms/step - loss: 202.6173 - mse: 202.6173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model2.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_predict, y_test)\n",
        "mae = mean_absolute_error(y_predict, y_test)\n",
        "y2 = r2_score(y_predict,y_test)\n",
        "print(\"MSE: \" , mse)\n",
        "print(\"MAE: \" , mae)\n",
        "print(\"R2: \" , y2)"
      ],
      "metadata": {
        "id": "nZSqYcjJTgkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e8d490-a7d1-42fb-a2a1-0fdee32889da"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2010/2010 [==============================] - 4s 2ms/step\n",
            "MSE:  202.61733336509104\n",
            "MAE:  10.960609502935737\n",
            "R2:  -0.6207774871565528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicciones = []\n",
        "for cols in df2.columns[5:]:\n",
        "  test_arr = np.array([[6,0,1,1,4,cols]]) #El primer ejemplo del dataset original\n",
        "  predicciones.append(model2.predict(test_arr))\n",
        "print(np.array(predicciones))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zN_K6-R1qCp",
        "outputId": "870aec32-9f4c-4e71-d016-85bfbfe46460"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "[[[80.09941 ]]\n",
            "\n",
            " [[73.406204]]\n",
            "\n",
            " [[53.352737]]\n",
            "\n",
            " [[61.1099  ]]\n",
            "\n",
            " [[68.86707 ]]\n",
            "\n",
            " [[75.70962 ]]\n",
            "\n",
            " [[78.167755]]\n",
            "\n",
            " [[79.29524 ]]\n",
            "\n",
            " [[79.646774]]\n",
            "\n",
            " [[79.99834 ]]\n",
            "\n",
            " [[80.34984 ]]\n",
            "\n",
            " [[81.51645 ]]\n",
            "\n",
            " [[82.59139 ]]\n",
            "\n",
            " [[84.35781 ]]\n",
            "\n",
            " [[82.45065 ]]\n",
            "\n",
            " [[78.63636 ]]\n",
            "\n",
            " [[80.29297 ]]\n",
            "\n",
            " [[81.981   ]]\n",
            "\n",
            " [[82.066795]]\n",
            "\n",
            " [[79.98147 ]]\n",
            "\n",
            " [[77.89612 ]]\n",
            "\n",
            " [[75.81077 ]]\n",
            "\n",
            " [[73.725426]]\n",
            "\n",
            " [[71.64007 ]]\n",
            "\n",
            " [[69.9229  ]]\n",
            "\n",
            " [[68.25976 ]]\n",
            "\n",
            " [[66.59663 ]]\n",
            "\n",
            " [[64.93352 ]]\n",
            "\n",
            " [[65.410065]]\n",
            "\n",
            " [[66.13591 ]]\n",
            "\n",
            " [[66.861755]]\n",
            "\n",
            " [[67.5876  ]]\n",
            "\n",
            " [[68.31345 ]]\n",
            "\n",
            " [[69.76513 ]]\n",
            "\n",
            " [[70.49098 ]]\n",
            "\n",
            " [[71.21683 ]]\n",
            "\n",
            " [[71.94266 ]]\n",
            "\n",
            " [[72.66851 ]]\n",
            "\n",
            " [[73.394356]]\n",
            "\n",
            " [[74.84604 ]]\n",
            "\n",
            " [[75.571884]]\n",
            "\n",
            " [[76.29773 ]]\n",
            "\n",
            " [[77.023575]]\n",
            "\n",
            " [[77.74942 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataframe original con una red neuronal FNN"
      ],
      "metadata": {
        "id": "fMqo4gMNIFhe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar el documento con los datos\n",
        "df3 = pd.read_excel('p2_serie_de_tiempo.xlsx')\n",
        "df3.drop(\"Unnamed: 0\",axis=1,inplace=True) #Elimina la columna indefinida, la que cuenta datos\n",
        "df3.drop(\"Fecha\",axis=1,inplace=True) #Elimina la columna de la fecha ya que tenemos de por si los datos de la fecha\n",
        "\n",
        "#Eliminacion de la columna fest\n",
        "df3.drop(\"fest\",axis=1,inplace=True) #Eliminar este\n",
        "\n",
        "#Reduccion de los años\n",
        "func = lambda x: (x - 2015)\n",
        "df3['Agno'] = df3['Agno'].apply(func)\n",
        "df3\n",
        "\n",
        "#Aproximacion de las velocidades\n",
        "df3.iloc[:,5:] = df3.iloc[:,5:].apply(lambda x: x.round(0).astype(int) )\n",
        "df3\n",
        "\n",
        "df3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "_na8_lMcIIuo",
        "outputId": "acefdb8e-2c80-4813-8f7a-62a7476c02dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-3fca7ef5851a>:15: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  df3.iloc[:,5:] = df3.iloc[:,5:].apply(lambda x: x.round(0).astype(int) )\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Horas  Agno  Mes  Dia_mes  Dia_sem  20  21  22  23  24  ...  58  59  60  \\\n",
              "0         6     0    1        1        4  77  66  59  60  78  ...  65  47  90   \n",
              "1         7     0    1        1        4  70  66  36  48  50  ...  57  49  75   \n",
              "2         8     0    1        1        4  78  47  26  33  40  ...  59  47  73   \n",
              "3         9     0    1        1        4  71  48  33  43  59  ...  55  49  76   \n",
              "4        10     0    1        1        4  68  51  25  45  52  ...  50  55  76   \n",
              "...     ...   ...  ...      ...      ...  ..  ..  ..  ..  ..  ...  ..  ..  ..   \n",
              "7303     15     1   12       30        5  12  38  22  25  36  ...  62  38  58   \n",
              "7304     16     1   12       30        5  50  44  24  36  48  ...  28  42  60   \n",
              "7305     17     1   12       30        5  30  54  25  58  52  ...  41  48  81   \n",
              "7306     18     1   12       30        5  38  38  27  15  39  ...  26  37  37   \n",
              "7307     19     1   12       30        5  36  28  27  27  26  ...  28  47  32   \n",
              "\n",
              "      61  62  64  65  66  67  68  \n",
              "0     83  91  86  86  88  86  85  \n",
              "1     80  68  83  81  78  73  70  \n",
              "2     71  65  82  79  77  70  57  \n",
              "3     79  79  85  80  84  76  66  \n",
              "4     82  75  86  81  84  78  66  \n",
              "...   ..  ..  ..  ..  ..  ..  ..  \n",
              "7303  55  22  26  36  46  33  38  \n",
              "7304  30  88  89  86  84  90  90  \n",
              "7305  72  77  71  79  74  72  77  \n",
              "7306  37  37  37  37  37  37  37  \n",
              "7307  25  25  26  58  71  73  75  \n",
              "\n",
              "[7308 rows x 49 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0ebd2b4-a840-446c-9195-ab6185dea026\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Horas</th>\n",
              "      <th>Agno</th>\n",
              "      <th>Mes</th>\n",
              "      <th>Dia_mes</th>\n",
              "      <th>Dia_sem</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>...</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>77</td>\n",
              "      <td>66</td>\n",
              "      <td>59</td>\n",
              "      <td>60</td>\n",
              "      <td>78</td>\n",
              "      <td>...</td>\n",
              "      <td>65</td>\n",
              "      <td>47</td>\n",
              "      <td>90</td>\n",
              "      <td>83</td>\n",
              "      <td>91</td>\n",
              "      <td>86</td>\n",
              "      <td>86</td>\n",
              "      <td>88</td>\n",
              "      <td>86</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>70</td>\n",
              "      <td>66</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>50</td>\n",
              "      <td>...</td>\n",
              "      <td>57</td>\n",
              "      <td>49</td>\n",
              "      <td>75</td>\n",
              "      <td>80</td>\n",
              "      <td>68</td>\n",
              "      <td>83</td>\n",
              "      <td>81</td>\n",
              "      <td>78</td>\n",
              "      <td>73</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>78</td>\n",
              "      <td>47</td>\n",
              "      <td>26</td>\n",
              "      <td>33</td>\n",
              "      <td>40</td>\n",
              "      <td>...</td>\n",
              "      <td>59</td>\n",
              "      <td>47</td>\n",
              "      <td>73</td>\n",
              "      <td>71</td>\n",
              "      <td>65</td>\n",
              "      <td>82</td>\n",
              "      <td>79</td>\n",
              "      <td>77</td>\n",
              "      <td>70</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>71</td>\n",
              "      <td>48</td>\n",
              "      <td>33</td>\n",
              "      <td>43</td>\n",
              "      <td>59</td>\n",
              "      <td>...</td>\n",
              "      <td>55</td>\n",
              "      <td>49</td>\n",
              "      <td>76</td>\n",
              "      <td>79</td>\n",
              "      <td>79</td>\n",
              "      <td>85</td>\n",
              "      <td>80</td>\n",
              "      <td>84</td>\n",
              "      <td>76</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>68</td>\n",
              "      <td>51</td>\n",
              "      <td>25</td>\n",
              "      <td>45</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>50</td>\n",
              "      <td>55</td>\n",
              "      <td>76</td>\n",
              "      <td>82</td>\n",
              "      <td>75</td>\n",
              "      <td>86</td>\n",
              "      <td>81</td>\n",
              "      <td>84</td>\n",
              "      <td>78</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7303</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>38</td>\n",
              "      <td>22</td>\n",
              "      <td>25</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>62</td>\n",
              "      <td>38</td>\n",
              "      <td>58</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>26</td>\n",
              "      <td>36</td>\n",
              "      <td>46</td>\n",
              "      <td>33</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7304</th>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>50</td>\n",
              "      <td>44</td>\n",
              "      <td>24</td>\n",
              "      <td>36</td>\n",
              "      <td>48</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>42</td>\n",
              "      <td>60</td>\n",
              "      <td>30</td>\n",
              "      <td>88</td>\n",
              "      <td>89</td>\n",
              "      <td>86</td>\n",
              "      <td>84</td>\n",
              "      <td>90</td>\n",
              "      <td>90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7305</th>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>30</td>\n",
              "      <td>54</td>\n",
              "      <td>25</td>\n",
              "      <td>58</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>41</td>\n",
              "      <td>48</td>\n",
              "      <td>81</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "      <td>71</td>\n",
              "      <td>79</td>\n",
              "      <td>74</td>\n",
              "      <td>72</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7306</th>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>27</td>\n",
              "      <td>15</td>\n",
              "      <td>39</td>\n",
              "      <td>...</td>\n",
              "      <td>26</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7307</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>30</td>\n",
              "      <td>5</td>\n",
              "      <td>36</td>\n",
              "      <td>28</td>\n",
              "      <td>27</td>\n",
              "      <td>27</td>\n",
              "      <td>26</td>\n",
              "      <td>...</td>\n",
              "      <td>28</td>\n",
              "      <td>47</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>58</td>\n",
              "      <td>71</td>\n",
              "      <td>73</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7308 rows × 49 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0ebd2b4-a840-446c-9195-ab6185dea026')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0ebd2b4-a840-446c-9195-ab6185dea026 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0ebd2b4-a840-446c-9195-ab6185dea026');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, 0:5].values\n",
        "y = df.iloc[:, 5:].values\n",
        "\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.65, random_state=0, shuffle=True)\n",
        "print(\"Train Size\", X_train.shape)\n",
        "print(\"Rem Size\", X_rem.shape)\n",
        "print(\"Train Label\", y_train.shape)\n",
        "print(\"RemLabel: \", y_rem.shape)\n",
        "\n",
        "print(\"-------------------------------------\")\n",
        "\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, train_size=0.4285,shuffle=True)\n",
        "print(\"X Valid:\", X_valid.shape)\n",
        "print(\"X test:\", X_test.shape)\n",
        "print(\"y valid:\", y_valid.shape)\n",
        "print(\"y test:\" , y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwHAha-QISLQ",
        "outputId": "7ded8758-78bc-43db-d412-c3673c33438a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size (4750, 5)\n",
            "Rem Size (2558, 5)\n",
            "Train Label (4750, 44)\n",
            "RemLabel:  (2558, 44)\n",
            "-------------------------------------\n",
            "X Valid: (1096, 5)\n",
            "X test: (1462, 5)\n",
            "y valid: (1096, 44)\n",
            "y test: (1462, 44)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhI_i6JDIo9c",
        "outputId": "90a8fef2-7d1f-4a2b-f152-c6fa5e05360a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 6,  0,  1,  1,  4],\n",
              "       [ 7,  0,  1,  1,  4],\n",
              "       [ 8,  0,  1,  1,  4],\n",
              "       ...,\n",
              "       [17,  1, 12, 30,  5],\n",
              "       [18,  1, 12, 30,  5],\n",
              "       [19,  1, 12, 30,  5]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAB3R2XgIsti",
        "outputId": "96db49cd-fb2c-4b05-eb14-53f02a9cf2c1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[77, 66, 59, ..., 88, 86, 85],\n",
              "       [70, 66, 36, ..., 78, 73, 70],\n",
              "       [78, 47, 26, ..., 77, 70, 57],\n",
              "       ...,\n",
              "       [30, 54, 25, ..., 74, 72, 77],\n",
              "       [38, 38, 27, ..., 37, 37, 37],\n",
              "       [36, 28, 27, ..., 71, 73, 75]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model3 = Sequential()\n",
        "\n",
        "model3.add(Dense(6, input_shape=(X.shape[-1],) , activation='relu'))\n",
        "model3.add(Dropout(0.002))\n",
        "\n",
        "model3.add(Dense(12, activation=\"relu\"))\n",
        "model3.add(Dropout(0.002))\n",
        "\n",
        "model3.add(Dense(24, activation=\"relu\"))\n",
        "model3.add(Dropout(0.002))\n",
        "\n",
        "model3.add(Dense(48, activation=\"relu\"))\n",
        "model3.add(Dropout(0.002))\n",
        "\n",
        "model3.add(Dense(44)) #Capa de salida \n",
        "\n",
        "model3.summary()\n",
        "\n",
        "model3.compile(loss=\"mean_squared_error\", optimizer=RMSprop(0.007), metrics=[\"mse\"] )\n",
        "\n",
        "H3 = model3.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=350, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhTKp8RyIuMb",
        "outputId": "509b46b0-4984-4b2e-cead-a6a1bde5201a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_9 (Dense)             (None, 6)                 36        \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 6)                 0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 12)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 24)                312       \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 24)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 48)                1200      \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 48)                0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 44)                2156      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,788\n",
            "Trainable params: 3,788\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/350\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 721.8046 - mse: 721.8046 - val_loss: 406.6172 - val_mse: 406.6172\n",
            "Epoch 2/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 399.7091 - mse: 399.7091 - val_loss: 389.3512 - val_mse: 389.3512\n",
            "Epoch 3/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 374.4428 - mse: 374.4428 - val_loss: 331.0188 - val_mse: 331.0188\n",
            "Epoch 4/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 334.8712 - mse: 334.8712 - val_loss: 327.8938 - val_mse: 327.8938\n",
            "Epoch 5/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 309.0276 - mse: 309.0276 - val_loss: 263.8815 - val_mse: 263.8815\n",
            "Epoch 6/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 284.7755 - mse: 284.7755 - val_loss: 324.0613 - val_mse: 324.0613\n",
            "Epoch 7/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 260.5441 - mse: 260.5441 - val_loss: 231.4340 - val_mse: 231.4340\n",
            "Epoch 8/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 239.9033 - mse: 239.9033 - val_loss: 209.5780 - val_mse: 209.5780\n",
            "Epoch 9/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 224.3436 - mse: 224.3436 - val_loss: 207.2390 - val_mse: 207.2390\n",
            "Epoch 10/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 217.2225 - mse: 217.2225 - val_loss: 206.1623 - val_mse: 206.1623\n",
            "Epoch 11/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 213.2628 - mse: 213.2628 - val_loss: 200.4865 - val_mse: 200.4865\n",
            "Epoch 12/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.6617 - mse: 208.6617 - val_loss: 198.0296 - val_mse: 198.0296\n",
            "Epoch 13/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.4215 - mse: 206.4215 - val_loss: 214.1242 - val_mse: 214.1242\n",
            "Epoch 14/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 203.2442 - mse: 203.2442 - val_loss: 205.7238 - val_mse: 205.7238\n",
            "Epoch 15/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 201.0315 - mse: 201.0315 - val_loss: 210.2738 - val_mse: 210.2738\n",
            "Epoch 16/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 196.6537 - mse: 196.6537 - val_loss: 191.0846 - val_mse: 191.0846\n",
            "Epoch 17/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 194.3121 - mse: 194.3121 - val_loss: 200.1389 - val_mse: 200.1389\n",
            "Epoch 18/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 191.9783 - mse: 191.9783 - val_loss: 216.0226 - val_mse: 216.0226\n",
            "Epoch 19/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 189.1681 - mse: 189.1681 - val_loss: 190.0451 - val_mse: 190.0451\n",
            "Epoch 20/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 187.8682 - mse: 187.8682 - val_loss: 176.7382 - val_mse: 176.7382\n",
            "Epoch 21/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 185.9549 - mse: 185.9549 - val_loss: 191.2957 - val_mse: 191.2957\n",
            "Epoch 22/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 184.3011 - mse: 184.3011 - val_loss: 182.4797 - val_mse: 182.4797\n",
            "Epoch 23/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 181.7886 - mse: 181.7886 - val_loss: 192.3968 - val_mse: 192.3968\n",
            "Epoch 24/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 181.0060 - mse: 181.0060 - val_loss: 204.3903 - val_mse: 204.3903\n",
            "Epoch 25/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 181.9952 - mse: 181.9952 - val_loss: 178.9346 - val_mse: 178.9346\n",
            "Epoch 26/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 180.4413 - mse: 180.4413 - val_loss: 171.9934 - val_mse: 171.9934\n",
            "Epoch 27/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 180.5518 - mse: 180.5518 - val_loss: 170.5704 - val_mse: 170.5704\n",
            "Epoch 28/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 179.3987 - mse: 179.3987 - val_loss: 176.8800 - val_mse: 176.8800\n",
            "Epoch 29/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 179.1882 - mse: 179.1882 - val_loss: 203.3806 - val_mse: 203.3806\n",
            "Epoch 30/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 177.3286 - mse: 177.3286 - val_loss: 193.5612 - val_mse: 193.5612\n",
            "Epoch 31/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 178.5188 - mse: 178.5188 - val_loss: 174.9314 - val_mse: 174.9314\n",
            "Epoch 32/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 177.6203 - mse: 177.6203 - val_loss: 182.6241 - val_mse: 182.6241\n",
            "Epoch 33/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 176.3858 - mse: 176.3858 - val_loss: 187.8306 - val_mse: 187.8306\n",
            "Epoch 34/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 175.6949 - mse: 175.6949 - val_loss: 174.8002 - val_mse: 174.8002\n",
            "Epoch 35/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 176.0245 - mse: 176.0245 - val_loss: 171.1933 - val_mse: 171.1933\n",
            "Epoch 36/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 176.6887 - mse: 176.6887 - val_loss: 165.6867 - val_mse: 165.6867\n",
            "Epoch 37/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 174.6127 - mse: 174.6127 - val_loss: 211.4424 - val_mse: 211.4424\n",
            "Epoch 38/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 172.3998 - mse: 172.3998 - val_loss: 177.3440 - val_mse: 177.3440\n",
            "Epoch 39/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 176.5491 - mse: 176.5491 - val_loss: 180.8946 - val_mse: 180.8946\n",
            "Epoch 40/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 172.5576 - mse: 172.5576 - val_loss: 188.8313 - val_mse: 188.8313\n",
            "Epoch 41/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 174.0558 - mse: 174.0558 - val_loss: 167.6752 - val_mse: 167.6752\n",
            "Epoch 42/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 173.2529 - mse: 173.2529 - val_loss: 170.2149 - val_mse: 170.2149\n",
            "Epoch 43/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 174.1510 - mse: 174.1510 - val_loss: 176.2052 - val_mse: 176.2052\n",
            "Epoch 44/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 171.1140 - mse: 171.1140 - val_loss: 174.1675 - val_mse: 174.1675\n",
            "Epoch 45/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 172.6629 - mse: 172.6629 - val_loss: 160.2570 - val_mse: 160.2570\n",
            "Epoch 46/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 172.5536 - mse: 172.5536 - val_loss: 208.6384 - val_mse: 208.6384\n",
            "Epoch 47/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 171.5390 - mse: 171.5390 - val_loss: 165.6309 - val_mse: 165.6309\n",
            "Epoch 48/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 171.4243 - mse: 171.4243 - val_loss: 170.7050 - val_mse: 170.7050\n",
            "Epoch 49/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 170.8029 - mse: 170.8029 - val_loss: 195.3484 - val_mse: 195.3484\n",
            "Epoch 50/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 170.6706 - mse: 170.6706 - val_loss: 174.8165 - val_mse: 174.8165\n",
            "Epoch 51/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 170.5412 - mse: 170.5412 - val_loss: 212.2016 - val_mse: 212.2016\n",
            "Epoch 52/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.1446 - mse: 169.1446 - val_loss: 168.8802 - val_mse: 168.8802\n",
            "Epoch 53/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.8748 - mse: 169.8748 - val_loss: 181.7977 - val_mse: 181.7977\n",
            "Epoch 54/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 170.1105 - mse: 170.1105 - val_loss: 206.2032 - val_mse: 206.2032\n",
            "Epoch 55/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 169.5496 - mse: 169.5496 - val_loss: 174.1897 - val_mse: 174.1897\n",
            "Epoch 56/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.5937 - mse: 168.5937 - val_loss: 162.2423 - val_mse: 162.2423\n",
            "Epoch 57/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.6855 - mse: 169.6855 - val_loss: 160.1411 - val_mse: 160.1411\n",
            "Epoch 58/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 169.0407 - mse: 169.0407 - val_loss: 173.6254 - val_mse: 173.6254\n",
            "Epoch 59/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.4045 - mse: 169.4045 - val_loss: 165.2657 - val_mse: 165.2657\n",
            "Epoch 60/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.8366 - mse: 168.8366 - val_loss: 181.8637 - val_mse: 181.8637\n",
            "Epoch 61/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.8362 - mse: 168.8362 - val_loss: 179.7799 - val_mse: 179.7799\n",
            "Epoch 62/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 169.6235 - mse: 169.6235 - val_loss: 168.8426 - val_mse: 168.8426\n",
            "Epoch 63/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.9989 - mse: 168.9989 - val_loss: 171.2211 - val_mse: 171.2211\n",
            "Epoch 64/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 168.7869 - mse: 168.7869 - val_loss: 185.1394 - val_mse: 185.1394\n",
            "Epoch 65/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 168.4128 - mse: 168.4128 - val_loss: 159.7218 - val_mse: 159.7218\n",
            "Epoch 66/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 167.2232 - mse: 167.2232 - val_loss: 186.5821 - val_mse: 186.5821\n",
            "Epoch 67/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 167.7679 - mse: 167.7679 - val_loss: 181.9356 - val_mse: 181.9356\n",
            "Epoch 68/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 168.1026 - mse: 168.1026 - val_loss: 168.3819 - val_mse: 168.3819\n",
            "Epoch 69/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.3540 - mse: 166.3540 - val_loss: 186.4817 - val_mse: 186.4817\n",
            "Epoch 70/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.8924 - mse: 166.8924 - val_loss: 162.9320 - val_mse: 162.9320\n",
            "Epoch 71/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 166.7661 - mse: 166.7661 - val_loss: 192.1134 - val_mse: 192.1134\n",
            "Epoch 72/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 165.5446 - mse: 165.5446 - val_loss: 168.1726 - val_mse: 168.1726\n",
            "Epoch 73/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 167.2069 - mse: 167.2069 - val_loss: 159.0049 - val_mse: 159.0049\n",
            "Epoch 74/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 166.2566 - mse: 166.2566 - val_loss: 165.8222 - val_mse: 165.8222\n",
            "Epoch 75/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 165.1206 - mse: 165.1206 - val_loss: 162.8884 - val_mse: 162.8884\n",
            "Epoch 76/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 167.4318 - mse: 167.4318 - val_loss: 158.0860 - val_mse: 158.0860\n",
            "Epoch 77/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 165.8811 - mse: 165.8811 - val_loss: 156.4438 - val_mse: 156.4438\n",
            "Epoch 78/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.2921 - mse: 166.2921 - val_loss: 158.0099 - val_mse: 158.0099\n",
            "Epoch 79/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 165.0619 - mse: 165.0619 - val_loss: 164.9189 - val_mse: 164.9189\n",
            "Epoch 80/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 166.7861 - mse: 166.7861 - val_loss: 183.3995 - val_mse: 183.3995\n",
            "Epoch 81/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 164.9248 - mse: 164.9248 - val_loss: 166.6511 - val_mse: 166.6511\n",
            "Epoch 82/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 165.7626 - mse: 165.7626 - val_loss: 164.2921 - val_mse: 164.2921\n",
            "Epoch 83/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 165.5684 - mse: 165.5684 - val_loss: 163.6323 - val_mse: 163.6323\n",
            "Epoch 84/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 165.7561 - mse: 165.7561 - val_loss: 173.2494 - val_mse: 173.2494\n",
            "Epoch 85/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.9952 - mse: 163.9952 - val_loss: 159.2864 - val_mse: 159.2864\n",
            "Epoch 86/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 165.5440 - mse: 165.5440 - val_loss: 188.9066 - val_mse: 188.9066\n",
            "Epoch 87/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 165.6232 - mse: 165.6232 - val_loss: 171.2404 - val_mse: 171.2404\n",
            "Epoch 88/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.8213 - mse: 163.8213 - val_loss: 191.4206 - val_mse: 191.4206\n",
            "Epoch 89/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 165.2397 - mse: 165.2397 - val_loss: 167.4264 - val_mse: 167.4264\n",
            "Epoch 90/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.2139 - mse: 163.2139 - val_loss: 156.9809 - val_mse: 156.9809\n",
            "Epoch 91/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.8521 - mse: 164.8521 - val_loss: 162.8135 - val_mse: 162.8135\n",
            "Epoch 92/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.6688 - mse: 163.6688 - val_loss: 159.9613 - val_mse: 159.9613\n",
            "Epoch 93/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.1758 - mse: 164.1758 - val_loss: 161.1815 - val_mse: 161.1815\n",
            "Epoch 94/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 165.8151 - mse: 165.8151 - val_loss: 169.3569 - val_mse: 169.3569\n",
            "Epoch 95/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.4396 - mse: 163.4396 - val_loss: 180.6121 - val_mse: 180.6121\n",
            "Epoch 96/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.1649 - mse: 164.1649 - val_loss: 156.3396 - val_mse: 156.3396\n",
            "Epoch 97/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.5347 - mse: 164.5347 - val_loss: 163.1911 - val_mse: 163.1911\n",
            "Epoch 98/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.3268 - mse: 164.3268 - val_loss: 189.3132 - val_mse: 189.3132\n",
            "Epoch 99/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 163.9749 - mse: 163.9749 - val_loss: 162.6837 - val_mse: 162.6837\n",
            "Epoch 100/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.1220 - mse: 164.1220 - val_loss: 175.6720 - val_mse: 175.6720\n",
            "Epoch 101/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 162.3685 - mse: 162.3685 - val_loss: 163.5758 - val_mse: 163.5758\n",
            "Epoch 102/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.5568 - mse: 163.5568 - val_loss: 193.3460 - val_mse: 193.3460\n",
            "Epoch 103/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.4755 - mse: 161.4755 - val_loss: 165.9126 - val_mse: 165.9126\n",
            "Epoch 104/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 164.0775 - mse: 164.0775 - val_loss: 164.3242 - val_mse: 164.3242\n",
            "Epoch 105/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 163.2733 - mse: 163.2733 - val_loss: 196.2298 - val_mse: 196.2298\n",
            "Epoch 106/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 162.4556 - mse: 162.4556 - val_loss: 193.2415 - val_mse: 193.2415\n",
            "Epoch 107/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 163.1395 - mse: 163.1395 - val_loss: 191.4930 - val_mse: 191.4930\n",
            "Epoch 108/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 162.9438 - mse: 162.9438 - val_loss: 176.5704 - val_mse: 176.5704\n",
            "Epoch 109/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 163.0603 - mse: 163.0603 - val_loss: 160.6250 - val_mse: 160.6250\n",
            "Epoch 110/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 162.4816 - mse: 162.4816 - val_loss: 161.9874 - val_mse: 161.9874\n",
            "Epoch 111/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 162.6611 - mse: 162.6611 - val_loss: 168.6669 - val_mse: 168.6669\n",
            "Epoch 112/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 161.6992 - mse: 161.6992 - val_loss: 155.4356 - val_mse: 155.4356\n",
            "Epoch 113/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.4545 - mse: 161.4545 - val_loss: 165.2581 - val_mse: 165.2581\n",
            "Epoch 114/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 162.4421 - mse: 162.4421 - val_loss: 155.6357 - val_mse: 155.6357\n",
            "Epoch 115/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 162.7786 - mse: 162.7786 - val_loss: 160.6570 - val_mse: 160.6570\n",
            "Epoch 116/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.6553 - mse: 161.6553 - val_loss: 160.4392 - val_mse: 160.4392\n",
            "Epoch 117/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 160.7719 - mse: 160.7719 - val_loss: 154.8143 - val_mse: 154.8143\n",
            "Epoch 118/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 161.1596 - mse: 161.1596 - val_loss: 165.2372 - val_mse: 165.2372\n",
            "Epoch 119/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.4756 - mse: 161.4756 - val_loss: 160.3861 - val_mse: 160.3861\n",
            "Epoch 120/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.8435 - mse: 159.8435 - val_loss: 172.5238 - val_mse: 172.5238\n",
            "Epoch 121/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.2713 - mse: 161.2713 - val_loss: 165.6097 - val_mse: 165.6097\n",
            "Epoch 122/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.4767 - mse: 161.4767 - val_loss: 217.3944 - val_mse: 217.3944\n",
            "Epoch 123/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 161.0872 - mse: 161.0872 - val_loss: 165.0112 - val_mse: 165.0112\n",
            "Epoch 124/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.9087 - mse: 159.9087 - val_loss: 226.0133 - val_mse: 226.0133\n",
            "Epoch 125/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.8481 - mse: 160.8481 - val_loss: 154.5587 - val_mse: 154.5587\n",
            "Epoch 126/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.8165 - mse: 160.8165 - val_loss: 188.7201 - val_mse: 188.7201\n",
            "Epoch 127/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.3615 - mse: 160.3615 - val_loss: 167.1085 - val_mse: 167.1085\n",
            "Epoch 128/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.0548 - mse: 160.0548 - val_loss: 161.1998 - val_mse: 161.1998\n",
            "Epoch 129/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.0582 - mse: 161.0582 - val_loss: 165.0694 - val_mse: 165.0694\n",
            "Epoch 130/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.3039 - mse: 160.3039 - val_loss: 175.2382 - val_mse: 175.2382\n",
            "Epoch 131/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.5020 - mse: 159.5020 - val_loss: 171.3514 - val_mse: 171.3514\n",
            "Epoch 132/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 160.2615 - mse: 160.2615 - val_loss: 157.3567 - val_mse: 157.3567\n",
            "Epoch 133/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.1223 - mse: 160.1223 - val_loss: 168.8857 - val_mse: 168.8857\n",
            "Epoch 134/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.6521 - mse: 159.6521 - val_loss: 174.5122 - val_mse: 174.5122\n",
            "Epoch 135/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 161.1165 - mse: 161.1165 - val_loss: 155.3732 - val_mse: 155.3732\n",
            "Epoch 136/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.4173 - mse: 159.4173 - val_loss: 154.5900 - val_mse: 154.5900\n",
            "Epoch 137/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.4766 - mse: 158.4766 - val_loss: 164.7736 - val_mse: 164.7736\n",
            "Epoch 138/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.7296 - mse: 159.7296 - val_loss: 175.0732 - val_mse: 175.0732\n",
            "Epoch 139/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 160.6208 - mse: 160.6208 - val_loss: 160.3469 - val_mse: 160.3469\n",
            "Epoch 140/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.1262 - mse: 159.1262 - val_loss: 159.6479 - val_mse: 159.6479\n",
            "Epoch 141/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.4323 - mse: 159.4323 - val_loss: 210.6995 - val_mse: 210.6995\n",
            "Epoch 142/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 159.8904 - mse: 159.8904 - val_loss: 156.4481 - val_mse: 156.4481\n",
            "Epoch 143/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 159.0977 - mse: 159.0977 - val_loss: 158.3999 - val_mse: 158.3999\n",
            "Epoch 144/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 160.1543 - mse: 160.1543 - val_loss: 156.9430 - val_mse: 156.9430\n",
            "Epoch 145/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 158.1534 - mse: 158.1534 - val_loss: 154.3844 - val_mse: 154.3844\n",
            "Epoch 146/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 159.2923 - mse: 159.2923 - val_loss: 160.7508 - val_mse: 160.7508\n",
            "Epoch 147/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 159.3239 - mse: 159.3239 - val_loss: 156.6077 - val_mse: 156.6077\n",
            "Epoch 148/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 159.2057 - mse: 159.2057 - val_loss: 169.4994 - val_mse: 169.4994\n",
            "Epoch 149/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.7761 - mse: 157.7761 - val_loss: 151.3126 - val_mse: 151.3126\n",
            "Epoch 150/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.3260 - mse: 158.3260 - val_loss: 164.2061 - val_mse: 164.2061\n",
            "Epoch 151/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.6379 - mse: 157.6379 - val_loss: 214.6956 - val_mse: 214.6956\n",
            "Epoch 152/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.6817 - mse: 158.6817 - val_loss: 165.9924 - val_mse: 165.9924\n",
            "Epoch 153/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.9758 - mse: 157.9758 - val_loss: 172.8801 - val_mse: 172.8801\n",
            "Epoch 154/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.5912 - mse: 157.5912 - val_loss: 160.9213 - val_mse: 160.9213\n",
            "Epoch 155/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.0245 - mse: 158.0245 - val_loss: 172.6757 - val_mse: 172.6757\n",
            "Epoch 156/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.0323 - mse: 158.0323 - val_loss: 154.7906 - val_mse: 154.7906\n",
            "Epoch 157/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.3653 - mse: 158.3653 - val_loss: 153.6787 - val_mse: 153.6787\n",
            "Epoch 158/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.2232 - mse: 158.2232 - val_loss: 152.8332 - val_mse: 152.8332\n",
            "Epoch 159/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.4033 - mse: 158.4033 - val_loss: 149.9692 - val_mse: 149.9692\n",
            "Epoch 160/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 158.4232 - mse: 158.4232 - val_loss: 173.7863 - val_mse: 173.7863\n",
            "Epoch 161/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 158.0477 - mse: 158.0477 - val_loss: 151.5700 - val_mse: 151.5700\n",
            "Epoch 162/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.2334 - mse: 157.2334 - val_loss: 164.8271 - val_mse: 164.8271\n",
            "Epoch 163/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.9257 - mse: 157.9257 - val_loss: 173.6154 - val_mse: 173.6154\n",
            "Epoch 164/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.6999 - mse: 157.6999 - val_loss: 161.5029 - val_mse: 161.5029\n",
            "Epoch 165/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.6387 - mse: 157.6387 - val_loss: 154.5512 - val_mse: 154.5512\n",
            "Epoch 166/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.3719 - mse: 157.3719 - val_loss: 158.7888 - val_mse: 158.7888\n",
            "Epoch 167/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.2567 - mse: 156.2567 - val_loss: 161.2708 - val_mse: 161.2708\n",
            "Epoch 168/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.3141 - mse: 157.3141 - val_loss: 168.2185 - val_mse: 168.2185\n",
            "Epoch 169/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.9657 - mse: 157.9657 - val_loss: 165.3033 - val_mse: 165.3033\n",
            "Epoch 170/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.4345 - mse: 157.4345 - val_loss: 150.9440 - val_mse: 150.9440\n",
            "Epoch 171/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.8539 - mse: 157.8539 - val_loss: 157.3146 - val_mse: 157.3146\n",
            "Epoch 172/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.1990 - mse: 157.1990 - val_loss: 152.1152 - val_mse: 152.1152\n",
            "Epoch 173/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 156.0243 - mse: 156.0243 - val_loss: 155.6084 - val_mse: 155.6084\n",
            "Epoch 174/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.7687 - mse: 157.7687 - val_loss: 179.3962 - val_mse: 179.3962\n",
            "Epoch 175/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.2072 - mse: 157.2072 - val_loss: 160.7186 - val_mse: 160.7186\n",
            "Epoch 176/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.3553 - mse: 156.3553 - val_loss: 198.3578 - val_mse: 198.3578\n",
            "Epoch 177/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 157.5222 - mse: 157.5222 - val_loss: 156.7596 - val_mse: 156.7596\n",
            "Epoch 178/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 156.5284 - mse: 156.5284 - val_loss: 154.5731 - val_mse: 154.5731\n",
            "Epoch 179/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 155.8280 - mse: 155.8280 - val_loss: 156.4484 - val_mse: 156.4484\n",
            "Epoch 180/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 157.5142 - mse: 157.5142 - val_loss: 162.2414 - val_mse: 162.2414\n",
            "Epoch 181/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 156.7152 - mse: 156.7152 - val_loss: 156.7609 - val_mse: 156.7609\n",
            "Epoch 182/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 157.0608 - mse: 157.0608 - val_loss: 178.8928 - val_mse: 178.8928\n",
            "Epoch 183/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.2757 - mse: 157.2757 - val_loss: 153.9835 - val_mse: 153.9835\n",
            "Epoch 184/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 157.0163 - mse: 157.0163 - val_loss: 165.5761 - val_mse: 165.5761\n",
            "Epoch 185/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.2874 - mse: 156.2874 - val_loss: 159.1457 - val_mse: 159.1457\n",
            "Epoch 186/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 156.2046 - mse: 156.2046 - val_loss: 155.2891 - val_mse: 155.2891\n",
            "Epoch 187/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.0603 - mse: 156.0603 - val_loss: 154.8554 - val_mse: 154.8554\n",
            "Epoch 188/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.2361 - mse: 156.2361 - val_loss: 157.3860 - val_mse: 157.3860\n",
            "Epoch 189/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.4296 - mse: 155.4296 - val_loss: 154.5067 - val_mse: 154.5067\n",
            "Epoch 190/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.9723 - mse: 155.9723 - val_loss: 181.2808 - val_mse: 181.2808\n",
            "Epoch 191/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.9161 - mse: 155.9161 - val_loss: 162.1039 - val_mse: 162.1039\n",
            "Epoch 192/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.1845 - mse: 156.1845 - val_loss: 156.5886 - val_mse: 156.5886\n",
            "Epoch 193/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.2989 - mse: 156.2989 - val_loss: 162.2246 - val_mse: 162.2246\n",
            "Epoch 194/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.7964 - mse: 155.7964 - val_loss: 165.3435 - val_mse: 165.3435\n",
            "Epoch 195/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.8727 - mse: 155.8727 - val_loss: 155.9897 - val_mse: 155.9897\n",
            "Epoch 196/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 156.0856 - mse: 156.0856 - val_loss: 152.8534 - val_mse: 152.8534\n",
            "Epoch 197/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6174 - mse: 154.6174 - val_loss: 159.7347 - val_mse: 159.7347\n",
            "Epoch 198/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.7927 - mse: 155.7927 - val_loss: 154.5175 - val_mse: 154.5175\n",
            "Epoch 199/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.7092 - mse: 155.7092 - val_loss: 154.7801 - val_mse: 154.7801\n",
            "Epoch 200/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.9633 - mse: 154.9633 - val_loss: 152.3197 - val_mse: 152.3197\n",
            "Epoch 201/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 156.3493 - mse: 156.3493 - val_loss: 153.7704 - val_mse: 153.7704\n",
            "Epoch 202/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.5155 - mse: 155.5155 - val_loss: 166.6389 - val_mse: 166.6389\n",
            "Epoch 203/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.3566 - mse: 155.3566 - val_loss: 154.4012 - val_mse: 154.4012\n",
            "Epoch 204/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.9325 - mse: 154.9325 - val_loss: 186.4624 - val_mse: 186.4624\n",
            "Epoch 205/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.4905 - mse: 155.4905 - val_loss: 150.3324 - val_mse: 150.3324\n",
            "Epoch 206/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.3872 - mse: 154.3872 - val_loss: 184.6036 - val_mse: 184.6036\n",
            "Epoch 207/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.0494 - mse: 155.0494 - val_loss: 159.2945 - val_mse: 159.2945\n",
            "Epoch 208/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.7278 - mse: 154.7278 - val_loss: 166.8436 - val_mse: 166.8436\n",
            "Epoch 209/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.8896 - mse: 154.8896 - val_loss: 163.7113 - val_mse: 163.7113\n",
            "Epoch 210/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.5366 - mse: 154.5366 - val_loss: 177.1704 - val_mse: 177.1704\n",
            "Epoch 211/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.9879 - mse: 154.9879 - val_loss: 166.8227 - val_mse: 166.8227\n",
            "Epoch 212/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 154.2292 - mse: 154.2292 - val_loss: 153.9038 - val_mse: 153.9038\n",
            "Epoch 213/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 155.8385 - mse: 155.8385 - val_loss: 154.1133 - val_mse: 154.1133\n",
            "Epoch 214/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 155.1942 - mse: 155.1942 - val_loss: 153.8073 - val_mse: 153.8073\n",
            "Epoch 215/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 154.3232 - mse: 154.3232 - val_loss: 157.1710 - val_mse: 157.1710\n",
            "Epoch 216/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 154.5957 - mse: 154.5957 - val_loss: 151.2861 - val_mse: 151.2861\n",
            "Epoch 217/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.8676 - mse: 154.8676 - val_loss: 154.0646 - val_mse: 154.0646\n",
            "Epoch 218/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.7496 - mse: 154.7496 - val_loss: 156.7437 - val_mse: 156.7437\n",
            "Epoch 219/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.9559 - mse: 154.9559 - val_loss: 186.7499 - val_mse: 186.7499\n",
            "Epoch 220/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.1066 - mse: 154.1066 - val_loss: 153.2551 - val_mse: 153.2551\n",
            "Epoch 221/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.5900 - mse: 154.5900 - val_loss: 152.3884 - val_mse: 152.3884\n",
            "Epoch 222/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.2165 - mse: 155.2165 - val_loss: 150.4807 - val_mse: 150.4807\n",
            "Epoch 223/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.9387 - mse: 153.9387 - val_loss: 152.9359 - val_mse: 152.9359\n",
            "Epoch 224/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 155.0935 - mse: 155.0935 - val_loss: 159.2806 - val_mse: 159.2806\n",
            "Epoch 225/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.9725 - mse: 154.9725 - val_loss: 157.3849 - val_mse: 157.3849\n",
            "Epoch 226/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.5834 - mse: 154.5834 - val_loss: 167.1778 - val_mse: 167.1778\n",
            "Epoch 227/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.0892 - mse: 154.0892 - val_loss: 165.8518 - val_mse: 165.8518\n",
            "Epoch 228/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.6659 - mse: 154.6659 - val_loss: 158.6759 - val_mse: 158.6759\n",
            "Epoch 229/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6435 - mse: 154.6435 - val_loss: 163.1409 - val_mse: 163.1409\n",
            "Epoch 230/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.1266 - mse: 154.1266 - val_loss: 165.0503 - val_mse: 165.0503\n",
            "Epoch 231/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.9416 - mse: 154.9416 - val_loss: 173.6872 - val_mse: 173.6872\n",
            "Epoch 232/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.3704 - mse: 154.3704 - val_loss: 152.6818 - val_mse: 152.6818\n",
            "Epoch 233/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.2281 - mse: 154.2281 - val_loss: 162.2296 - val_mse: 162.2296\n",
            "Epoch 234/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.8160 - mse: 154.8160 - val_loss: 148.5003 - val_mse: 148.5003\n",
            "Epoch 235/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.7900 - mse: 153.7900 - val_loss: 151.2334 - val_mse: 151.2334\n",
            "Epoch 236/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.4628 - mse: 154.4628 - val_loss: 163.0745 - val_mse: 163.0745\n",
            "Epoch 237/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.2112 - mse: 154.2112 - val_loss: 165.9660 - val_mse: 165.9660\n",
            "Epoch 238/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.8774 - mse: 153.8774 - val_loss: 188.6403 - val_mse: 188.6403\n",
            "Epoch 239/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6576 - mse: 154.6576 - val_loss: 163.6093 - val_mse: 163.6093\n",
            "Epoch 240/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 155.0101 - mse: 155.0101 - val_loss: 148.7939 - val_mse: 148.7939\n",
            "Epoch 241/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.7707 - mse: 153.7707 - val_loss: 171.7328 - val_mse: 171.7328\n",
            "Epoch 242/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.5026 - mse: 154.5026 - val_loss: 152.7854 - val_mse: 152.7854\n",
            "Epoch 243/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.1550 - mse: 154.1550 - val_loss: 153.6023 - val_mse: 153.6023\n",
            "Epoch 244/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.1087 - mse: 154.1087 - val_loss: 155.8058 - val_mse: 155.8058\n",
            "Epoch 245/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.2459 - mse: 154.2459 - val_loss: 151.3426 - val_mse: 151.3426\n",
            "Epoch 246/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 154.3932 - mse: 154.3932 - val_loss: 170.5596 - val_mse: 170.5596\n",
            "Epoch 247/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.8595 - mse: 153.8595 - val_loss: 156.2944 - val_mse: 156.2944\n",
            "Epoch 248/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.7767 - mse: 153.7767 - val_loss: 160.9691 - val_mse: 160.9691\n",
            "Epoch 249/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.9878 - mse: 153.9878 - val_loss: 151.1340 - val_mse: 151.1340\n",
            "Epoch 250/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.7144 - mse: 153.7144 - val_loss: 157.4149 - val_mse: 157.4149\n",
            "Epoch 251/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.8513 - mse: 153.8513 - val_loss: 150.6396 - val_mse: 150.6396\n",
            "Epoch 252/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.9811 - mse: 153.9811 - val_loss: 151.2148 - val_mse: 151.2148\n",
            "Epoch 253/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.0059 - mse: 154.0059 - val_loss: 153.1428 - val_mse: 153.1428\n",
            "Epoch 254/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.2222 - mse: 154.2222 - val_loss: 152.9144 - val_mse: 152.9144\n",
            "Epoch 255/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.3590 - mse: 153.3590 - val_loss: 169.3146 - val_mse: 169.3146\n",
            "Epoch 256/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.9807 - mse: 153.9807 - val_loss: 153.4673 - val_mse: 153.4673\n",
            "Epoch 257/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.6608 - mse: 153.6608 - val_loss: 178.8627 - val_mse: 178.8627\n",
            "Epoch 258/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.1914 - mse: 153.1914 - val_loss: 155.5952 - val_mse: 155.5952\n",
            "Epoch 259/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.5738 - mse: 153.5738 - val_loss: 149.5618 - val_mse: 149.5618\n",
            "Epoch 260/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.5332 - mse: 153.5332 - val_loss: 167.3131 - val_mse: 167.3131\n",
            "Epoch 261/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.6200 - mse: 154.6200 - val_loss: 152.3251 - val_mse: 152.3251\n",
            "Epoch 262/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.0950 - mse: 152.0950 - val_loss: 156.5960 - val_mse: 156.5960\n",
            "Epoch 263/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 154.0205 - mse: 154.0205 - val_loss: 177.3866 - val_mse: 177.3866\n",
            "Epoch 264/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.3058 - mse: 153.3058 - val_loss: 151.9271 - val_mse: 151.9271\n",
            "Epoch 265/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.6317 - mse: 153.6317 - val_loss: 150.2902 - val_mse: 150.2902\n",
            "Epoch 266/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.7092 - mse: 153.7092 - val_loss: 154.0869 - val_mse: 154.0869\n",
            "Epoch 267/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.9906 - mse: 152.9906 - val_loss: 149.5443 - val_mse: 149.5443\n",
            "Epoch 268/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.3530 - mse: 154.3530 - val_loss: 154.2804 - val_mse: 154.2804\n",
            "Epoch 269/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.5326 - mse: 153.5326 - val_loss: 149.9623 - val_mse: 149.9623\n",
            "Epoch 270/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.6606 - mse: 153.6606 - val_loss: 152.4017 - val_mse: 152.4017\n",
            "Epoch 271/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 154.0983 - mse: 154.0983 - val_loss: 154.3577 - val_mse: 154.3577\n",
            "Epoch 272/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.4386 - mse: 153.4386 - val_loss: 171.6531 - val_mse: 171.6531\n",
            "Epoch 273/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.8789 - mse: 153.8789 - val_loss: 168.9725 - val_mse: 168.9725\n",
            "Epoch 274/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.9571 - mse: 153.9571 - val_loss: 171.1171 - val_mse: 171.1171\n",
            "Epoch 275/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.2919 - mse: 153.2919 - val_loss: 151.6812 - val_mse: 151.6812\n",
            "Epoch 276/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.9274 - mse: 152.9274 - val_loss: 167.0525 - val_mse: 167.0525\n",
            "Epoch 277/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.7626 - mse: 153.7626 - val_loss: 158.1356 - val_mse: 158.1356\n",
            "Epoch 278/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.0825 - mse: 153.0825 - val_loss: 158.0508 - val_mse: 158.0508\n",
            "Epoch 279/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.5514 - mse: 153.5514 - val_loss: 152.7358 - val_mse: 152.7358\n",
            "Epoch 280/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.2751 - mse: 153.2751 - val_loss: 147.8294 - val_mse: 147.8294\n",
            "Epoch 281/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.2472 - mse: 153.2472 - val_loss: 153.3357 - val_mse: 153.3357\n",
            "Epoch 282/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.3891 - mse: 153.3891 - val_loss: 149.9643 - val_mse: 149.9643\n",
            "Epoch 283/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.7173 - mse: 153.7173 - val_loss: 155.8069 - val_mse: 155.8069\n",
            "Epoch 284/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.0219 - mse: 153.0219 - val_loss: 152.2699 - val_mse: 152.2699\n",
            "Epoch 285/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.5303 - mse: 153.5303 - val_loss: 156.0647 - val_mse: 156.0647\n",
            "Epoch 286/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.5724 - mse: 153.5724 - val_loss: 149.6859 - val_mse: 149.6859\n",
            "Epoch 287/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.7397 - mse: 152.7397 - val_loss: 160.8125 - val_mse: 160.8125\n",
            "Epoch 288/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.4744 - mse: 153.4744 - val_loss: 161.1144 - val_mse: 161.1144\n",
            "Epoch 289/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.0112 - mse: 153.0112 - val_loss: 149.9660 - val_mse: 149.9660\n",
            "Epoch 290/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.1586 - mse: 153.1586 - val_loss: 150.4085 - val_mse: 150.4085\n",
            "Epoch 291/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.3989 - mse: 153.3989 - val_loss: 151.6486 - val_mse: 151.6486\n",
            "Epoch 292/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.1574 - mse: 152.1574 - val_loss: 184.7790 - val_mse: 184.7790\n",
            "Epoch 293/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.7085 - mse: 153.7085 - val_loss: 165.2794 - val_mse: 165.2794\n",
            "Epoch 294/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.5497 - mse: 153.5497 - val_loss: 183.3805 - val_mse: 183.3805\n",
            "Epoch 295/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.4203 - mse: 153.4203 - val_loss: 167.3083 - val_mse: 167.3083\n",
            "Epoch 296/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.3570 - mse: 153.3570 - val_loss: 151.2230 - val_mse: 151.2230\n",
            "Epoch 297/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.5873 - mse: 153.5873 - val_loss: 192.5719 - val_mse: 192.5719\n",
            "Epoch 298/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.1033 - mse: 153.1033 - val_loss: 162.1740 - val_mse: 162.1740\n",
            "Epoch 299/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.4264 - mse: 152.4264 - val_loss: 149.6283 - val_mse: 149.6283\n",
            "Epoch 300/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.4043 - mse: 153.4043 - val_loss: 156.4533 - val_mse: 156.4533\n",
            "Epoch 301/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.3015 - mse: 153.3015 - val_loss: 148.1495 - val_mse: 148.1495\n",
            "Epoch 302/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.1035 - mse: 153.1035 - val_loss: 153.0514 - val_mse: 153.0514\n",
            "Epoch 303/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.4030 - mse: 152.4030 - val_loss: 156.6124 - val_mse: 156.6124\n",
            "Epoch 304/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.3717 - mse: 153.3717 - val_loss: 156.4407 - val_mse: 156.4407\n",
            "Epoch 305/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.3416 - mse: 153.3416 - val_loss: 154.6070 - val_mse: 154.6070\n",
            "Epoch 306/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.2830 - mse: 152.2830 - val_loss: 159.5540 - val_mse: 159.5540\n",
            "Epoch 307/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.7263 - mse: 152.7263 - val_loss: 150.3951 - val_mse: 150.3951\n",
            "Epoch 308/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.0881 - mse: 153.0881 - val_loss: 184.0423 - val_mse: 184.0423\n",
            "Epoch 309/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.8209 - mse: 153.8209 - val_loss: 161.8669 - val_mse: 161.8669\n",
            "Epoch 310/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.1331 - mse: 153.1331 - val_loss: 155.3178 - val_mse: 155.3178\n",
            "Epoch 311/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.9415 - mse: 152.9415 - val_loss: 159.6061 - val_mse: 159.6061\n",
            "Epoch 312/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.3351 - mse: 152.3351 - val_loss: 151.2055 - val_mse: 151.2055\n",
            "Epoch 313/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.1048 - mse: 152.1048 - val_loss: 149.2891 - val_mse: 149.2891\n",
            "Epoch 314/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.7311 - mse: 152.7311 - val_loss: 180.6864 - val_mse: 180.6864\n",
            "Epoch 315/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.1964 - mse: 153.1964 - val_loss: 150.6136 - val_mse: 150.6136\n",
            "Epoch 316/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 152.9213 - mse: 152.9213 - val_loss: 154.2861 - val_mse: 154.2861\n",
            "Epoch 317/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 153.2160 - mse: 153.2160 - val_loss: 163.2996 - val_mse: 163.2996\n",
            "Epoch 318/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 153.2100 - mse: 153.2100 - val_loss: 172.3998 - val_mse: 172.3998\n",
            "Epoch 319/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 152.9619 - mse: 152.9619 - val_loss: 158.8204 - val_mse: 158.8204\n",
            "Epoch 320/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.8001 - mse: 152.8001 - val_loss: 157.2146 - val_mse: 157.2146\n",
            "Epoch 321/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.1496 - mse: 152.1496 - val_loss: 151.5042 - val_mse: 151.5042\n",
            "Epoch 322/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.9321 - mse: 152.9321 - val_loss: 163.0241 - val_mse: 163.0241\n",
            "Epoch 323/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.1104 - mse: 152.1104 - val_loss: 155.8395 - val_mse: 155.8395\n",
            "Epoch 324/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.8532 - mse: 152.8532 - val_loss: 154.1236 - val_mse: 154.1236\n",
            "Epoch 325/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.1228 - mse: 153.1228 - val_loss: 150.7104 - val_mse: 150.7104\n",
            "Epoch 326/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.5654 - mse: 152.5654 - val_loss: 152.4682 - val_mse: 152.4682\n",
            "Epoch 327/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.3163 - mse: 152.3163 - val_loss: 156.7401 - val_mse: 156.7401\n",
            "Epoch 328/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.5698 - mse: 152.5698 - val_loss: 167.6491 - val_mse: 167.6491\n",
            "Epoch 329/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.8925 - mse: 152.8925 - val_loss: 153.4646 - val_mse: 153.4646\n",
            "Epoch 330/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.9041 - mse: 152.9041 - val_loss: 149.0328 - val_mse: 149.0328\n",
            "Epoch 331/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.2067 - mse: 152.2067 - val_loss: 147.8448 - val_mse: 147.8448\n",
            "Epoch 332/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.3527 - mse: 152.3527 - val_loss: 156.7016 - val_mse: 156.7016\n",
            "Epoch 333/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.7489 - mse: 152.7489 - val_loss: 148.1145 - val_mse: 148.1145\n",
            "Epoch 334/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 151.8637 - mse: 151.8637 - val_loss: 171.0244 - val_mse: 171.0244\n",
            "Epoch 335/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.7372 - mse: 152.7372 - val_loss: 155.9950 - val_mse: 155.9950\n",
            "Epoch 336/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 151.9708 - mse: 151.9708 - val_loss: 166.5599 - val_mse: 166.5599\n",
            "Epoch 337/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.1239 - mse: 153.1239 - val_loss: 158.2038 - val_mse: 158.2038\n",
            "Epoch 338/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.5891 - mse: 151.5891 - val_loss: 159.9551 - val_mse: 159.9551\n",
            "Epoch 339/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.1008 - mse: 153.1008 - val_loss: 149.7074 - val_mse: 149.7074\n",
            "Epoch 340/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 151.9859 - mse: 151.9859 - val_loss: 151.3908 - val_mse: 151.3908\n",
            "Epoch 341/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.7979 - mse: 152.7979 - val_loss: 148.6295 - val_mse: 148.6295\n",
            "Epoch 342/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 153.0372 - mse: 153.0372 - val_loss: 156.8280 - val_mse: 156.8280\n",
            "Epoch 343/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.2021 - mse: 152.2021 - val_loss: 159.1086 - val_mse: 159.1086\n",
            "Epoch 344/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.3952 - mse: 152.3952 - val_loss: 167.0358 - val_mse: 167.0358\n",
            "Epoch 345/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 152.8063 - mse: 152.8063 - val_loss: 151.9748 - val_mse: 151.9748\n",
            "Epoch 346/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.8943 - mse: 152.8943 - val_loss: 163.0872 - val_mse: 163.0872\n",
            "Epoch 347/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 152.2863 - mse: 152.2863 - val_loss: 158.9054 - val_mse: 158.9054\n",
            "Epoch 348/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 151.5196 - mse: 151.5196 - val_loss: 162.0887 - val_mse: 162.0887\n",
            "Epoch 349/350\n",
            "75/75 [==============================] - 0s 7ms/step - loss: 152.4669 - mse: 152.4669 - val_loss: 151.9427 - val_mse: 151.9427\n",
            "Epoch 350/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 152.0160 - mse: 152.0160 - val_loss: 149.5420 - val_mse: 149.5420\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model3.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-8DriDTOMZn",
        "outputId": "0304a920-154c-49c6-a0f0-227e926a5ee3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 0s 3ms/step - loss: 151.1354 - mse: 151.1354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model3.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_predict, y_test)\n",
        "mae = mean_absolute_error(y_predict, y_test)\n",
        "y2 = r2_score(y_predict,y_test)\n",
        "print(\"MSE: \" , mse)\n",
        "print(\"MAE: \" , mae)\n",
        "print(\"R2: \" , y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4kfOMBjR5Jw",
        "outputId": "04b47f51-055b-4437-a9f6-9ecdbc7fde90"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 0s 2ms/step\n",
            "MSE:  151.13537395700263\n",
            "MAE:  8.813048176853801\n",
            "R2:  -3.534102567928214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_arr = np.array([[6,0,1,1,4]]) #El primer ejemplo del dataset original\n",
        "predicciones = model3.predict(test_arr)\n",
        "print(predicciones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnRzVhm9R-la",
        "outputId": "18b8eba3-dfa2-499b-ecce-8fdc05321427"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 41ms/step\n",
            "[[72.757484 63.89083  48.363228 52.892033 57.486088 71.88479  77.33642\n",
            "  77.53402  79.310646 74.05368  81.138756 86.474335 78.94323  81.12917\n",
            "  78.785095 82.13963  85.37621  79.81961  80.634476 77.44662  72.032455\n",
            "  68.5659   66.514366 62.954117 68.63799  72.65547  63.541992 49.28145\n",
            "  63.130444 56.79541  75.79409  83.75952  84.67656  79.04825  65.497185\n",
            "  52.1123   80.27318  85.76367  84.63028  87.4665   85.58552  83.28594\n",
            "  85.943344 82.666466]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# El modelo 4 usara los mismos datos anteriores pero se le añadira una capa oculta con igual cantidad de nodos que el original"
      ],
      "metadata": {
        "id": "6F6yZolzpci8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = Sequential()\n",
        "\n",
        "model4.add(Dense(6, input_shape=(X.shape[-1],) , activation='relu'))\n",
        "model4.add(Dropout(0.002))\n",
        "\n",
        "model4.add(Dense(12, activation=\"relu\"))\n",
        "model4.add(Dropout(0.002))\n",
        "\n",
        "model4.add(Dense(24, activation=\"relu\"))\n",
        "model4.add(Dropout(0.002))\n",
        "\n",
        "model4.add(Dense(48, activation=\"relu\"))\n",
        "model4.add(Dropout(0.002))\n",
        "\n",
        "model4.add(Dense(96, activation=\"relu\"))\n",
        "model4.add(Dropout(0.002))\n",
        "\n",
        "model4.add(Dense(44)) #Capa de salida \n",
        "\n",
        "model4.summary()\n",
        "\n",
        "model4.compile(loss=\"mean_squared_error\", optimizer=RMSprop(0.007), metrics=[\"mse\"] )\n",
        "\n",
        "H4 = model4.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=350, batch_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IICJaAlpb8U",
        "outputId": "4aff85c9-fba7-4ab8-ba8b-2f4c77d3333e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 6)                 36        \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 6)                 0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 12)                84        \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 12)                0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 24)                312       \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 24)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 48)                1200      \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 48)                0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 96)                4704      \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 96)                0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 44)                4268      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,604\n",
            "Trainable params: 10,604\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/350\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 845.6277 - mse: 845.6277 - val_loss: 257.8748 - val_mse: 257.8748\n",
            "Epoch 2/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 250.9679 - mse: 250.9679 - val_loss: 210.4578 - val_mse: 210.4578\n",
            "Epoch 3/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 238.3602 - mse: 238.3602 - val_loss: 209.1095 - val_mse: 209.1095\n",
            "Epoch 4/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 237.5489 - mse: 237.5489 - val_loss: 213.6622 - val_mse: 213.6622\n",
            "Epoch 5/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 235.8898 - mse: 235.8898 - val_loss: 208.6545 - val_mse: 208.6545\n",
            "Epoch 6/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 233.9319 - mse: 233.9319 - val_loss: 234.8630 - val_mse: 234.8630\n",
            "Epoch 7/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 229.8592 - mse: 229.8592 - val_loss: 209.9814 - val_mse: 209.9814\n",
            "Epoch 8/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 232.0454 - mse: 232.0454 - val_loss: 208.9537 - val_mse: 208.9537\n",
            "Epoch 9/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 228.1729 - mse: 228.1729 - val_loss: 227.1838 - val_mse: 227.1838\n",
            "Epoch 10/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 228.2713 - mse: 228.2713 - val_loss: 213.8408 - val_mse: 213.8408\n",
            "Epoch 11/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 227.8838 - mse: 227.8838 - val_loss: 208.2929 - val_mse: 208.2929\n",
            "Epoch 12/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 224.9067 - mse: 224.9067 - val_loss: 231.9751 - val_mse: 231.9751\n",
            "Epoch 13/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 223.0953 - mse: 223.0953 - val_loss: 350.6770 - val_mse: 350.6770\n",
            "Epoch 14/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 226.3374 - mse: 226.3374 - val_loss: 281.3934 - val_mse: 281.3934\n",
            "Epoch 15/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 225.3960 - mse: 225.3960 - val_loss: 281.7948 - val_mse: 281.7948\n",
            "Epoch 16/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 223.6126 - mse: 223.6126 - val_loss: 210.4370 - val_mse: 210.4370\n",
            "Epoch 17/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 222.1808 - mse: 222.1808 - val_loss: 244.1058 - val_mse: 244.1058\n",
            "Epoch 18/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 220.4783 - mse: 220.4783 - val_loss: 261.9095 - val_mse: 261.9095\n",
            "Epoch 19/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 220.8640 - mse: 220.8640 - val_loss: 214.8481 - val_mse: 214.8481\n",
            "Epoch 20/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 221.3774 - mse: 221.3774 - val_loss: 220.2113 - val_mse: 220.2113\n",
            "Epoch 21/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 218.7245 - mse: 218.7245 - val_loss: 228.2637 - val_mse: 228.2637\n",
            "Epoch 22/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 217.7882 - mse: 217.7882 - val_loss: 224.1940 - val_mse: 224.1940\n",
            "Epoch 23/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 218.5629 - mse: 218.5629 - val_loss: 208.5104 - val_mse: 208.5104\n",
            "Epoch 24/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 217.4683 - mse: 217.4683 - val_loss: 259.4175 - val_mse: 259.4175\n",
            "Epoch 25/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 219.0340 - mse: 219.0340 - val_loss: 211.7416 - val_mse: 211.7416\n",
            "Epoch 26/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 215.6627 - mse: 215.6627 - val_loss: 219.3071 - val_mse: 219.3071\n",
            "Epoch 27/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 217.3837 - mse: 217.3837 - val_loss: 217.2277 - val_mse: 217.2277\n",
            "Epoch 28/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 217.3288 - mse: 217.3288 - val_loss: 212.0742 - val_mse: 212.0742\n",
            "Epoch 29/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 216.8486 - mse: 216.8486 - val_loss: 246.3674 - val_mse: 246.3674\n",
            "Epoch 30/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 215.3812 - mse: 215.3812 - val_loss: 235.8941 - val_mse: 235.8941\n",
            "Epoch 31/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 215.4056 - mse: 215.4056 - val_loss: 207.5387 - val_mse: 207.5387\n",
            "Epoch 32/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 214.7178 - mse: 214.7178 - val_loss: 207.6942 - val_mse: 207.6942\n",
            "Epoch 33/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 214.2965 - mse: 214.2965 - val_loss: 224.3744 - val_mse: 224.3744\n",
            "Epoch 34/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 213.3939 - mse: 213.3939 - val_loss: 274.7912 - val_mse: 274.7912\n",
            "Epoch 35/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 213.7734 - mse: 213.7734 - val_loss: 206.0956 - val_mse: 206.0956\n",
            "Epoch 36/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 212.7413 - mse: 212.7413 - val_loss: 207.7241 - val_mse: 207.7241\n",
            "Epoch 37/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 213.7565 - mse: 213.7565 - val_loss: 212.5836 - val_mse: 212.5836\n",
            "Epoch 38/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 213.1268 - mse: 213.1268 - val_loss: 207.3712 - val_mse: 207.3712\n",
            "Epoch 39/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 213.6808 - mse: 213.6808 - val_loss: 231.6323 - val_mse: 231.6323\n",
            "Epoch 40/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.9896 - mse: 209.9896 - val_loss: 211.9255 - val_mse: 211.9255\n",
            "Epoch 41/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 212.1850 - mse: 212.1850 - val_loss: 208.3903 - val_mse: 208.3903\n",
            "Epoch 42/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 212.1389 - mse: 212.1389 - val_loss: 235.3182 - val_mse: 235.3182\n",
            "Epoch 43/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 210.9416 - mse: 210.9416 - val_loss: 245.1627 - val_mse: 245.1627\n",
            "Epoch 44/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 211.0483 - mse: 211.0483 - val_loss: 205.9552 - val_mse: 205.9552\n",
            "Epoch 45/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.7965 - mse: 210.7965 - val_loss: 227.2365 - val_mse: 227.2365\n",
            "Epoch 46/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 211.4998 - mse: 211.4998 - val_loss: 246.2339 - val_mse: 246.2339\n",
            "Epoch 47/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.7896 - mse: 210.7896 - val_loss: 207.4126 - val_mse: 207.4126\n",
            "Epoch 48/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 210.3929 - mse: 210.3929 - val_loss: 225.6117 - val_mse: 225.6117\n",
            "Epoch 49/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 211.0679 - mse: 211.0679 - val_loss: 218.7088 - val_mse: 218.7088\n",
            "Epoch 50/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.2333 - mse: 210.2333 - val_loss: 207.9920 - val_mse: 207.9920\n",
            "Epoch 51/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 211.3183 - mse: 211.3183 - val_loss: 226.0641 - val_mse: 226.0641\n",
            "Epoch 52/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.2868 - mse: 210.2868 - val_loss: 212.6741 - val_mse: 212.6741\n",
            "Epoch 53/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 210.8640 - mse: 210.8640 - val_loss: 209.7898 - val_mse: 209.7898\n",
            "Epoch 54/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 211.2486 - mse: 211.2486 - val_loss: 207.0338 - val_mse: 207.0338\n",
            "Epoch 55/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.1627 - mse: 210.1627 - val_loss: 244.2814 - val_mse: 244.2814\n",
            "Epoch 56/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.0049 - mse: 210.0049 - val_loss: 228.3102 - val_mse: 228.3102\n",
            "Epoch 57/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 210.1743 - mse: 210.1743 - val_loss: 210.7613 - val_mse: 210.7613\n",
            "Epoch 58/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 210.4167 - mse: 210.4167 - val_loss: 206.7563 - val_mse: 206.7563\n",
            "Epoch 59/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.0431 - mse: 209.0431 - val_loss: 217.2411 - val_mse: 217.2411\n",
            "Epoch 60/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.5710 - mse: 209.5710 - val_loss: 207.2249 - val_mse: 207.2249\n",
            "Epoch 61/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 209.1256 - mse: 209.1256 - val_loss: 229.9008 - val_mse: 229.9008\n",
            "Epoch 62/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 210.3691 - mse: 210.3691 - val_loss: 219.8959 - val_mse: 219.8959\n",
            "Epoch 63/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 209.1320 - mse: 209.1320 - val_loss: 216.8057 - val_mse: 216.8057\n",
            "Epoch 64/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.6509 - mse: 208.6509 - val_loss: 207.6328 - val_mse: 207.6328\n",
            "Epoch 65/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.8901 - mse: 208.8901 - val_loss: 207.6947 - val_mse: 207.6947\n",
            "Epoch 66/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.0379 - mse: 209.0379 - val_loss: 220.7379 - val_mse: 220.7379\n",
            "Epoch 67/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.4323 - mse: 208.4323 - val_loss: 232.3704 - val_mse: 232.3704\n",
            "Epoch 68/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.5801 - mse: 208.5801 - val_loss: 218.3900 - val_mse: 218.3900\n",
            "Epoch 69/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.1552 - mse: 209.1552 - val_loss: 205.8725 - val_mse: 205.8725\n",
            "Epoch 70/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.6845 - mse: 208.6845 - val_loss: 206.7461 - val_mse: 206.7461\n",
            "Epoch 71/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.3963 - mse: 208.3963 - val_loss: 210.7594 - val_mse: 210.7594\n",
            "Epoch 72/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.7797 - mse: 208.7797 - val_loss: 207.0072 - val_mse: 207.0072\n",
            "Epoch 73/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.1857 - mse: 209.1857 - val_loss: 220.1805 - val_mse: 220.1805\n",
            "Epoch 74/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.0412 - mse: 209.0412 - val_loss: 216.8656 - val_mse: 216.8656\n",
            "Epoch 75/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.1860 - mse: 209.1860 - val_loss: 248.1336 - val_mse: 248.1336\n",
            "Epoch 76/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.4666 - mse: 209.4666 - val_loss: 205.5239 - val_mse: 205.5239\n",
            "Epoch 77/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.4750 - mse: 209.4750 - val_loss: 206.8618 - val_mse: 206.8618\n",
            "Epoch 78/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.8338 - mse: 208.8338 - val_loss: 207.4862 - val_mse: 207.4862\n",
            "Epoch 79/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.2830 - mse: 209.2830 - val_loss: 216.1723 - val_mse: 216.1723\n",
            "Epoch 80/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 210.1004 - mse: 210.1004 - val_loss: 205.4495 - val_mse: 205.4495\n",
            "Epoch 81/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.7222 - mse: 208.7222 - val_loss: 221.6791 - val_mse: 221.6791\n",
            "Epoch 82/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.1223 - mse: 209.1223 - val_loss: 210.9546 - val_mse: 210.9546\n",
            "Epoch 83/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.1391 - mse: 209.1391 - val_loss: 222.3604 - val_mse: 222.3604\n",
            "Epoch 84/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.7748 - mse: 208.7748 - val_loss: 227.1010 - val_mse: 227.1010\n",
            "Epoch 85/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.7126 - mse: 208.7126 - val_loss: 217.0412 - val_mse: 217.0412\n",
            "Epoch 86/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.2477 - mse: 209.2477 - val_loss: 216.3381 - val_mse: 216.3381\n",
            "Epoch 87/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.9732 - mse: 208.9732 - val_loss: 206.6625 - val_mse: 206.6625\n",
            "Epoch 88/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.5434 - mse: 208.5434 - val_loss: 214.0903 - val_mse: 214.0903\n",
            "Epoch 89/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.5674 - mse: 209.5674 - val_loss: 215.4266 - val_mse: 215.4266\n",
            "Epoch 90/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.9216 - mse: 207.9216 - val_loss: 222.9636 - val_mse: 222.9636\n",
            "Epoch 91/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.7824 - mse: 208.7824 - val_loss: 209.1888 - val_mse: 209.1888\n",
            "Epoch 92/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.3873 - mse: 209.3873 - val_loss: 212.2437 - val_mse: 212.2437\n",
            "Epoch 93/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.6127 - mse: 208.6127 - val_loss: 213.4269 - val_mse: 213.4269\n",
            "Epoch 94/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.6417 - mse: 208.6417 - val_loss: 212.5936 - val_mse: 212.5936\n",
            "Epoch 95/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.4066 - mse: 208.4066 - val_loss: 214.5654 - val_mse: 214.5654\n",
            "Epoch 96/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 209.2808 - mse: 209.2808 - val_loss: 238.8329 - val_mse: 238.8329\n",
            "Epoch 97/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.9129 - mse: 208.9129 - val_loss: 206.2990 - val_mse: 206.2990\n",
            "Epoch 98/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.9796 - mse: 208.9796 - val_loss: 210.5620 - val_mse: 210.5620\n",
            "Epoch 99/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 209.0076 - mse: 209.0076 - val_loss: 208.0787 - val_mse: 208.0787\n",
            "Epoch 100/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.6610 - mse: 208.6610 - val_loss: 224.4127 - val_mse: 224.4127\n",
            "Epoch 101/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.0284 - mse: 209.0284 - val_loss: 212.4364 - val_mse: 212.4364\n",
            "Epoch 102/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.7150 - mse: 208.7150 - val_loss: 220.9267 - val_mse: 220.9267\n",
            "Epoch 103/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.1103 - mse: 208.1103 - val_loss: 205.5058 - val_mse: 205.5058\n",
            "Epoch 104/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.2630 - mse: 208.2630 - val_loss: 206.2475 - val_mse: 206.2475\n",
            "Epoch 105/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.9169 - mse: 208.9169 - val_loss: 207.1659 - val_mse: 207.1659\n",
            "Epoch 106/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.5237 - mse: 208.5237 - val_loss: 246.1038 - val_mse: 246.1038\n",
            "Epoch 107/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.6718 - mse: 208.6718 - val_loss: 210.8450 - val_mse: 210.8450\n",
            "Epoch 108/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.8999 - mse: 208.8999 - val_loss: 215.5008 - val_mse: 215.5008\n",
            "Epoch 109/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.1052 - mse: 209.1052 - val_loss: 218.2890 - val_mse: 218.2890\n",
            "Epoch 110/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.6721 - mse: 208.6721 - val_loss: 209.9880 - val_mse: 209.9880\n",
            "Epoch 111/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.4084 - mse: 208.4084 - val_loss: 207.6711 - val_mse: 207.6711\n",
            "Epoch 112/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.4607 - mse: 208.4607 - val_loss: 212.4596 - val_mse: 212.4596\n",
            "Epoch 113/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.7055 - mse: 208.7055 - val_loss: 217.9501 - val_mse: 217.9501\n",
            "Epoch 114/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 209.1069 - mse: 209.1069 - val_loss: 210.9175 - val_mse: 210.9175\n",
            "Epoch 115/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.4133 - mse: 208.4133 - val_loss: 205.7028 - val_mse: 205.7028\n",
            "Epoch 116/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.3685 - mse: 208.3685 - val_loss: 230.6762 - val_mse: 230.6762\n",
            "Epoch 117/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.2068 - mse: 209.2068 - val_loss: 206.5386 - val_mse: 206.5386\n",
            "Epoch 118/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.6949 - mse: 208.6949 - val_loss: 205.5983 - val_mse: 205.5983\n",
            "Epoch 119/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.3547 - mse: 208.3547 - val_loss: 216.6076 - val_mse: 216.6076\n",
            "Epoch 120/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.5064 - mse: 208.5064 - val_loss: 208.8988 - val_mse: 208.8988\n",
            "Epoch 121/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.0446 - mse: 209.0446 - val_loss: 215.4934 - val_mse: 215.4934\n",
            "Epoch 122/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.6590 - mse: 208.6590 - val_loss: 255.7846 - val_mse: 255.7846\n",
            "Epoch 123/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.2090 - mse: 209.2090 - val_loss: 207.7148 - val_mse: 207.7148\n",
            "Epoch 124/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.5969 - mse: 208.5969 - val_loss: 220.7806 - val_mse: 220.7806\n",
            "Epoch 125/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.3393 - mse: 207.3393 - val_loss: 227.1334 - val_mse: 227.1334\n",
            "Epoch 126/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.4755 - mse: 208.4755 - val_loss: 222.0130 - val_mse: 222.0130\n",
            "Epoch 127/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.1204 - mse: 208.1204 - val_loss: 209.3821 - val_mse: 209.3821\n",
            "Epoch 128/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.5562 - mse: 208.5562 - val_loss: 223.2484 - val_mse: 223.2484\n",
            "Epoch 129/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.5728 - mse: 208.5728 - val_loss: 206.1620 - val_mse: 206.1620\n",
            "Epoch 130/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.4893 - mse: 208.4893 - val_loss: 215.5865 - val_mse: 215.5865\n",
            "Epoch 131/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.5698 - mse: 208.5698 - val_loss: 207.1498 - val_mse: 207.1498\n",
            "Epoch 132/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 208.6772 - mse: 208.6772 - val_loss: 205.7359 - val_mse: 205.7359\n",
            "Epoch 133/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.2583 - mse: 208.2583 - val_loss: 209.8506 - val_mse: 209.8506\n",
            "Epoch 134/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.8347 - mse: 207.8347 - val_loss: 222.1369 - val_mse: 222.1369\n",
            "Epoch 135/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.7082 - mse: 208.7082 - val_loss: 206.0945 - val_mse: 206.0945\n",
            "Epoch 136/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 209.0164 - mse: 209.0164 - val_loss: 205.6872 - val_mse: 205.6872\n",
            "Epoch 137/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.2494 - mse: 208.2494 - val_loss: 206.2334 - val_mse: 206.2334\n",
            "Epoch 138/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.3708 - mse: 208.3708 - val_loss: 212.0551 - val_mse: 212.0551\n",
            "Epoch 139/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.6445 - mse: 207.6445 - val_loss: 207.2037 - val_mse: 207.2037\n",
            "Epoch 140/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.5308 - mse: 208.5308 - val_loss: 207.4099 - val_mse: 207.4099\n",
            "Epoch 141/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.0956 - mse: 208.0956 - val_loss: 211.8741 - val_mse: 211.8741\n",
            "Epoch 142/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.4536 - mse: 208.4536 - val_loss: 206.0354 - val_mse: 206.0354\n",
            "Epoch 143/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.5579 - mse: 208.5579 - val_loss: 205.5252 - val_mse: 205.5252\n",
            "Epoch 144/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.3264 - mse: 208.3264 - val_loss: 206.8697 - val_mse: 206.8697\n",
            "Epoch 145/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.6003 - mse: 207.6003 - val_loss: 205.9419 - val_mse: 205.9419\n",
            "Epoch 146/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.2401 - mse: 208.2401 - val_loss: 213.6921 - val_mse: 213.6921\n",
            "Epoch 147/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.6712 - mse: 208.6712 - val_loss: 206.2099 - val_mse: 206.2099\n",
            "Epoch 148/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.2619 - mse: 208.2619 - val_loss: 207.3552 - val_mse: 207.3552\n",
            "Epoch 149/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.5240 - mse: 207.5240 - val_loss: 223.0591 - val_mse: 223.0591\n",
            "Epoch 150/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.6030 - mse: 208.6030 - val_loss: 206.9272 - val_mse: 206.9272\n",
            "Epoch 151/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.1532 - mse: 208.1532 - val_loss: 212.7283 - val_mse: 212.7283\n",
            "Epoch 152/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.3212 - mse: 208.3212 - val_loss: 207.9721 - val_mse: 207.9721\n",
            "Epoch 153/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.9625 - mse: 207.9625 - val_loss: 213.0431 - val_mse: 213.0431\n",
            "Epoch 154/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7504 - mse: 207.7504 - val_loss: 221.4494 - val_mse: 221.4494\n",
            "Epoch 155/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7736 - mse: 207.7736 - val_loss: 227.1882 - val_mse: 227.1882\n",
            "Epoch 156/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.3001 - mse: 208.3001 - val_loss: 206.6450 - val_mse: 206.6450\n",
            "Epoch 157/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.6653 - mse: 207.6653 - val_loss: 208.4272 - val_mse: 208.4272\n",
            "Epoch 158/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.8499 - mse: 207.8499 - val_loss: 212.8407 - val_mse: 212.8407\n",
            "Epoch 159/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7007 - mse: 207.7007 - val_loss: 214.5920 - val_mse: 214.5920\n",
            "Epoch 160/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.3658 - mse: 208.3658 - val_loss: 209.6038 - val_mse: 209.6038\n",
            "Epoch 161/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.9942 - mse: 207.9942 - val_loss: 207.0704 - val_mse: 207.0704\n",
            "Epoch 162/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.8637 - mse: 207.8637 - val_loss: 210.0945 - val_mse: 210.0945\n",
            "Epoch 163/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.7104 - mse: 207.7104 - val_loss: 206.2148 - val_mse: 206.2148\n",
            "Epoch 164/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.6658 - mse: 207.6658 - val_loss: 206.0356 - val_mse: 206.0356\n",
            "Epoch 165/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.9235 - mse: 207.9235 - val_loss: 206.1657 - val_mse: 206.1657\n",
            "Epoch 166/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7385 - mse: 207.7385 - val_loss: 207.5779 - val_mse: 207.5779\n",
            "Epoch 167/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7206 - mse: 207.7206 - val_loss: 210.6276 - val_mse: 210.6276\n",
            "Epoch 168/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.0469 - mse: 208.0469 - val_loss: 207.3715 - val_mse: 207.3715\n",
            "Epoch 169/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.0190 - mse: 208.0190 - val_loss: 208.0575 - val_mse: 208.0575\n",
            "Epoch 170/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.4468 - mse: 207.4468 - val_loss: 218.5126 - val_mse: 218.5126\n",
            "Epoch 171/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7890 - mse: 207.7890 - val_loss: 212.8311 - val_mse: 212.8311\n",
            "Epoch 172/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.2174 - mse: 208.2174 - val_loss: 207.6541 - val_mse: 207.6541\n",
            "Epoch 173/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.8950 - mse: 206.8950 - val_loss: 236.9738 - val_mse: 236.9738\n",
            "Epoch 174/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.0011 - mse: 208.0011 - val_loss: 212.9919 - val_mse: 212.9919\n",
            "Epoch 175/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.5217 - mse: 207.5217 - val_loss: 207.7880 - val_mse: 207.7880\n",
            "Epoch 176/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 208.0658 - mse: 208.0658 - val_loss: 224.1133 - val_mse: 224.1133\n",
            "Epoch 177/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.4091 - mse: 207.4091 - val_loss: 205.9774 - val_mse: 205.9774\n",
            "Epoch 178/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.5521 - mse: 207.5521 - val_loss: 207.5962 - val_mse: 207.5962\n",
            "Epoch 179/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7526 - mse: 207.7526 - val_loss: 208.9009 - val_mse: 208.9009\n",
            "Epoch 180/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.4460 - mse: 207.4460 - val_loss: 210.2859 - val_mse: 210.2859\n",
            "Epoch 181/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.2077 - mse: 207.2077 - val_loss: 222.5865 - val_mse: 222.5865\n",
            "Epoch 182/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.0416 - mse: 208.0416 - val_loss: 211.7245 - val_mse: 211.7245\n",
            "Epoch 183/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.1724 - mse: 207.1724 - val_loss: 212.2284 - val_mse: 212.2284\n",
            "Epoch 184/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.8007 - mse: 207.8007 - val_loss: 208.7144 - val_mse: 208.7144\n",
            "Epoch 185/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.4790 - mse: 207.4790 - val_loss: 228.8275 - val_mse: 228.8275\n",
            "Epoch 186/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.1025 - mse: 208.1025 - val_loss: 219.8283 - val_mse: 219.8283\n",
            "Epoch 187/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.9754 - mse: 207.9754 - val_loss: 212.8129 - val_mse: 212.8129\n",
            "Epoch 188/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.6026 - mse: 207.6026 - val_loss: 221.8182 - val_mse: 221.8182\n",
            "Epoch 189/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 208.0149 - mse: 208.0149 - val_loss: 216.6116 - val_mse: 216.6116\n",
            "Epoch 190/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.2282 - mse: 207.2282 - val_loss: 218.8427 - val_mse: 218.8427\n",
            "Epoch 191/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.8649 - mse: 207.8649 - val_loss: 213.8698 - val_mse: 213.8698\n",
            "Epoch 192/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.9843 - mse: 207.9843 - val_loss: 207.6569 - val_mse: 207.6569\n",
            "Epoch 193/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.7235 - mse: 207.7235 - val_loss: 218.1741 - val_mse: 218.1741\n",
            "Epoch 194/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.2054 - mse: 207.2054 - val_loss: 213.3550 - val_mse: 213.3550\n",
            "Epoch 195/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.8347 - mse: 206.8347 - val_loss: 218.1404 - val_mse: 218.1404\n",
            "Epoch 196/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.2406 - mse: 207.2406 - val_loss: 219.7832 - val_mse: 219.7832\n",
            "Epoch 197/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.3850 - mse: 207.3850 - val_loss: 223.4256 - val_mse: 223.4256\n",
            "Epoch 198/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 207.7902 - mse: 207.7902 - val_loss: 205.9730 - val_mse: 205.9730\n",
            "Epoch 199/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.3688 - mse: 207.3688 - val_loss: 205.9155 - val_mse: 205.9155\n",
            "Epoch 200/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.0958 - mse: 207.0958 - val_loss: 213.7346 - val_mse: 213.7346\n",
            "Epoch 201/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.9911 - mse: 206.9911 - val_loss: 215.3427 - val_mse: 215.3427\n",
            "Epoch 202/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.2016 - mse: 207.2016 - val_loss: 205.8043 - val_mse: 205.8043\n",
            "Epoch 203/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.4768 - mse: 207.4768 - val_loss: 206.2680 - val_mse: 206.2680\n",
            "Epoch 204/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.8781 - mse: 206.8781 - val_loss: 216.0585 - val_mse: 216.0585\n",
            "Epoch 205/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.8798 - mse: 206.8798 - val_loss: 215.3959 - val_mse: 215.3959\n",
            "Epoch 206/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.2579 - mse: 207.2579 - val_loss: 209.9384 - val_mse: 209.9384\n",
            "Epoch 207/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.1574 - mse: 207.1574 - val_loss: 209.4545 - val_mse: 209.4545\n",
            "Epoch 208/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.3979 - mse: 207.3979 - val_loss: 205.9290 - val_mse: 205.9290\n",
            "Epoch 209/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.1702 - mse: 207.1702 - val_loss: 216.3285 - val_mse: 216.3285\n",
            "Epoch 210/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 207.0092 - mse: 207.0092 - val_loss: 209.2963 - val_mse: 209.2963\n",
            "Epoch 211/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.5739 - mse: 206.5739 - val_loss: 205.5765 - val_mse: 205.5765\n",
            "Epoch 212/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.8511 - mse: 206.8511 - val_loss: 208.7701 - val_mse: 208.7701\n",
            "Epoch 213/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.4084 - mse: 206.4084 - val_loss: 208.9996 - val_mse: 208.9996\n",
            "Epoch 214/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.0019 - mse: 207.0019 - val_loss: 210.3877 - val_mse: 210.3877\n",
            "Epoch 215/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 207.0106 - mse: 207.0106 - val_loss: 206.7248 - val_mse: 206.7248\n",
            "Epoch 216/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.7608 - mse: 206.7608 - val_loss: 205.9187 - val_mse: 205.9187\n",
            "Epoch 217/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.5202 - mse: 206.5202 - val_loss: 208.3767 - val_mse: 208.3767\n",
            "Epoch 218/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.9112 - mse: 206.9112 - val_loss: 205.3387 - val_mse: 205.3387\n",
            "Epoch 219/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.6311 - mse: 206.6311 - val_loss: 205.6232 - val_mse: 205.6232\n",
            "Epoch 220/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.3374 - mse: 206.3374 - val_loss: 205.6116 - val_mse: 205.6116\n",
            "Epoch 221/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.8496 - mse: 206.8496 - val_loss: 206.8750 - val_mse: 206.8750\n",
            "Epoch 222/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.5414 - mse: 206.5414 - val_loss: 206.4136 - val_mse: 206.4136\n",
            "Epoch 223/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.6396 - mse: 206.6396 - val_loss: 206.0441 - val_mse: 206.0441\n",
            "Epoch 224/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.5971 - mse: 206.5971 - val_loss: 205.4595 - val_mse: 205.4595\n",
            "Epoch 225/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.8480 - mse: 206.8480 - val_loss: 206.1403 - val_mse: 206.1403\n",
            "Epoch 226/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.5528 - mse: 206.5528 - val_loss: 206.4422 - val_mse: 206.4422\n",
            "Epoch 227/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.4582 - mse: 206.4582 - val_loss: 205.6987 - val_mse: 205.6987\n",
            "Epoch 228/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.8416 - mse: 206.8416 - val_loss: 209.6183 - val_mse: 209.6183\n",
            "Epoch 229/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.5504 - mse: 206.5504 - val_loss: 205.9371 - val_mse: 205.9371\n",
            "Epoch 230/350\n",
            "75/75 [==============================] - 1s 8ms/step - loss: 206.7381 - mse: 206.7381 - val_loss: 206.7165 - val_mse: 206.7165\n",
            "Epoch 231/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.4121 - mse: 206.4121 - val_loss: 205.0357 - val_mse: 205.0357\n",
            "Epoch 232/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.4057 - mse: 206.4057 - val_loss: 206.6400 - val_mse: 206.6400\n",
            "Epoch 233/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.6892 - mse: 206.6892 - val_loss: 208.9713 - val_mse: 208.9713\n",
            "Epoch 234/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2305 - mse: 206.2305 - val_loss: 205.5511 - val_mse: 205.5511\n",
            "Epoch 235/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.3239 - mse: 206.3239 - val_loss: 206.2070 - val_mse: 206.2070\n",
            "Epoch 236/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.5159 - mse: 206.5159 - val_loss: 210.1706 - val_mse: 210.1706\n",
            "Epoch 237/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.4693 - mse: 206.4693 - val_loss: 206.5698 - val_mse: 206.5698\n",
            "Epoch 238/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1800 - mse: 206.1800 - val_loss: 205.7848 - val_mse: 205.7848\n",
            "Epoch 239/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2197 - mse: 206.2197 - val_loss: 209.2098 - val_mse: 209.2098\n",
            "Epoch 240/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.3611 - mse: 206.3611 - val_loss: 205.4974 - val_mse: 205.4974\n",
            "Epoch 241/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0631 - mse: 206.0631 - val_loss: 205.8823 - val_mse: 205.8823\n",
            "Epoch 242/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0913 - mse: 206.0913 - val_loss: 206.2972 - val_mse: 206.2972\n",
            "Epoch 243/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.4094 - mse: 206.4094 - val_loss: 207.5483 - val_mse: 207.5483\n",
            "Epoch 244/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.8056 - mse: 205.8056 - val_loss: 205.8857 - val_mse: 205.8857\n",
            "Epoch 245/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2521 - mse: 206.2521 - val_loss: 206.6555 - val_mse: 206.6555\n",
            "Epoch 246/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9004 - mse: 205.9004 - val_loss: 205.5726 - val_mse: 205.5726\n",
            "Epoch 247/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1240 - mse: 206.1240 - val_loss: 205.6598 - val_mse: 205.6598\n",
            "Epoch 248/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.3659 - mse: 206.3659 - val_loss: 206.0614 - val_mse: 206.0614\n",
            "Epoch 249/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1330 - mse: 206.1330 - val_loss: 205.1944 - val_mse: 205.1944\n",
            "Epoch 250/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0040 - mse: 206.0040 - val_loss: 209.0561 - val_mse: 209.0561\n",
            "Epoch 251/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.0345 - mse: 206.0345 - val_loss: 204.9621 - val_mse: 204.9621\n",
            "Epoch 252/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0169 - mse: 206.0169 - val_loss: 206.6221 - val_mse: 206.6221\n",
            "Epoch 253/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0572 - mse: 206.0572 - val_loss: 208.9181 - val_mse: 208.9181\n",
            "Epoch 254/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2529 - mse: 206.2529 - val_loss: 205.0965 - val_mse: 205.0965\n",
            "Epoch 255/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0373 - mse: 206.0373 - val_loss: 205.6334 - val_mse: 205.6334\n",
            "Epoch 256/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.8344 - mse: 205.8344 - val_loss: 204.9897 - val_mse: 204.9897\n",
            "Epoch 257/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1645 - mse: 206.1645 - val_loss: 207.4858 - val_mse: 207.4858\n",
            "Epoch 258/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1380 - mse: 206.1380 - val_loss: 205.9725 - val_mse: 205.9725\n",
            "Epoch 259/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.1513 - mse: 206.1513 - val_loss: 205.4267 - val_mse: 205.4267\n",
            "Epoch 260/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.1211 - mse: 206.1211 - val_loss: 205.3300 - val_mse: 205.3300\n",
            "Epoch 261/350\n",
            "75/75 [==============================] - 1s 7ms/step - loss: 205.9756 - mse: 205.9756 - val_loss: 204.9596 - val_mse: 204.9596\n",
            "Epoch 262/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.9658 - mse: 205.9658 - val_loss: 205.2424 - val_mse: 205.2424\n",
            "Epoch 263/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.2447 - mse: 206.2447 - val_loss: 205.1682 - val_mse: 205.1682\n",
            "Epoch 264/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 205.9754 - mse: 205.9754 - val_loss: 205.6202 - val_mse: 205.6202\n",
            "Epoch 265/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1810 - mse: 206.1810 - val_loss: 206.0721 - val_mse: 206.0721\n",
            "Epoch 266/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1285 - mse: 206.1285 - val_loss: 205.0704 - val_mse: 205.0704\n",
            "Epoch 267/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0477 - mse: 206.0477 - val_loss: 205.4461 - val_mse: 205.4461\n",
            "Epoch 268/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1093 - mse: 206.1093 - val_loss: 205.6037 - val_mse: 205.6037\n",
            "Epoch 269/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1472 - mse: 206.1472 - val_loss: 206.6349 - val_mse: 206.6349\n",
            "Epoch 270/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9437 - mse: 205.9437 - val_loss: 208.1212 - val_mse: 208.1212\n",
            "Epoch 271/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9166 - mse: 205.9166 - val_loss: 205.1859 - val_mse: 205.1859\n",
            "Epoch 272/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0909 - mse: 206.0909 - val_loss: 206.0900 - val_mse: 206.0900\n",
            "Epoch 273/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0965 - mse: 206.0965 - val_loss: 207.1142 - val_mse: 207.1142\n",
            "Epoch 274/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1738 - mse: 206.1738 - val_loss: 205.3754 - val_mse: 205.3754\n",
            "Epoch 275/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0280 - mse: 206.0280 - val_loss: 206.2834 - val_mse: 206.2834\n",
            "Epoch 276/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.3157 - mse: 206.3157 - val_loss: 205.4403 - val_mse: 205.4403\n",
            "Epoch 277/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9734 - mse: 205.9734 - val_loss: 205.5573 - val_mse: 205.5573\n",
            "Epoch 278/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0672 - mse: 206.0672 - val_loss: 205.4719 - val_mse: 205.4719\n",
            "Epoch 279/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1379 - mse: 206.1379 - val_loss: 206.7850 - val_mse: 206.7850\n",
            "Epoch 280/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2540 - mse: 206.2540 - val_loss: 205.0712 - val_mse: 205.0712\n",
            "Epoch 281/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.3451 - mse: 206.3451 - val_loss: 205.4202 - val_mse: 205.4202\n",
            "Epoch 282/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0453 - mse: 206.0453 - val_loss: 205.6362 - val_mse: 205.6362\n",
            "Epoch 283/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.8797 - mse: 205.8797 - val_loss: 208.3752 - val_mse: 208.3752\n",
            "Epoch 284/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1677 - mse: 206.1677 - val_loss: 205.5034 - val_mse: 205.5034\n",
            "Epoch 285/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0029 - mse: 206.0029 - val_loss: 205.1771 - val_mse: 205.1771\n",
            "Epoch 286/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1072 - mse: 206.1072 - val_loss: 207.1934 - val_mse: 207.1934\n",
            "Epoch 287/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0742 - mse: 206.0742 - val_loss: 206.2457 - val_mse: 206.2457\n",
            "Epoch 288/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2269 - mse: 206.2269 - val_loss: 206.3950 - val_mse: 206.3950\n",
            "Epoch 289/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.8775 - mse: 205.8775 - val_loss: 208.6612 - val_mse: 208.6612\n",
            "Epoch 290/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.8966 - mse: 205.8966 - val_loss: 206.0113 - val_mse: 206.0113\n",
            "Epoch 291/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.8092 - mse: 205.8092 - val_loss: 208.2046 - val_mse: 208.2046\n",
            "Epoch 292/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.8436 - mse: 205.8436 - val_loss: 205.5521 - val_mse: 205.5521\n",
            "Epoch 293/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.3092 - mse: 206.3092 - val_loss: 204.9462 - val_mse: 204.9462\n",
            "Epoch 294/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.8525 - mse: 205.8525 - val_loss: 205.3106 - val_mse: 205.3106\n",
            "Epoch 295/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.9848 - mse: 205.9848 - val_loss: 205.5115 - val_mse: 205.5115\n",
            "Epoch 296/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.8388 - mse: 205.8388 - val_loss: 205.1684 - val_mse: 205.1684\n",
            "Epoch 297/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0186 - mse: 206.0186 - val_loss: 205.9610 - val_mse: 205.9610\n",
            "Epoch 298/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0148 - mse: 206.0148 - val_loss: 206.5534 - val_mse: 206.5534\n",
            "Epoch 299/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.2426 - mse: 206.2426 - val_loss: 205.4548 - val_mse: 205.4548\n",
            "Epoch 300/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1125 - mse: 206.1125 - val_loss: 205.3653 - val_mse: 205.3653\n",
            "Epoch 301/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0836 - mse: 206.0836 - val_loss: 205.7809 - val_mse: 205.7809\n",
            "Epoch 302/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.7432 - mse: 205.7432 - val_loss: 206.9091 - val_mse: 206.9091\n",
            "Epoch 303/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1763 - mse: 206.1763 - val_loss: 205.4393 - val_mse: 205.4393\n",
            "Epoch 304/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9359 - mse: 205.9359 - val_loss: 208.4517 - val_mse: 208.4517\n",
            "Epoch 305/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0396 - mse: 206.0396 - val_loss: 205.0709 - val_mse: 205.0709\n",
            "Epoch 306/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9765 - mse: 205.9765 - val_loss: 205.4549 - val_mse: 205.4549\n",
            "Epoch 307/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.3203 - mse: 206.3203 - val_loss: 205.1375 - val_mse: 205.1375\n",
            "Epoch 308/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9699 - mse: 205.9699 - val_loss: 207.7126 - val_mse: 207.7126\n",
            "Epoch 309/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0408 - mse: 206.0408 - val_loss: 204.8370 - val_mse: 204.8370\n",
            "Epoch 310/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1960 - mse: 206.1960 - val_loss: 205.8369 - val_mse: 205.8369\n",
            "Epoch 311/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1866 - mse: 206.1866 - val_loss: 205.3288 - val_mse: 205.3288\n",
            "Epoch 312/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9854 - mse: 205.9854 - val_loss: 205.1254 - val_mse: 205.1254\n",
            "Epoch 313/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9018 - mse: 205.9018 - val_loss: 207.4762 - val_mse: 207.4762\n",
            "Epoch 314/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.2052 - mse: 206.2052 - val_loss: 206.7457 - val_mse: 206.7457\n",
            "Epoch 315/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9568 - mse: 205.9568 - val_loss: 206.1123 - val_mse: 206.1123\n",
            "Epoch 316/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1126 - mse: 206.1126 - val_loss: 205.4721 - val_mse: 205.4721\n",
            "Epoch 317/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9391 - mse: 205.9391 - val_loss: 206.6289 - val_mse: 206.6289\n",
            "Epoch 318/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0185 - mse: 206.0185 - val_loss: 205.2996 - val_mse: 205.2996\n",
            "Epoch 319/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.0243 - mse: 206.0243 - val_loss: 206.2953 - val_mse: 206.2953\n",
            "Epoch 320/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0064 - mse: 206.0064 - val_loss: 205.1709 - val_mse: 205.1709\n",
            "Epoch 321/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9178 - mse: 205.9178 - val_loss: 208.9133 - val_mse: 208.9133\n",
            "Epoch 322/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.3446 - mse: 206.3446 - val_loss: 205.2056 - val_mse: 205.2056\n",
            "Epoch 323/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.0853 - mse: 206.0853 - val_loss: 205.4815 - val_mse: 205.4815\n",
            "Epoch 324/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.2083 - mse: 206.2083 - val_loss: 205.7631 - val_mse: 205.7631\n",
            "Epoch 325/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.0637 - mse: 206.0637 - val_loss: 205.6919 - val_mse: 205.6919\n",
            "Epoch 326/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 205.9071 - mse: 205.9071 - val_loss: 206.4525 - val_mse: 206.4525\n",
            "Epoch 327/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.1172 - mse: 206.1172 - val_loss: 206.2227 - val_mse: 206.2227\n",
            "Epoch 328/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2611 - mse: 206.2611 - val_loss: 205.3549 - val_mse: 205.3549\n",
            "Epoch 329/350\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 206.1205 - mse: 206.1205 - val_loss: 205.3848 - val_mse: 205.3848\n",
            "Epoch 330/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.1186 - mse: 206.1186 - val_loss: 205.3689 - val_mse: 205.3689\n",
            "Epoch 331/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.8188 - mse: 205.8188 - val_loss: 205.8650 - val_mse: 205.8650\n",
            "Epoch 332/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1795 - mse: 206.1795 - val_loss: 205.3385 - val_mse: 205.3385\n",
            "Epoch 333/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9525 - mse: 205.9525 - val_loss: 204.9083 - val_mse: 204.9083\n",
            "Epoch 334/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.7628 - mse: 205.7628 - val_loss: 207.9819 - val_mse: 207.9819\n",
            "Epoch 335/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2892 - mse: 206.2892 - val_loss: 206.9824 - val_mse: 206.9824\n",
            "Epoch 336/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1494 - mse: 206.1494 - val_loss: 205.4118 - val_mse: 205.4118\n",
            "Epoch 337/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1243 - mse: 206.1243 - val_loss: 205.6298 - val_mse: 205.6298\n",
            "Epoch 338/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9409 - mse: 205.9409 - val_loss: 205.8915 - val_mse: 205.8915\n",
            "Epoch 339/350\n",
            "75/75 [==============================] - 0s 6ms/step - loss: 206.1848 - mse: 206.1848 - val_loss: 205.8711 - val_mse: 205.8711\n",
            "Epoch 340/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9551 - mse: 205.9551 - val_loss: 205.8547 - val_mse: 205.8547\n",
            "Epoch 341/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0290 - mse: 206.0290 - val_loss: 205.1389 - val_mse: 205.1389\n",
            "Epoch 342/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2883 - mse: 206.2883 - val_loss: 204.9654 - val_mse: 204.9654\n",
            "Epoch 343/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9828 - mse: 205.9828 - val_loss: 205.1599 - val_mse: 205.1599\n",
            "Epoch 344/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0557 - mse: 206.0557 - val_loss: 209.0386 - val_mse: 209.0386\n",
            "Epoch 345/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1788 - mse: 206.1788 - val_loss: 205.6330 - val_mse: 205.6330\n",
            "Epoch 346/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1725 - mse: 206.1725 - val_loss: 205.2571 - val_mse: 205.2571\n",
            "Epoch 347/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.1205 - mse: 206.1205 - val_loss: 205.5710 - val_mse: 205.5710\n",
            "Epoch 348/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 205.9448 - mse: 205.9448 - val_loss: 205.7359 - val_mse: 205.7359\n",
            "Epoch 349/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.2701 - mse: 206.2701 - val_loss: 205.9398 - val_mse: 205.9398\n",
            "Epoch 350/350\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 206.0907 - mse: 206.0907 - val_loss: 206.0893 - val_mse: 206.0893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model4.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED-6-mUWqvkp",
        "outputId": "6dae2e8b-3eb4-4846-ff85-322bc345478f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 0s 2ms/step - loss: 205.1266 - mse: 205.1266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict = model3.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_predict, y_test)\n",
        "mae = mean_absolute_error(y_predict, y_test)\n",
        "y2 = r2_score(y_predict,y_test)\n",
        "print(\"MSE: \" , mse)\n",
        "print(\"MAE: \" , mae)\n",
        "print(\"R2: \" , y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTEew8HTq550",
        "outputId": "d043ad54-617b-4783-951b-be90543979c1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "46/46 [==============================] - 0s 2ms/step\n",
            "MSE:  151.13537395700263\n",
            "MAE:  8.813048176853801\n",
            "R2:  -3.534102567928214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_arr = np.array([[6,0,1,1,4]]) #El primer ejemplo del dataset original\n",
        "predicciones = model4.predict(test_arr)\n",
        "print(predicciones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4zJqwblq6Nu",
        "outputId": "614c30bb-e268-4bc0-92ca-3a29c8c01bef"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 69ms/step\n",
            "[[69.850075 53.986816 31.077312 46.08039  54.1419   67.256744 70.77414\n",
            "  70.19811  71.65818  69.616005 70.77061  73.31954  74.32336  73.968445\n",
            "  69.68287  75.449326 78.62221  75.631226 76.609764 78.69107  70.562996\n",
            "  69.331505 67.60063  61.082626 65.67988  59.648407 54.09131  48.222458\n",
            "  49.130527 45.934196 65.59868  71.023506 72.8403   67.43111  45.356514\n",
            "  44.297703 67.21324  73.15995  72.77331  75.11641  74.10496  73.87152\n",
            "  74.1202   61.469376]]\n"
          ]
        }
      ]
    }
  ]
}